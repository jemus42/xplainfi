---
title: "LOCO and LOCI"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{LOCO and LOCI}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
set.seed(123)
# Quiet down
lgr::get_logger("mlr3")$set_threshold("warn")
```

```{r setup}
library(xplainfi)
library(mlr3)
library(mlr3learners)
library(data.table)
library(ggplot2)
```

## Example Data: Interaction Effects

To illustrate the key differences between LOCO and LOCI, we'll use a data generating process with interaction effects:

$$y = 2 \cdot x_1 \cdot x_2 + x_3 + \epsilon$$

where $\epsilon \sim N(0, 0.5^2)$ and all features $x_1, x_2, x_3, noise_1, noise_2 \sim N(0,1)$ are independent.

**Key characteristics:**

- **$x_1, x_2$**: Have NO individual effects, only interact with each other
- **$x_3$**: Has a direct main effect on $y$  
- **$noise_1, noise_2$**: Pure noise variables with no effect on $y$

This setup highlights how LOCO and LOCI handle interaction effects differently.

## Leave-One-Covariate-Out (LOCO)

LOCO measures feature importance by comparing model performance with and without each feature. For each feature, the learner is retrained without that feature and the performance difference indicates the feature's importance.

For feature $j$, LOCO is calculated as the difference in expected loss of the model fit without the feature and the full model:
$$\text{LOCO}_j = \mathbb{E}(L(Y, f_{-j}(X_{-j}))) - \mathbb{E}(L(Y, f(X)))$$


Higher values indicate more important features (larger performance drop when removed).

```{r loco-example}
task <- sim_dgp_interactions(n = 500)
learner <- lrn("regr.lm")
measure <- msr("regr.mse")

loco <- LOCO$new(
  task = task,
  learner = learner,
  measure = measure
)

loco$compute()
```

## Leave-One-Covariate-In (LOCI)

LOCI measures feature importance by training models with only each individual feature and comparing their performance to a featureless (baseline) model. This shows how much predictive power each feature provides on its own, above and beyond the optimal constant prediction.

For feature $j$, LOCI is calculated as the difference in expected loss of the featureless learner or constant model and the model including only the feature:
$$\text{LOCI}_j = \mathbb{E}(L(Y, f_{\emptyset})) - \mathbb{E}(L(Y, f_j(X_{j})))$$

Higher values indicate more important features (better individual performance compared to baseline).

```{r loci-example}
loci <- LOCI$new(
  task = task,
  learner = learner,
  measure = measure
)

loci$compute()
```

## Understanding the Results

**LOCO results interpretation:**

- $x_1$ and $x_2$ show low importance because removing either one eliminates the interaction ($x_1 \cdot x_2$ becomes zero)
- $x_3$ shows higher importance due to its direct main effect
- This demonstrates LOCO's limitation with interaction effects

**LOCI results interpretation:**  

- $x_1$ and $x_2$ show low importance because individually they have no predictive power
- $x_3$ shows high importance because it alone can predict $y$ reasonably well
- This shows how LOCI focuses on individual feature contributions

## Multiple Refits

Like PFI has `iters_perm` for multiple permutation iterations, LOCO and LOCI support `iters_refit` for multiple refit iterations per resampling iteration:

```{r multiple-refits}
loco_multi = LOCO$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = rsmp("cv", folds = 3),
  iters_refit = 3L
)

loco_multi$compute()

# Check individual scores with multiple refits
loco_multi$scores[1:10, ] |>
  knitr::kable(digits = 4, caption = "LOCO scores per refit and resampling fold")
```

## Comparing LOCO and LOCI

```{r comparison}
# Combine results for comparison
importance_combined <- rbind(
  loco$importance[, method := "LOCO"],
  loci$importance[, method := "LOCI"]
)

importance_combined <- importance_combined |>
  dcast(feature ~ method, value.var = "importance")

importance_combined |>
  knitr::kable(digits = 4, caption = "LOCO vs LOCI importance scores")
```

**Interpreting the results:**

- **LOCO**: Higher values indicate more important features (larger performance drop when removed)
- **LOCI**: Higher values indicate more important features (better individual performance compared to baseline)
  - Positive values: feature performs better than featureless baseline
  - Negative values: feature performs worse than featureless baseline

```{r plot}
#| echo: false
#| fig-width: 7
#| fig-height: 7

importance_combined |>
  data.table::melt(
    id.vars = "feature",
    value.name = "score",
    variable.name = "method"
  ) |>
  ggplot(aes(x = score, y = reorder(feature, score), color = method, fill = method)) +
  facet_wrap(vars(method), ncol = 1, scales = "free") +
  geom_col(position = "dodge", alpha = 0.7) +
  scale_color_brewer(palette = "Set1", aesthetics = c("color", "fill")) +
  labs(
    title = "LOCO vs LOCI Feature Importance",
    x = "Importance Score",
    y = "Feature",
    color = "Method",
    fill = "Method"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    plot.title.position = "plot"
  ) 
```

## Micro-averaged vs Macro-averaged LOCO

The examples above use the **micro-averaged** approach (`obs_loss = FALSE`), computing importance as the difference of expected losses using the measure's default aggregation: $\mathbb{E}(L(Y, f_{-j}(X_{-j}))) - \mathbb{E}(L(Y, f(X)))$. 

LOCO can also be computed using the **macro-averaged** approach (`obs_loss = TRUE`), which follows the original paper formulation by aggregating observation-wise differences with the median: $\mathrm{median}(\{L(y_i, f_{-j}(x_{i,-j})) - L(y_i, f(x_i))\}_{i=1}^n)$.

The macro-averaged approach provides access to individual observation-level differences:

```{r obs-loss-regression}
# Use macro-averaged approach with median aggregation
loco_obs <- LOCO$new(
  task = task,
  learner = learner,
  measure = msr("regr.mae"),  # MAE supports obs_loss
  obs_loss = TRUE,
  aggregation_fun = median
)

loco_obs$compute()

# Compare aggregated results
rbind(
  loco$importance[, method := "Micro-averaged (measure default)"],
  loco_obs$importance[, method := "Macro-averaged (median of diffs)"]
) |>
  dcast(feature ~ method, value.var = "importance") |>
  knitr::kable(digits = 4, caption = "Comparison of LOCO aggregation approaches")
```

The `$obs_losses` field provides detailed observation-level information:

```{r obs-loss-details}
# Examine observation-wise data for first feature
feature_data <- loco_obs$obs_losses[feature == "x3"]

# Show structure
str(feature_data)

# Display first few observations
head(feature_data) |>
  knitr::kable(digits = 3, caption = "Observation-wise losses for x3")
```

Each row contains:

- `row_ids`: Test set observation identifiers
- `truth`: Original target values  
- `response_ref`: Full model predictions
- `response_feature`: Reduced model predictions (without this feature)
- `loss_ref` / `loss_feature`: Individual losses from each model
- `obs_diff`: Observation-wise importance differences

Note that in case of `regr.mse` which aggregates with the mean and using `aggregation_fun = mean` to aggregate the observation-wise losses, both approaches yield the same result, but in slightly different ways:

```{r obs-loss-mean-identity}
set.seed(1)
resampling = rsmp("holdout")
resampling$instantiate(task)

loco_orig <- LOCO$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure
)

loco_obsloss <- LOCO$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure,
  obs_loss = TRUE,
  aggregation_fun = mean
)

rbind(
  loco_orig$compute()[, method := "micro-averaged"],
  loco_obsloss$compute()[, method := "macro-averaged"]
) |>
  dcast(feature ~ method, value.var = "importance")
```

## Classification Example with Probability Access

For classification tasks, the `obs_losses` field stores predicted classes while preserving access to probabilities from the original mlr3 prediction objects:

```{r classification-example}
# Binary classification task
task_classif <- tgen("circle", d = 6)$generate(n = 200)

# Learner with probability predictions (required for accessing probabilities later)
learner_classif <- lrn("classif.rpart", predict_type = "prob")

# LOCO with observation-wise losses
loco_classif <- LOCO$new(
  task = task_classif,
  learner = learner_classif,
  measure = msr("classif.ce"),  # Classification error supports obs_loss (zero-one in this case)
  obs_loss = TRUE,
  features = c("x1", "x2"),  # Focus on just two features for clarity
  aggregation_fun = mean
)

loco_classif$compute()

# Show classification importance
loco_classif$importance |>
  knitr::kable(digits = 4, caption = "LOCO importance for classification")
```

Examine the observation-wise classification results:

```{r classification-obs-details}
# Look at observation-wise data
classif_data <- head(loco_classif$obs_losses, 8)

classif_data |>
  knitr::kable(caption = "Observation-wise classification losses")
```

For classification, the response columns contain predicted classes (factors). Note that `$resample_result` only contains the reference model predictions (full model for LOCO, featureless for LOCI), not the feature-specific models:

```{r access-probabilities}
# Access feature-specific predictions (available with obs_loss = TRUE)
head(loco_classif$predictions)

# Get probabilities for a specific feature and iteration  
if (nrow(loco_classif$predictions) > 0) {
  feature_pred <- loco_classif$predictions[1, ]$prediction[[1]]
  
  # Access both predicted classes and probabilities
  # Note: probabilities are only available because we used predict_type = "prob"
  head(feature_pred$response)  # Predicted classes
  head(feature_pred$prob)      # Probability matrix
}

# Compare with reference model (full model for LOCO)
ref_pred <- loco_classif$resample_result$prediction(1)
head(ref_pred$prob)  # Reference model probabilities
```

The `$predictions` field provides complete access to feature-specific prediction objects:

- `feature`: Which feature was left out (LOCO) or left in (LOCI)
- `iteration`: Resampling iteration
- `iter_refit`: Refit iteration within resampling  
- `prediction`: Complete mlr3 prediction object with responses, probabilities, etc.

This gives you full access to:

- **Feature-specific probabilities**: `prediction$prob`
- **Feature-specific predicted classes**: `prediction$response`
- **Additional prediction data**: Any other fields the learner provides

The observation-wise approach is particularly useful for:

- **Robust aggregation**: Using median instead of mean to reduce outlier influence
- **Individual analysis**: Examining which specific observations drive importance scores
- **Distribution analysis**: Understanding the variability in feature effects across observations
- **Model debugging**: Identifying problematic observations or prediction patterns
