---
title: "Getting Started with xplainfi"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with xplainfi}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
set.seed(123)
# Quiet down
lgr::get_logger("mlr3")$set_threshold("warn")
```

```{r setup}
library(xplainfi)
library(mlr3)
library(mlr3learners)
library(data.table)
library(ggplot2)
```

The **xplainfi** package provides feature importance methods for machine learning models. It implements several approaches for measuring how much each feature contributes to model predictions, with a focus on model-agnostic methods that work with any learner.

## Core Concepts

Feature importance methods in xplainfi answer different but related questions:

- **How much does each feature contribute to model performance?** (Permutation Feature Importance)
- **What happens when we remove features and retrain?** (Leave-One-Covariate-Out)  
- **How much does each feature contribute individually?** (Leave-One-Covariate-In)
- **How do features depend on each other?** (Conditional and Relative methods)

All methods share a common interface built on [mlr3](https://mlr3.mlr-org.com/), making them easy to use with any task, learner, measure, and resampling strategy.

## Basic Example

Let's use the Friedman1 task, which provides an ideal setup for demonstrating feature importance methods with known ground truth:

```{r setup-problem}
task <- tgen("friedman1")$generate(n = 300)
learner <- lrn("regr.ranger", num.trees = 100)
measure <- msr("regr.mse")
resampling <- rsmp("cv", folds = 3)
```

The task has `r task$nrow` observations with `r length(task$feature_names)` features. Features `important1` through `important5` truly affect the target, while `unimportant1` through `unimportant5` are pure noise. We'll use a random forest learner with cross-validation for more stable estimates.

The target function is: $y = 10 * \operatorname{sin}(\pi * x_1 * x_2) + 20 * (x_3 - 0.5)^2 + 10 * x_4 + 5 * x_5 + \epsilon$


## Permutation Feature Importance (PFI)

PFI is the most straightforward method: for each feature, we permute (shuffle) its values and measure how much model performance deteriorates. More important features cause larger performance drops when shuffled.

```{r pfi-basic}
pfi <- PFI$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling
)

pfi_results <- pfi$compute()
pfi_results
```

The `importance` column shows the performance difference when each feature is permuted. Higher values indicate more important features.

For more stable estimates, we can use multiple permutation iterations per resampling fold:

```{r pfi-parameters}
pfi_stable <- PFI$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling,
  iters_perm = 5
)

pfi_stable$compute()
```

We can also use ratio instead of difference for the importance calculation:

```{r pfi-ratio}
pfi_stable$compute(relation = "ratio")
```


## Feature Samplers

For advanced methods that account for feature dependencies, xplainfi provides different sampling strategies. While PFI uses simple permutation (marginal sampling), conditional samplers can preserve feature relationships.

Let's demonstrate conditional sampling using Adversarial Random Forests, which preserves relationships between features when sampling:

```{r samplers-demo}
arf_sampler <- ARFSampler$new(task)

sample_data <- task$data(rows = 1:5)
sample_data$important1
```

Now we'll conditionally sample the `important1` feature given the values of `important2` and `important3`:

```{r conditional-sampling}
sampled_conditional <- arf_sampler$sample(
  feature = "important1", 
  data = sample_data,
  conditioning_features = c("important2", "important3")
)

sampled_conditional$important1
```

Original values: `r paste(round(sample_data$important1, 3), collapse = ", ")`

Conditionally sampled values: `r paste(round(sampled_conditional$important1, 3), collapse = ", ")`

Note how the sampled values respect the conditional distribution given the values of important2 and important3.

This conditional sampling is essential for methods like CFI and RFI that need to preserve feature dependencies. See `vignette("perturbation-importance")` for detailed comparisons.

## When to Use Each Method

**Choose PFI when:**
- You want fast, simple importance estimates
- Features are relatively independent  
- You need a quick baseline assessment

**Choose LOCO when:**
- You want the most accurate importance estimates
- Computational cost is not a major concern
- Features may be correlated
- You want to understand the impact of completely removing features

**Choose LOCI when:**
- You want to understand individual feature contributions
- You're interested in feature selection (negative LOCI scores suggest features to remove)
- You want to compare features to a meaningful baseline

## Advanced Features

xplainfi supports many advanced features for robust importance estimation:

- **Multiple resampling strategies**: Cross-validation, bootstrap, custom splits
- **Multiple permutation/refit iterations**: For more stable estimates
- **Feature grouping**: Compute importance for groups of related features
- **Different relation types**: Difference vs. ratio scoring
- **Conditional sampling**: Account for feature dependencies (see `vignette("perturbation-importance")`)
- **SAGE methods**: Shapley-based approaches (see `vignette("sage-methods")`)

## Detailed Scoring Information

All methods store detailed scoring information for further analysis. Let's examine the structure of PFI's detailed scores:

```{r detailed-scores}
head(pfi$scores, 10) |>
  knitr::kable(digits = 4, caption = "Detailed PFI scores (first 10 rows)")
```

We can also summarize the scoring structure:

```{r scoring-summary}
pfi$scores[, .(
  features = uniqueN(feature),
  resampling_folds = uniqueN(iter_rsmp), 
  permutation_iters = uniqueN(iter_perm),
  total_scores = .N
)]
```

## Next Steps

This vignette covered the basics of feature importance with xplainfi. For more advanced usage:

- **`vignette("perturbation-importance")`**: Deep dive into PFI, CFI, and RFI methods with conditional sampling
- **`vignette("loco-loci")`**: Detailed examples of LOCO and LOCI methods  
- **`vignette("sage-methods")`**: SAGE-based Shapley value methods

The package documentation (`?PFI`, `?LOCO`, `?LOCI`) provides complete API references.
