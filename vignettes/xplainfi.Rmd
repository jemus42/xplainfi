---
title: "Getting Started with xplainfi"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with xplainfi}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
set.seed(123)
# Quiet down
lgr::get_logger("mlr3")$set_threshold("warn")
```

```{r setup}
library(xplainfi)
library(mlr3)
library(mlr3learners)
library(data.table)
library(ggplot2)
```

The **xplainfi** package provides feature importance methods for machine learning models. It implements several approaches for measuring how much each feature contributes to model predictions, with a focus on model-agnostic methods that work with any learner.

## Core Concepts

Feature importance methods in xplainfi answer different but related questions:

- **How much does each feature contribute to model performance?** (Permutation Feature Importance)
- **What happens when we remove features and retrain?** (Leave-One-Covariate-Out)  
- **How much does each feature contribute individually?** (Leave-One-Covariate-In)
- **How do features depend on each other?** (Conditional and Relative methods)

All methods share a common interface built on [mlr3](https://mlr3.mlr-org.com/), making them easy to use with any task, learner, measure, and resampling strategy.

## Basic Example

Let's use the Friedman1 task, which provides an ideal setup for demonstrating feature importance methods with known ground truth:

```{r setup-problem}
task <- tgen("friedman1")$generate(n = 300)
learner <- lrn("regr.ranger", num.trees = 100)
measure <- msr("regr.mse")
resampling <- rsmp("cv", folds = 3)
```

The task has `r task$nrow` observations with `r length(task$feature_names)` features. Features `important1` through `important5` truly affect the target, while `unimportant1` through `unimportant5` are pure noise. We'll use a random forest learner with cross-validation for more stable estimates.

The target function is: $y = 10 * \operatorname{sin}(\pi * x_1 * x_2) + 20 * (x_3 - 0.5)^2 + 10 * x_4 + 5 * x_5 + \epsilon$


## Permutation Feature Importance (PFI)

PFI is the most straightforward method: for each feature, we permute (shuffle) its values and measure how much model performance deteriorates. More important features cause larger performance drops when shuffled.

```{r pfi-basic}
pfi <- PFI$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling
)

pfi_results <- pfi$compute()
pfi_results
```

The `importance` column shows the performance difference when each feature is permuted. Higher values indicate more important features.

For more stable estimates, we can use multiple permutation iterations per resampling fold:

```{r pfi-parameters}
pfi_stable <- PFI$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling,
  iters_perm = 5
)

pfi_stable$compute()
```

We can also use ratio instead of difference for the importance calculation:

```{r pfi-ratio}
pfi_stable$compute(relation = "ratio")
```

## Leave-One-Covariate-Out (LOCO)

LOCO measures importance by retraining the model without each feature and comparing performance to the full model. This shows the contribution of each feature when all other features are present.

```{r loco-basic}
loco <- LOCO$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling
)

loco_results <- loco$compute()
loco_results
```

LOCO is computationally expensive (requires retraining for each feature) but provides clear interpretation: higher values mean larger performance drop when the feature is removed. **Important limitation**: LOCO cannot distinguish between direct effects and indirect effects through correlated features.

## Leave-One-Covariate-In (LOCI)

LOCI takes the opposite approach: it trains models with only single features and compares to a naive baseline (featureless learner that predicts the optimal constant).

```{r loci-basic}
loci <- LOCI$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling
)

loci_results <- loci$compute()
loci_results
```

LOCI measures the intrinsic predictive power of each feature in isolation. Positive values indicate the feature alone performs better than constant prediction; negative values suggest the feature is harmful when used alone. **Key insight**: LOCI ignores all feature interactions and synergies.

## Feature Samplers

For advanced methods that account for feature dependencies, xplainfi provides different sampling strategies. While PFI uses simple permutation (marginal sampling), conditional samplers can preserve feature relationships.

Let's demonstrate conditional sampling using Adversarial Random Forests, which preserves relationships between features when sampling:

```{r samplers-demo}
arf_sampler <- ARFSampler$new(task)

sample_data <- task$data(rows = 1:5)
sample_data[, .(y, important1, important2)]
```

Now we'll conditionally sample the `important1` feature given the values of `important2` and `important3`:

```{r conditional-sampling}
sampled_conditional <- arf_sampler$sample(
  feature = "important1", 
  data = sample_data,
  conditioning_set = c("important2", "important3")
)

sample_data[, .(y, important1, important2, important3)]
sampled_conditional[, .(y, important1, important2, important3)]
```

This conditional sampling is essential for methods like CFI and RFI that need to preserve feature dependencies. See `vignette("perturbation-importance")` for detailed comparisons.

## When to Use Each Method

**Choose PFI when:**

- You want fast, simple importance estimates
- Features are relatively independent  
- You need a quick baseline assessment
- You accept that feature correlations are ignored

**Choose LOCO when:**

- You want to understand the impact of completely removing features
- Computational cost is not a major concern
- You need interpretable importance relative to the full model
- Note: LOCO cannot account for feature correlations/dependencies

**Choose LOCI when:**

- You want to understand individual feature contributions in isolation
- You're interested in feature selection (negative LOCI scores suggest harmful features)
- You want to compare features to a naive baseline (optimal constant prediction)
- Note: LOCI measures intrinsic predictive power, not interactions with other features

**Important distinction**: Neither LOCO nor LOCI can account for conditional dependencies between features. For that, you need methods like CFI (Conditional Feature Importance) or RFI (Relative Feature Importance) that use conditional sampling. See `vignette("perturbation-importance")` for these advanced methods.

## Advanced Features

xplainfi supports many advanced features for robust importance estimation:

- **Multiple resampling strategies**: Cross-validation, bootstrap, custom splits
- **Multiple permutation/refit iterations**: For more stable estimates
- **Feature grouping**: Compute importance for groups of related features
- **Different relation types**: Difference vs. ratio scoring
- **Conditional sampling**: Account for feature dependencies (see `vignette("perturbation-importance")`)
- **SAGE methods**: Shapley-based approaches (see `vignette("sage-methods")`)

## Detailed Scoring Information

All methods store detailed scoring information for further analysis. Let's examine the structure of PFI's detailed scores:

```{r detailed-scores}
head(pfi$scores, 10) |>
  knitr::kable(digits = 4, caption = "Detailed PFI scores (first 10 rows)")
```

We can also summarize the scoring structure:

```{r scoring-summary}
pfi$scores[, .(
  features = uniqueN(feature),
  resampling_folds = uniqueN(iter_rsmp), 
  permutation_iters = uniqueN(iter_perm),
  total_scores = .N
)]
```

## Next Steps

This vignette covered the basics of feature importance with xplainfi. For more advanced usage:

- **`vignette("perturbation-importance")`**: Deep dive into PFI, CFI, and RFI methods with conditional sampling
- **`vignette("loco-loci")`**: Detailed examples of LOCO and LOCI methods  
- **`vignette("sage-methods")`**: SAGE-based Shapley value methods

The package documentation (`?PFI`, `?LOCO`, `?LOCI`) provides complete API references.
