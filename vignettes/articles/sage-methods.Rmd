---
title: "Shapley Additive Global Importance (SAGE)"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
	collapse = TRUE,
	comment = "#>",
	fig.width = 8,
	fig.height = 6
)
set.seed(123)
options("xplain.progress" = interactive())
```

```{r setup}
library(xplainfi)
library(mlr3)
library(mlr3learners)
library(ggplot2)
```

## Introduction

Shapley Additive Global Importance (SAGE) is a feature importance method based on cooperative game theory that uses Shapley values to fairly distribute the total prediction performance among all features. Unlike permutation-based methods that measure the drop in performance when features are perturbed, SAGE measures how much each feature contributes to the model's overall performance by marginalizing (removing) features.

The key insight of SAGE is that it provides a complete decomposition of the model's performance: the sum of all SAGE values equals the difference between the model's performance and the performance when all features are marginalized.

**xplainfi** provides two implementations of SAGE:

- **MarginalSAGE**: Marginalizes features independently (standard SAGE)
- **ConditionalSAGE**: Marginalizes features conditionally using ARF sampling

## Demonstration with Correlated Features

To showcase the difference between Marginal and Conditional SAGE, we'll use the `sim_dgp_correlated()` function which creates highly correlated features. This is similar to how PFI and CFI behave differently with correlated features.

**Model:**
$$X_1 \sim N(0,1)$$
$$X_2 = X_1 + \varepsilon_2, \quad \varepsilon_2 \sim N(0, 0.05^2)$$
$$X_3 \sim N(0,1), \quad X_4 \sim N(0,1)$$
$$Y = 2 \cdot X_1 + X_3 + \varepsilon$$

where $\varepsilon \sim N(0, 0.2^2)$.

**Key properties:**

- `x1` has a direct causal effect on y (β=2.0)
- `x2` is highly correlated with x1 (r ≈ 0.999) but has **no causal effect** on y
- `x3` is independent with a causal effect (β=1.0)
- `x4` is independent noise (β=0)

```{r data-setup}
set.seed(123)
task = sim_dgp_correlated(n = 1000)

# Check the correlation structure
task_data = task$data()
correlation_matrix = cor(task_data[, c("x1", "x2", "x3", "x4")])
round(correlation_matrix, 3)
```

**Expected behavior:**

- **Marginal SAGE**: Should show high importance for both x1 and x2 due to their correlation, even though x2 has no causal effect
- **Conditional SAGE**: Should show high importance for x1 but near-zero importance for x2 (correctly identifying the spurious predictor)

Let's set up our learner and measure. We'll use a random forest and instantiate a resampling to ensure both methods see the same data:

```{r learner-setup}
learner = lrn("regr.ranger")
measure = msr("regr.mse")
resampling = rsmp("holdout")
resampling$instantiate(task)
```

## Marginal SAGE

Marginal SAGE marginalizes features independently by averaging predictions over a reference dataset. This is the standard SAGE implementation described in the original paper.

```{r marginal-sage}
# Create Marginal SAGE instance
marginal_sage = MarginalSAGE$new(
	task = task,
	learner = learner,
	measure = measure,
	resampling = resampling,
	n_permutations = 30L, # More permutations for stable results
	max_reference_size = 100L,
	batch_size = 5000L
)

# Compute SAGE values
marginal_sage$compute()
```

Let's visualize the results:

```{r marginal-sage-plot}
#| echo: false
# Extract importance scores
marginal_results = marginal_sage$importance()
marginal_results$method = "Marginal SAGE"

# Create a factor with proper ordering
marginal_results$feature = factor(
	marginal_results$feature,
	levels = marginal_results$feature[order(marginal_results$importance, decreasing = TRUE)]
)

# Color features by type (causal vs noise)
marginal_results$feature_type = ifelse(
	marginal_results$feature %in% c("x1", "x3"),
	"Causal",
	"Noise"
)

# Create bar plot
ggplot(marginal_results, aes(x = feature, y = importance)) +
	geom_col(aes(fill = feature_type), alpha = 0.8) +
	scale_fill_manual(
		values = c("Causal" = "steelblue", "Noise" = "lightcoral"),
		name = "Feature type"
	) +
	labs(
		title = "Marginal SAGE Feature Importance",
		subtitle = "Correlated features: x1 and x2 are highly correlated (r ≈ 0.999)\nx2 has no effect on y",
		x = "Features",
		y = "SAGE Value"
	) +
	theme_minimal(base_size = 14)
```

We can also keep track of the SAGE value approximation across permutations:

```{r convergance-marginal}
marginal_sage$plot_convergence()
```

## Conditional SAGE

Conditional SAGE uses conditional sampling (via ARF by default) to marginalize features while preserving dependencies between the remaining features. This can provide different insights, especially when features are correlated.

```{r conditional-sage}
# Create Conditional SAGE instance
conditional_sage = ConditionalSAGE$new(
	task = task,
	learner = learner,
	measure = measure,
	resampling = resampling,
	n_permutations = 30L,
	max_reference_size = 100L
)

# Compute SAGE values
conditional_sage$compute(batch_size = 5000L)
```

Let's visualize the conditional SAGE results:

```{r conditional-sage-plot}
#| echo: false

# Extract importance scores
conditional_results = conditional_sage$importance()
conditional_results$method = "Conditional SAGE"

# Create a factor with proper ordering
conditional_results$feature = factor(
	conditional_results$feature,
	levels = conditional_results$feature[order(conditional_results$importance, decreasing = TRUE)]
)

# Color features by type (causal vs noise)
conditional_results$feature_type = ifelse(
	conditional_results$feature %in% c("x1", "x3"),
	"Causal",
	"Noise"
)

# Create bar plot
ggplot(conditional_results, aes(x = feature, y = importance)) +
	geom_col(aes(fill = feature_type), alpha = 0.8) +
	scale_fill_manual(
		values = c("Causal" = "steelblue", "Noise" = "lightcoral"),
		name = "Feature type"
	) +
	labs(
		title = "Conditional SAGE Feature Importance",
		subtitle = "Conditional sampling should reduce importance of redundant correlated features",
		x = "Features",
		y = "SAGE Value"
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "bottom")
```


```{r convergance-conditional}
conditional_sage$plot_convergence()
```

## Comparison of Methods

Let's compare the two SAGE methods side by side:

```{r comparison}
#| echo: false

# Ensure both datasets have feature_type for consistency
marginal_results$feature_type = ifelse(
	marginal_results$feature %in% c("x1", "x3"),
	"Causal",
	"Noise"
)

# Combine results
combined_results = rbind(marginal_results, conditional_results)

# Create comparison plot
ggplot(combined_results, aes(x = feature, y = importance, fill = method)) +
	facet_wrap(~feature_type, scales = "free_x") +
	geom_col(position = "dodge", alpha = 0.8) +
	scale_fill_manual(values = c("Marginal SAGE" = "steelblue", "Conditional SAGE" = "darkgreen")) +
	labs(
		title = "Marginal vs Conditional SAGE Comparison",
		x = "Features",
		y = "SAGE Value",
		fill = "Method"
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "bottom")
```

Let's also create a correlation plot to see how similar the rankings are:

```{r correlation-plot}
#| echo: false

# Merge the two results for correlation analysis
merged_results = merge(
	marginal_results[, c("feature", "importance", "feature_type")],
	conditional_results[, c("feature", "importance")],
	by = "feature",
	suffixes = c("_marginal", "_conditional")
)

# Calculate correlation
correlation = cor(merged_results$importance_marginal, merged_results$importance_conditional)

# Create scatter plot
ggplot(merged_results, aes(x = importance_marginal, y = importance_conditional)) +
	geom_point(aes(color = feature_type), size = 3, alpha = 0.8) +
	geom_smooth(method = "lm", se = FALSE, color = "gray50", linetype = "dashed") +
	scale_color_manual(
		values = c("Causal" = "steelblue", "Noise" = "lightcoral"),
		name = "Feature type"
	) +
	labs(
		title = "Marginal vs Conditional SAGE Correlation",
		subtitle = sprintf("Pearson correlation: %.3f", correlation),
		x = "Marginal SAGE Value",
		y = "Conditional SAGE Value"
	) +
	theme_minimal(base_size = 14) +
	geom_text(aes(label = feature), hjust = 0, vjust = -0.5, size = 3)
```

### Interpretation

The results demonstrate the key difference between marginal and conditional SAGE:

1. **Marginal SAGE** treats each feature independently, so highly correlated features x1 and x2 both receive substantial importance scores reflecting their individual marginal contributions.

2. **Conditional SAGE** accounts for feature dependencies through conditional sampling. When marginalizing x1, it properly conditions on x2 (and vice versa), leading to lower importance scores for the correlated features since they provide redundant information.

3. **Independent feature x3** shows similar importance in both methods since it doesn't depend on other features.

4. **Noise feature x4** correctly receives near-zero importance in both methods.

This pattern mirrors the difference between PFI and CFI: marginal methods show inflated importance for correlated features, while conditional methods provide a more accurate assessment of each feature's unique contribution.

## Comparison with PFI and CFI

For reference, let's compare SAGE methods with the analogous PFI and CFI methods on the same data:

```{r pfi-cfi-comparison}
# Quick PFI and CFI comparison for context
pfi = PFI$new(task, learner, measure)
cfi = CFI$new(task, learner, measure)

pfi$compute()
cfi$compute()
pfi_results = pfi$importance()
cfi_results = cfi$importance()

# Create comparison data frame
method_comparison = data.frame(
	feature = rep(c("x1", "x2", "x3", "x4"), 4),
	importance = c(
		pfi_results$importance,
		cfi_results$importance,
		marginal_results$importance,
		conditional_results$importance
	),
	method = rep(c("PFI", "CFI", "Marginal SAGE", "Conditional SAGE"), each = 4),
	approach = rep(c("Marginal", "Conditional", "Marginal", "Conditional"), each = 4)
)

# Create comparison plot
ggplot(method_comparison, aes(x = feature, y = importance, fill = method)) +
	geom_col(position = "dodge", alpha = 0.8) +
	scale_fill_manual(
		values = c(
			"PFI" = "lightblue",
			"CFI" = "blue",
			"Marginal SAGE" = "lightcoral",
			"Conditional SAGE" = "darkred"
		)
	) +
	labs(
		title = "Comparison: PFI/CFI vs Marginal/Conditional SAGE",
		subtitle = "Both pairs show similar patterns: marginal methods inflate correlated feature importance",
		x = "Features",
		y = "Importance Value",
		fill = "Method"
	) +
	theme_minimal(base_size = 14) +
	theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Key Observations:**

- **Marginal methods** (PFI, Marginal SAGE) both assign high importance to correlated features x1 and x2
- **Conditional methods** (CFI, Conditional SAGE) both reduce importance for correlated features, accounting for redundancy
- **Independent feature x3** receives consistent importance across all methods
- **Noise feature x4** is correctly identified as unimportant by all methods

This demonstrates that the marginal vs conditional distinction is a fundamental concept that applies across different importance method families.
