---
title: "Shapley Additive Global Importance (SAGE)"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
set.seed(123)
```

```{r setup}
library(xplainfi)
library(mlr3)
library(mlr3learners)
library(ggplot2)
```

## Introduction

Shapley Additive Global Importance (SAGE) is a feature importance method based on cooperative game theory that uses Shapley values to fairly distribute the total prediction performance among all features. Unlike permutation-based methods that measure the drop in performance when features are perturbed, SAGE measures how much each feature contributes to the model's overall performance by marginalizing (removing) features.

The key insight of SAGE is that it provides a complete decomposition of the model's performance: the sum of all SAGE values equals the difference between the model's performance and the performance when all features are marginalized.

**xplainfi** provides two implementations of SAGE:

- **MarginalSAGE**: Marginalizes features independently (standard SAGE)
- **ConditionalSAGE**: Marginalizes features conditionally using ARF sampling

## Demonstration with Correlated Features

To showcase the difference between Marginal and Conditional SAGE, we'll use the `sim_dgp_correlated()` function which creates highly correlated features. This is similar to how PFI and CFI behave differently with correlated features.

**Model:**
$$X_1 \sim N(0,1)$$
$$X_2 = X_1 + \varepsilon_2, \quad \varepsilon_2 \sim N(0, 0.05^2)$$
$$X_3 \sim N(0,1), \quad X_4 \sim N(0,1)$$
$$Y = 2 \cdot X_1 + X_3 + \varepsilon$$

where $\varepsilon \sim N(0, 0.2^2)$.

**Key properties:**

- `x1` has a direct causal effect on y (β=2.0)
- `x2` is highly correlated with x1 (r ≈ 0.999) but has **no causal effect** on y
- `x3` is independent with a causal effect (β=1.0)
- `x4` is independent noise (β=0)

```{r data-setup}
set.seed(123)
task = sim_dgp_correlated(n = 800)

# Check the correlation structure
task_data = task$data()
correlation_matrix = cor(task_data[, c("x1", "x2", "x3", "x4")])
round(correlation_matrix, 3)
```

**Expected behavior:**

- **Marginal SAGE**: Should show high importance for both x1 and x2 due to their correlation, even though x2 has no causal effect
- **Conditional SAGE**: Should show high importance for x1 but near-zero importance for x2 (correctly identifying the spurious predictor)

Let's set up our learner and measure. We'll use a random forest and instantiate a resampling to ensure both methods see the same data:

```{r learner-setup}
learner = lrn("regr.ranger", num.trees = 400)
measure = msr("regr.mse")
resampling = rsmp("holdout")
resampling$instantiate(task)
```

## Marginal SAGE

Marginal SAGE marginalizes features independently by averaging predictions over a reference dataset. This is the standard SAGE implementation described in the original paper.

```{r marginal-sage}
# Create Marginal SAGE instance
marginal_sage = MarginalSAGE$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling,
  n_permutations = 30L,  # More permutations for stable results
  max_reference_size = 100L
)

# Compute SAGE values
marginal_sage$compute(batch_size = 500L)
```

Let's visualize the results:

```{r marginal-sage-plot}
#| echo: false
# Extract importance scores
marginal_results = marginal_sage$importance
marginal_results$method = "Marginal SAGE"

# Create a factor with proper ordering
marginal_results$feature = factor(
  marginal_results$feature,
  levels = marginal_results$feature[order(marginal_results$importance, decreasing = TRUE)]
)

# Color features by type (causal vs noise)
marginal_results$feature_type = ifelse(
  marginal_results$feature %in% c("x1", "x2", "x3"), 
  "Causal", 
  "Noise"
)

# Create bar plot
ggplot(marginal_results, aes(x = feature, y = importance)) +
  geom_col(aes(fill = feature_type), alpha = 0.8) +
  scale_fill_manual(
    values = c("Causal" = "steelblue", "Noise" = "lightcoral"),
    name = "Feature type"
  ) +
  labs(
    title = "Marginal SAGE Feature Importance",
    subtitle = "Correlated features: x1 and x2 are highly correlated (r ≈ 0.999)",
    x = "Features", 
    y = "SAGE Value"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Conditional SAGE

Conditional SAGE uses conditional sampling (via ARF by default) to marginalize features while preserving dependencies between the remaining features. This can provide different insights, especially when features are correlated.

```{r conditional-sage}
# Create Conditional SAGE instance
conditional_sage = ConditionalSAGE$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling,
  n_permutations = 30L,
  max_reference_size = 100L
)

# Compute SAGE values
conditional_sage$compute()
```

Let's visualize the conditional SAGE results:

```{r conditional-sage-plot}
#| echo: false

# Extract importance scores
conditional_results = conditional_sage$importance
conditional_results$method = "Conditional SAGE"

# Create a factor with proper ordering
conditional_results$feature = factor(
  conditional_results$feature,
  levels = conditional_results$feature[order(conditional_results$importance, decreasing = TRUE)]
)

# Color features by type (causal vs noise)
conditional_results$feature_type = ifelse(
  conditional_results$feature %in% c("x1", "x2", "x3"), 
  "Causal", 
  "Noise"
)

# Create bar plot
ggplot(conditional_results, aes(x = feature, y = importance)) +
  geom_col(aes(fill = feature_type), alpha = 0.8) +
  scale_fill_manual(
    values = c("Causal" = "darkgreen", "Noise" = "lightcoral"),
    name = "Feature type"
  ) +
  labs(
    title = "Conditional SAGE Feature Importance",
    subtitle = "Conditional sampling should reduce importance of redundant correlated features",
    x = "Features", 
    y = "SAGE Value"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Comparison of Methods

Let's compare the two SAGE methods side by side:

```{r comparison}
#| echo: false

# Ensure both datasets have feature_type for consistency
marginal_results$feature_type = ifelse(
  marginal_results$feature %in% c("x1", "x2", "x3"), 
  "Causal", 
  "Noise"
)

# Combine results
combined_results = rbind(marginal_results, conditional_results)

# Create comparison plot
ggplot(combined_results, aes(x = feature, y = importance, fill = method)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c("Marginal SAGE" = "steelblue", "Conditional SAGE" = "darkgreen")) +
  labs(
    title = "Marginal vs Conditional SAGE Comparison",
    subtitle = "Key difference: How x1 and x2 (correlated features) are valued",
    x = "Features", 
    y = "SAGE Value",
    fill = "Method"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~ feature_type, 
             labeller = labeller(feature_type = c("Causal" = "Causal Features", "Noise" = "Noise Features")),
             scales = "free_x")
```

Let's also create a correlation plot to see how similar the rankings are:

```{r correlation-plot}
#| echo: false

# Merge the two results for correlation analysis
merged_results = merge(
  marginal_results[, c("feature", "importance", "feature_type")], 
  conditional_results[, c("feature", "importance")], 
  by = "feature", 
  suffixes = c("_marginal", "_conditional")
)

# Calculate correlation
correlation = cor(merged_results$importance_marginal, merged_results$importance_conditional)

# Create scatter plot
ggplot(merged_results, aes(x = importance_marginal, y = importance_conditional)) +
  geom_point(aes(color = feature_type), size = 3, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE, color = "gray50", linetype = "dashed") +
  scale_color_manual(
    values = c("Causal" = "steelblue", "Noise" = "lightcoral"),
    name = "Feature type"
  ) +
  labs(
    title = "Marginal vs Conditional SAGE Correlation",
    subtitle = sprintf("Pearson correlation: %.3f", correlation),
    x = "Marginal SAGE Value",
    y = "Conditional SAGE Value"
  ) +
  theme_minimal(base_size = 14) +
  geom_text(aes(label = feature), hjust = 0, vjust = -0.5, size = 3)
```

## Understanding the Results

The visualization and correlation analysis above demonstrate the key differences between Marginal and Conditional SAGE. The correlation plot shows the overall relationship between the two methods, while the side-by-side comparison highlights specific patterns for different feature types.

### Interpretation

The results demonstrate the key difference between marginal and conditional SAGE:

1. **Marginal SAGE** treats each feature independently, so highly correlated features x1 and x2 both receive substantial importance scores reflecting their individual marginal contributions.

2. **Conditional SAGE** accounts for feature dependencies through conditional sampling. When marginalizing x1, it properly conditions on x2 (and vice versa), leading to lower importance scores for the correlated features since they provide redundant information.

3. **Independent feature x3** shows similar importance in both methods since it doesn't depend on other features.

4. **Noise feature x4** correctly receives near-zero importance in both methods.

This pattern mirrors the difference between PFI and CFI: marginal methods show inflated importance for correlated features, while conditional methods provide a more accurate assessment of each feature's unique contribution.

## Comparison with PFI and CFI

For reference, let's compare SAGE methods with the analogous PFI and CFI methods on the same data:

```{r pfi-cfi-comparison}
# Quick PFI and CFI comparison for context
pfi = PFI$new(task, learner, measure)
cfi = CFI$new(task, learner, measure) 

pfi_results = pfi$compute()
cfi_results = cfi$compute()

# Create comparison data frame
method_comparison = data.frame(
  feature = rep(c("x1", "x2", "x3", "x4"), 4),
  importance = c(
    pfi_results$importance,
    cfi_results$importance,
    marginal_results$importance,
    conditional_results$importance
  ),
  method = rep(c("PFI", "CFI", "Marginal SAGE", "Conditional SAGE"), each = 4),
  approach = rep(c("Marginal", "Conditional", "Marginal", "Conditional"), each = 4)
)

# Create comparison plot
ggplot(method_comparison, aes(x = feature, y = importance, fill = method)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c(
    "PFI" = "lightblue", 
    "CFI" = "blue", 
    "Marginal SAGE" = "lightcoral", 
    "Conditional SAGE" = "darkred"
  )) +
  labs(
    title = "Comparison: PFI/CFI vs Marginal/Conditional SAGE",
    subtitle = "Both pairs show similar patterns: marginal methods inflate correlated feature importance",
    x = "Features", 
    y = "Importance Value",
    fill = "Method"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Key Observations:**

- **Marginal methods** (PFI, Marginal SAGE) both assign high importance to correlated features x1 and x2
- **Conditional methods** (CFI, Conditional SAGE) both reduce importance for correlated features, accounting for redundancy
- **Independent feature x3** receives consistent importance across all methods
- **Noise feature x4** is correctly identified as unimportant by all methods

This demonstrates that the marginal vs conditional distinction is a fundamental concept that applies across different importance method families.

## Example 2: Confounding Scenario

Let's explore another scenario where marginal and conditional SAGE differ: confounding. We'll use the `sim_dgp_confounded()` function which includes a hidden confounder that affects both features and the outcome.

**Mathematical Model:**
$$H \sim N(0,1) \quad \text{(hidden confounder)}$$
$$X_1 = H + \varepsilon_1, \quad X_2 = H + \varepsilon_2$$
$$\text{proxy} = H + \varepsilon_p, \quad \text{independent} \sim N(0,1)$$
$$Y = H + 0.5 \cdot X_1 + 0.5 \cdot X_2 + \text{independent} + \varepsilon$$

where all $\varepsilon \sim N(0, 0.5^2)$ independently.

**Key properties:**
- Hidden confounder H affects x1, x2, and y
- x1 and x2 have both direct causal effects (β=0.5 each) and confounded effects (through H)
- proxy is an observable (but noisy) measurement of the confounder
- independent is truly unaffected by confounding

```{r confounding-setup}
set.seed(456)  # Different seed for variety
task_confounded = sim_dgp_confounded(n = 800, hidden = TRUE)

# Check the structure
conf_data = task_confounded$data()
conf_correlation = cor(conf_data[, c("x1", "x2", "proxy", "independent")])
print("Feature correlations (confounding scenario):")
print(round(conf_correlation, 3))
```

**Expected behavior:**

- **Marginal SAGE**: Will show inflated importance for x1 and x2 due to confounding bias
- **Conditional SAGE**: Should partially account for confounding through conditional sampling

```{r confounding-sage}
# Create SAGE instances for confounding scenario  
marginal_sage_conf = MarginalSAGE$new(
  task = task_confounded,
  learner = learner,
  measure = measure,
  n_permutations = 30L,
  max_reference_size = 100L
)

conditional_sage_conf = ConditionalSAGE$new(
  task = task_confounded,
  learner = learner,
  measure = measure,
  n_permutations = 30L,
  max_reference_size = 100L
)

# Compute SAGE values
marginal_conf_results = marginal_sage_conf$compute()
conditional_conf_results = conditional_sage_conf$compute()

marginal_conf_results
conditional_conf_results
```

```{r confounding-visualization}
# Prepare data for visualization
marginal_conf_results$method = "Marginal SAGE"
conditional_conf_results$method = "Conditional SAGE"

marginal_conf_results$feature_type = ifelse(
  marginal_conf_results$feature %in% c("x1", "x2"), "Confounded",
  ifelse(marginal_conf_results$feature == "independent", "Independent", "Proxy")
)

conditional_conf_results$feature_type = ifelse(
  conditional_conf_results$feature %in% c("x1", "x2"), "Confounded", 
  ifelse(conditional_conf_results$feature == "independent", "Independent", "Proxy")
)

# Combine results
combined_conf_results = rbind(marginal_conf_results, conditional_conf_results)

# Create comparison plot
ggplot(combined_conf_results, aes(x = feature, y = importance, fill = method)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c("Marginal SAGE" = "orange", "Conditional SAGE" = "purple")) +
  labs(
    title = "Marginal vs Conditional SAGE: Confounding Scenario",
    subtitle = "x1 and x2 are confounded by hidden variable H",
    x = "Features", 
    y = "SAGE Value",
    fill = "Method"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~ feature_type, scales = "free_x")
```

```{r confounding-analysis}
# Calculate specific metrics for confounding scenario
x1_marg_conf = round(marginal_conf_results$importance[marginal_conf_results$feature == "x1"], 4)
x1_cond_conf = round(conditional_conf_results$importance[conditional_conf_results$feature == "x1"], 4)
x2_marg_conf = round(marginal_conf_results$importance[marginal_conf_results$feature == "x2"], 4)
x2_cond_conf = round(conditional_conf_results$importance[conditional_conf_results$feature == "x2"], 4)
indep_marg_conf = round(marginal_conf_results$importance[marginal_conf_results$feature == "independent"], 4)
indep_cond_conf = round(conditional_conf_results$importance[conditional_conf_results$feature == "independent"], 4)
proxy_marg_conf = round(marginal_conf_results$importance[marginal_conf_results$feature == "proxy"], 4)
proxy_cond_conf = round(conditional_conf_results$importance[conditional_conf_results$feature == "proxy"], 4)

confounded_bias_reduction = round(mean(c(x1_marg_conf, x2_marg_conf)) - mean(c(x1_cond_conf, x2_cond_conf)), 4)
```

### Confounding Results Analysis

**Individual Feature Analysis (Confounding)**:
- **x1** (confounded, β=0.5): Marginal = `r x1_marg_conf` vs Conditional = `r x1_cond_conf`
- **x2** (confounded, β=0.5): Marginal = `r x2_marg_conf` vs Conditional = `r x2_cond_conf`
- **independent** (unconfounded, β=1.0): Marginal = `r indep_marg_conf` vs Conditional = `r indep_cond_conf`
- **proxy** (confounder proxy, β=0): Marginal = `r proxy_marg_conf` vs Conditional = `r proxy_cond_conf`

**Key Insights:**
- **Bias reduction**: Conditional SAGE reduces confounding bias by `r confounded_bias_reduction` on average for confounded features
- **Independent feature**: Shows similar importance in both methods (`r indep_marg_conf` vs `r indep_cond_conf`)
- **Proxy variable**: May show some importance in marginal SAGE due to its correlation with confounded variables

### Interpretation: Confounding vs Correlation

The confounding example demonstrates a different mechanism than the correlation example:

1. **Correlation example**: Features are redundant due to high correlation
   - Conditional SAGE reduces importance because features provide overlapping information
   
2. **Confounding example**: Features appear more important due to spurious associations
   - Conditional SAGE reduces importance by accounting for confounding pathways
   - The true causal effect (β=0.5 each) is masked by the confounding bias

Both scenarios show conditional SAGE providing more accurate importance estimates, but through different mechanisms: **redundancy reduction** vs **bias correction**.

## Summary and Recommendations

This article demonstrated two key scenarios where Marginal and Conditional SAGE provide different insights:

### When to Use Each Method

**Marginal SAGE** is appropriate when:
- Features are mostly independent
- You want to measure marginal contributions (how much each feature matters individually)
- Computational efficiency is a priority
- You're working with the standard SAGE interpretation from the literature

**Conditional SAGE** is recommended when:
- Features are correlated or have complex dependencies
- You suspect confounding relationships
- You want to measure conditional contributions (how much each feature matters given other features)
- You need more accurate importance estimates in the presence of feature interactions

### Key Takeaways

1. **Parallel to PFI/CFI**: The marginal vs conditional distinction in SAGE mirrors the well-established difference between PFI and CFI
2. **Different mechanisms**: Conditional sampling addresses both redundancy (correlation) and bias (confounding)
3. **Complementary insights**: Both methods provide valuable but different perspectives on feature importance
4. **Context matters**: The choice depends on your research question and data structure

The implementation in **xplainfi** makes it easy to compute both variants and compare their results, providing a comprehensive view of feature importance that accounts for different types of feature relationships.
