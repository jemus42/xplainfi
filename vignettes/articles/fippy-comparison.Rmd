---
title: "Comparison with fippy (Python Implementation)"
subtitle: "Cross-language validation of feature importance methods"
editor_options: 
  chunk_output_type: console
---

```{r setup, message=FALSE}
library(data.table)
library(ggplot2)
library(jsonlite)
library(knitr)
library(glue)
library(here)

# Set seed for reproducibility
set.seed(123)
```

## Overview

This article compares xplainfi's feature importance implementations with those from [fippy](https://github.com/gcskoenig/fippy), a Python package implementing similar methods. This comparison serves as a regression test to ensure methodological consistency across language implementations.

The comparison includes:

- **PFI** (Permutation Feature Importance)
- **CFI** (Conditional Feature Importance) 
- **RFI** (Relative Feature Importance)
- **SAGE** (Shapley Additive Global Importance) - both marginal and conditional variants

## Methodology

Both implementations use:

- **Dataset**: Ewald et al. simulation with 500 observations (`sim_dgp_ewald()`) - a synthetic regression task with known feature importance structure
- **Model**: Random Forest with 100 trees
- **Evaluation**: Same train/test data
- **Metrics**: Mean Squared Error for importance calculations

The Ewald simulation provides an interpretable test case where we can better understand expected feature importance patterns, particularly for conditional methods that account for feature dependencies.

Due to the difference in underlying conditional samplers (ARF for xplainfi vs Gaussian samplers in fippy) we expect conditional methods to show more variation than marginal ones.

## Setup and Execution

The comparison uses separate calculation scripts:

```{bash}
#| eval: false

# 1. Calculate xplainfi results
cd vignettes/articles/fippy-comparison
Rscript calculate_xplainfi.R

# 2. Calculate fippy results (portable with uv - automatically installs dependencies)
./calculate_fippy.py
```

Both scripts generate JSON files with results that are loaded below for comparison.

### Expected Feature Importance Patterns

The Ewald simulation (`sim_dgp_ewald`) generates a regression task with 5 features (x1-x5) where:

- All features contribute to the target variable, but with different weights
- Some features may be correlated, making conditional methods particularly interesting
- The known data generating process allows us to validate whether the methods identify sensible patterns

This provides a better foundation for understanding method differences compared to more complex synthetic tasks.

## Load Results

```{r load-results}
# Check that both result files exist
# Look in the fippy-comparison subdirectory
base_dir <- here::here("vignettes", "articles", "fippy-comparison")
xplainfi_results_path <- file.path(base_dir, "xplainfi_results.json")
fippy_results_path <- file.path(base_dir, "fippy_results.json")

if (!file.exists(xplainfi_results_path)) {
  stop("xplainfi_results.json not found. Please run calculate_xplainfi.R first.")
}

if (!file.exists(fippy_results_path)) {
  stop("fippy_results.json not found. Please run calculate_fippy.py first.")
}

# Load results from both implementations
xplainfi_results <- fromJSON(xplainfi_results_path)
fippy_results <- fromJSON(fippy_results_path)
```

## Model Performance Comparison

```{r model-performance}
performance_comparison <- data.table(
  Implementation = c("xplainfi (R)", "fippy (Python)"),
  R_squared = c(
    round(xplainfi_results$model_performance$r_squared, 4),
    round(fippy_results$model_performance$r_squared, 4)
  )
)

kable(performance_comparison, caption = "Model Performance Comparison")
```

## Method Comparisons

```{r comparison-function}
compare_method <- function(method_name, xplainfi_result, fippy_result) {
  # Both implementations available
  method_dt <- data.table(
    feature = xplainfi_result$feature,
    xplainfi = xplainfi_result$importance,
    fippy = fippy_result$importance
  )
  
  # Return table and correlation for display
  correlation <- cor(method_dt$xplainfi, method_dt$fippy)
  correlation_spearman = cor(method_dt$xplainfi, method_dt$fippy, method = "spearman")
  
  list(
    method = method_name, 
    table = kable(method_dt[order(-xplainfi)], 
                  caption = glue("{method_name} Results Comparison"), 
                  digits = 4),
    correlation = correlation,
    correlation_spearman = correlation_spearman
  )
}
```

### PFI (Permutation Feature Importance)

```{r pfi-comparison}
pfi_result <- compare_method("PFI", xplainfi_results$PFI, fippy_results$PFI)
pfi_result$table

glue("PFI Correlation: {round(pfi_result$correlation, 3)}")

pfi_result$correlation
pfi_result$correlation_spearman
```

### CFI (Conditional Feature Importance)

```{r cfi-comparison}
cfi_result <- compare_method("CFI", xplainfi_results$CFI, fippy_results$CFI)
cfi_result$table

glue("CFI Correlation: {round(cfi_result$correlation, 3)}")

cfi_result$correlation
cfi_result$correlation_spearman
```

### RFI (Relative Feature Importance)

```{r rfi-comparison}
rfi_result <- compare_method("RFI", xplainfi_results$RFI, fippy_results$RFI)
rfi_result$table

glue("RFI Correlation: {round(rfi_result$correlation, 3)}")

rfi_result$correlation
rfi_result$correlation_spearman

glue("xplainfi conditioning set: {paste(xplainfi_results$RFI$conditioning_set, collapse = ', ')}")
glue("fippy conditioning set: {paste(fippy_results$RFI$conditioning_set, collapse = ', ')}")
```

### SAGE Methods

#### Marginal SAGE

```{r sage-marginal-comparison}
sage_marginal_result <- compare_method("Marginal SAGE", xplainfi_results$SAGE_Marginal, fippy_results$SAGE_Marginal)
sage_marginal_result$table

glue("Marginal SAGE Correlation: {round(sage_marginal_result$correlation, 3)}")

sage_marginal_result$correlation
sage_marginal_result$correlation_spearman
```

#### Conditional SAGE

```{r sage-conditional-comparison}
sage_conditional_result <- compare_method("Conditional SAGE", xplainfi_results$SAGE_Conditional, fippy_results$SAGE_Conditional)
sage_conditional_result$table

glue("Conditional SAGE Correlation: {round(sage_conditional_result$correlation, 3)}")

sage_conditional_result$correlation
sage_conditional_result$correlation_spearman
```

## Correlation Summary

```{r correlation-summary}
correlations <- rbindlist(list(
  pfi_result[c("method", "correlation", "correlation_spearman")],
  cfi_result[c("method", "correlation", "correlation_spearman")],
  rfi_result[c("method", "correlation", "correlation_spearman")],
  sage_marginal_result[c("method", "correlation", "correlation_spearman")],
  sage_conditional_result[c("method", "correlation", "correlation_spearman")]
))

kable(
  correlations, 
  caption = "Pearson and Spearman Correlations between xplainfi and fippy", 
  col.names = c("Method", "Pearson Corr.", "Spearman Corr.")
)

melt(correlations, id.vars = "method") |>
  ggplot(aes(x = reorder(method, value), y = value)) +
    facet_wrap(vars(variable), ncol = 1, labeller = as_labeller(c(correlation = "Pearson's", correlation_spearman = "Spearman's"))) +
    geom_col(fill = "steelblue", alpha = 0.7) +
    geom_hline(yintercept = c(0.5, 1), linetype = "dashed", color = "red", alpha = 0.7) +
    coord_flip() +
    labs(
      title = "Implementation Correlations",
      subtitle = "xplainfi (R) vs fippy (Python)",
      x = "Method",
      y = "Correlation"
    ) +
    theme_minimal(base_size = 14) +
    theme(
      plot.title.position = "plot"
    )

```

## Interpretation in Context of Ewald Simulation

The Ewald simulation provides interpretable feature importance patterns that help validate both implementations:

```{r interpretation}
# Extract feature importance rankings from each method
create_ranking_summary <- function(results, method_name) {
  if (is.null(results[[method_name]])) return(NULL)
  
  data.table(
    method = method_name,
    feature = results[[method_name]]$feature,
    importance = results[[method_name]]$importance,
    rank = rank(-results[[method_name]]$importance)
  )
}

# Combine rankings from both implementations
xplainfi_rankings <- rbindlist(lapply(
  c("PFI", "CFI", "RFI", "SAGE_Marginal", "SAGE_Conditional"),
  function(m) create_ranking_summary(xplainfi_results, m)
))
xplainfi_rankings[, implementation := "xplainfi"]

fippy_rankings <- rbindlist(lapply(
  c("PFI", "CFI", "RFI", "SAGE_Marginal", "SAGE_Conditional"),
  function(m) create_ranking_summary(fippy_results, m)
))
fippy_rankings[, implementation := "fippy"]

all_rankings <- rbind(xplainfi_rankings, fippy_rankings)

# Create ranking comparison plot
ggplot(all_rankings, aes(x = feature, y = rank, fill = implementation)) +
  geom_col(position = "dodge", alpha = 0.7) +
  facet_wrap(~method, scales = "free_y") +
  scale_y_reverse(name = "Rank (1 = highest importance)") +
  scale_fill_manual(values = c("xplainfi" = "steelblue", "fippy" = "orange")) +
  labs(
    title = "Feature Importance Rankings Comparison",
    subtitle = "Ewald Simulation: Lower rank = higher importance",
    x = "Feature",
    fill = "Implementation"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )
```

### Method-Specific Insights

**Marginal vs Conditional Methods:**
- **PFI** and **Marginal SAGE** ignore feature correlations
- **CFI**, **RFI**, and **Conditional SAGE** account for feature dependencies
- Differences between marginal and conditional variants reveal the impact of feature correlations

**RFI Conditioning:**
Both implementations use `{x1, x2}` as the conditioning set, allowing us to see how other features' importance changes when conditioned on these two features.

**Cross-Implementation Consistency:**
High correlations indicate that both xplainfi and fippy identify similar underlying feature importance patterns despite using different:
- Programming languages (R vs Python)
- Conditional sampling approaches (ARF vs Gaussian)
- Implementation details

This cross-validation strengthens confidence in both implementations and demonstrates the robustness of these feature importance methods across different computational environments.

```
