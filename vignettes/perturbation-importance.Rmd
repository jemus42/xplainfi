---
title: "Perturbation-based Feature Importance Methods"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Perturbation-based Feature Importance Methods}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
set.seed(123)
# Quiet down
lgr::get_logger("mlr3")$set_threshold("warn")
```

```{r setup}
library(xplainfi)
library(mlr3)
library(mlr3learners)
library(data.table)
library(ggplot2)
library(patchwork)
```

This vignette demonstrates the three perturbation-based feature importance methods implemented in xplainfi:

- **PFI (Permutation Feature Importance)**: Uses marginal sampling (simple permutation)
- **CFI (Conditional Feature Importance)**: Uses conditional sampling via Adversarial Random Forests
- **RFI (Relative Feature Importance)**: Uses conditional sampling on a user-specified subset of features

## Problem Setup: Friedman1 Task

We'll use the Friedman1 task generator which provides an ideal setup for demonstrating feature importance methods. This synthetic regression task has a known ground truth:

- **5 important features** (`important1` to `important5`) that actually affect the target
- **5 unimportant features** (`unimportant1` to `unimportant5`) that are pure noise

The target function is: $y = 10 * \operatorname{sin}(\pi * x_1 * x_2) + 20 * (x_3 - 0.5)^2 + 10 * x_4 + 5 * x_5 + \epsilon$

This makes it easy to evaluate whether our importance methods correctly identify the truly important features.

```{r setup-problem}
# Generate the task
task <- tgen("friedman1")$generate(n = 400)
learner <- lrn("regr.ranger", num.trees = 100)
resampling <- rsmp("cv", folds = 3)
measure <- msr("regr.mse")
```

The task has `r task$nrow` observations with `r length(task$feature_names)` features: `r paste(task$feature_names, collapse = ", ")`. The target variable is `r task$target_names`.

## Permutation Feature Importance (PFI)

PFI shuffles each feature independently, breaking the association between the feature and the target while preserving the marginal distribution.

For more stable results, we use:

- 3-fold CV for more reliable performance estimates
- Within each resampling iteration, we repeat the permutation-prediction-scoring step `iters_perm` times

```{r pfi}
pfi <- PFI$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling,
  iters_perm = 5
)

# Compute importance scores
pfi_results <- pfi$compute(relation = "difference")
pfi_results

# Also stored in
pfi$importance

# Show a sample of detailed scores
head(pfi$scores, 10) |>
  knitr::kable(digits = 4, caption = "Sample of PFI detailed scores")
```

## Conditional Feature Importance (CFI)

CFI uses conditional sampling to preserve the joint distribution of all other features when perturbing a feature of interest. By default we use Adversarial Random Forests (ARF) as conditional sampler internally.

```{r sampler-arf}
sampler = ARFSampler$new(
  task = task, 
  arf_args = list(verbose = FALSE),
  forde_args = list()
)

# Example sampling for 5 randomly chosen rows from the task
sample_data <- task$data(rows = sample(task$nrow, size = 5))
sampled_result <- sampler$sample(
  feature = "important1", 
  data = sample_data, 
  conditioning_set = "important2"
)
```

Original `important1` values: `r paste(round(sample_data$important1, 3), collapse = ", ")`

Sampled `important1` values (conditioned on `important2`): `r paste(round(sampled_result$important1, 3), collapse = ", ")`

```{r cfi}
cfi <- CFI$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling,
  iters_perm = 5,
  sampler = sampler
)

# Compute importance scores
cfi_results <- cfi$compute(relation = "difference")
cfi_results
```

## Relative Feature Importance (RFI)

RFI conditions on a specific subset of features, measuring importance relative to those features. Let's condition on two of the important features to see how the others rank relative to this baseline.

```{r rfi}
conditioning_set <- c("important1", "important2")

rfi <- RFI$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling,
  conditioning_set = conditioning_set,
  iters_perm = 5,
  sampler = sampler
)

# Compute importance scores
rfi_results <- rfi$compute(relation = "difference")
rfi_results
```

## Comparing Methods

Now let's compare the results from all three methods:

```{r comparison}
# Combine results for comparison
comparison <- merge(
  pfi_results[, .(feature, pfi = importance)],
  cfi_results[, .(feature, cfi = importance)],
  by = "feature"
)
comparison <- merge(
  comparison,
  rfi_results[, .(feature, rfi = importance)],
  by = "feature"
)

# Add feature type for analysis
comparison[, feature_type := ifelse(grepl("^important", feature), "Important", "Noise")]

comparison |>
  knitr::kable(
    digits = 4, 
    caption = "Feature Importance Comparison (Difference Scores)",
    col.names = c("Feature", "PFI", "CFI", "RFI", "Type")
  )
```

## Visualization

Let's create comprehensive visualizations to understand the results:

```{r visualization-comparison}
# Reshape data for plotting
plot_data <- comparison |>
  melt(
    id.vars = c("feature", "feature_type"),
    measure.vars = c("pfi", "cfi", "rfi"),
    value.name = "importance",
    variable.name = "method"
  )

# Clean up method names
plot_data[, method := toupper(method)]

# Create the comparison plot
ggplot(plot_data, aes(x = importance, y = reorder(feature, importance), fill = method)) +
  facet_wrap(~ feature_type, scales = "free_y", ncol = 1) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c("PFI" = "steelblue", "CFI" = "darkgreen", "RFI" = "orange")) +
  labs(
    title = "Feature Importance Comparison: PFI vs CFI vs RFI",
    subtitle = glue::glue("Friedman1 task: 5 important + 5 noise features
                          RFI conditioned on: {paste(conditioning_set, collapse = ', ')}"),
    x = "Importance Score (Difference)",
    y = "Feature",
    fill = "Method",
    caption = glue::glue("Using {resampling$iters}-fold cross-validation
                          and 5 permutation iterations")
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    plot.title.position = "plot"
  )
```

Let's also create a correlation plot to see how similar the methods are:

```{r correlation-analysis}
#| fig-height: 10
# Calculate correlations between methods
pfi_cfi_cor <- cor(comparison$pfi, comparison$cfi)
pfi_rfi_cor <- cor(comparison$pfi, comparison$rfi)
cfi_rfi_cor <- cor(comparison$cfi, comparison$rfi)

# Create correlation matrix plot
cor_data <- comparison[, .(feature, pfi, cfi, rfi, feature_type)]

# PFI vs CFI
p1 <- ggplot(cor_data, aes(x = pfi, y = cfi, color = feature_type)) +
  geom_point(size = 3, alpha = 0.8) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "gray50", linetype = "dashed") +
  scale_color_manual(values = c("Important" = "steelblue", "Noise" = "lightcoral")) +
  labs(
    title = "PFI vs CFI",
    subtitle = sprintf("Correlation: %.3f", pfi_cfi_cor),
    x = "PFI Score", y = "CFI Score"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

# PFI vs RFI  
p2 <- ggplot(cor_data, aes(x = pfi, y = rfi, color = feature_type)) +
  geom_point(size = 3, alpha = 0.8) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "gray50", linetype = "dashed") +
  scale_color_manual(values = c("Important" = "steelblue", "Noise" = "lightcoral")) +
  labs(
    title = "PFI vs RFI",
    subtitle = sprintf("Correlation: %.3f", pfi_rfi_cor),
    x = "PFI Score", y = "RFI Score"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

# CFI vs RFI
p3 <- ggplot(cor_data, aes(x = cfi, y = rfi, color = feature_type)) +
  geom_point(size = 3, alpha = 0.8) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "gray50", linetype = "dashed") +
  scale_color_manual(
    values = c("Important" = "steelblue", "Noise" = "lightcoral"),
    name = "Feature Type"
  ) +
  labs(
    title = "CFI vs RFI", 
    subtitle = sprintf("Correlation: %.3f", cfi_rfi_cor),
    x = "CFI Score", y = "RFI Score"
  ) +
  theme_minimal(base_size = 14)

# Combine plots
(p1 / p2 / p3) + 
  plot_annotation(
    title = "Method Correlations",
    subtitle = "Each point represents one feature"
  ) +
  theme(legend.position = "bottom")
```

## Understanding the Results

Let's analyze how well each method distinguishes important from noise features:

```{r analysis}
# Calculate summary statistics by feature type
summary_stats <- comparison[, .(
  mean_importance = mean(c(pfi, cfi, rfi)),
  pfi_mean = mean(pfi),
  cfi_mean = mean(cfi), 
  rfi_mean = mean(rfi),
  n_features = .N
), by = feature_type]

summary_stats |> knitr::kable(digits = 4, caption = "Summary by feature type")

# Calculate separation ratios (how well each method separates signal from noise)
important_pfi <- comparison[feature_type == "Important", mean(pfi)]
noise_pfi <- comparison[feature_type == "Noise", mean(abs(pfi))]
pfi_separation <- important_pfi / noise_pfi

important_cfi <- comparison[feature_type == "Important", mean(cfi)]
noise_cfi <- comparison[feature_type == "Noise", mean(abs(cfi))]
cfi_separation <- important_cfi / noise_cfi

important_rfi <- comparison[feature_type == "Important", mean(rfi)]
noise_rfi <- comparison[feature_type == "Noise", mean(abs(rfi))]
rfi_separation <- important_rfi / noise_rfi

# Store results for inline reporting
pfi_sep <- round(pfi_separation, 2)
cfi_sep <- round(cfi_separation, 2) 
rfi_sep <- round(rfi_separation, 2)
pfi_cfi <- round(pfi_cfi_cor, 3)
pfi_rfi <- round(pfi_rfi_cor, 3)
cfi_rfi <- round(cfi_rfi_cor, 3)
```

### Method Performance

**Separation ratios** (how well each method distinguishes important from noise features):
- PFI: `r pfi_sep`
- CFI: `r cfi_sep`  
- RFI: `r rfi_sep`

**Correlations between methods:**
- PFI vs CFI: `r pfi_cfi`
- PFI vs RFI: `r pfi_rfi`
- CFI vs RFI: `r cfi_rfi`

## Understanding the Differences

The three methods can yield different results because they make different assumptions:

1. **PFI** assumes features are independent and measures the marginal importance of each feature.

2. **CFI** preserves the conditional distribution $P(X_{-j} | X_j)$ when perturbing feature $j$, providing a more realistic assessment when features are correlated.

3. **RFI** measures importance relative to a specific conditioning set, answering "what additional information does this feature provide beyond what we already know from the conditioning features?"

In our Friedman1 example:

- All methods successfully identify that `important1` through `important5` are more important than `unimportant1` through `unimportant5`
- The relative rankings may differ because:
  - PFI treats each feature in isolation
  - CFI accounts for dependencies between all features
  - RFI shows which features add value beyond `important1` and `important2`

## Variability Analysis

Let's examine the stability of our importance estimates across resampling- and permutation iterations using the `$scores` tables:

```{r variability-analysis}
#| fig-height: 9
detailed_scores <- rbindlist(list(
  pfi$scores[, method := "PFI"],
  cfi$scores[, method := "CFI"],
  rfi$scores[, method := "RFI"]
))

score_summary <- detailed_scores[, .(
  mean_importance = mean(importance),
  sd_importance = sd(importance),
  n_iterations = .N
), by = .(feature, method)]

# Add feature type
score_summary[, feature_type := ifelse(grepl("^important", feature), "Important", "Noise")]

# Plot error bars
ggplot(score_summary, aes(
    y = reorder(feature, mean_importance), 
    x = mean_importance,
    xmin = mean_importance - sd_importance, 
    xmax = mean_importance + sd_importance,
    color = method)
  ) +
  facet_wrap(vars(feature_type), ncol = 1, scales = "free_y") +
  geom_point(size = 3) +
  geom_errorbarh(linewidth = 1, height = 0.3) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Importance Score Variability: Mean Â± SD",
    subtitle = glue::glue("Error bars show standard deviation across iterations
                      RFI conditioned on: {paste(conditioning_set, collapse = ', ')}"),
    x = "Score (Difference)",
    y = "Feature",
    color = "Method",
    caption = paste("Using", resampling$iters, "-fold CV and", pfi$param_set$values$iters_perm, "permutations each")
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title.position = "plot",
    legend.position = "bottom"
  )
```

## Advanced Usage

### Custom Samplers

You can provide custom samplers for more control:

```{r custom-sampler, eval=FALSE}
# Create a custom ARF sampler with specific parameters
custom_sampler <- ARFSampler$new(
  task = task,
  arf_args = list(num.trees = 50),  # Fewer trees for faster computation
  forde_args = list(finite_bounds = "no") # Allow extrapolation
)

cfi_custom <- CFI$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling,
  sampler = custom_sampler
)
```

### Empty Conditioning Set for RFI

RFI with an empty conditioning set should produce similar results to PFI:

```{r rfi-empty, eval=TRUE}
rfi_empty <- RFI$new(
  task = task,
  learner = learner,
  measure = measure,
  resampling = resampling,
  conditioning_set = character(0), # Empty conditioning set
  iters_perm = 5,
  sampler = sampler
)

rfi_empty_results <- rfi_empty$compute()

# Compare with PFI
empty_comparison <- merge(
  pfi_results[, .(feature, pfi = importance)],
  rfi_empty_results[, .(feature, rfi_empty = importance)],
  by = "feature"
)

correlation_empty <- cor(empty_comparison$pfi, empty_comparison$rfi_empty)
```

The correlation between PFI and RFI with empty conditioning set is `r round(correlation_empty, 3)`, confirming that $\mathrm{RFI}(\emptyset) \approx \mathrm{PFI}$ as expected.

## Key Takeaways

1. **All methods correctly identify important features** in this well-structured synthetic task
2. **PFI provides a baseline** for marginal feature importance
3. **CFI accounts for feature dependencies** through conditional sampling
4. **RFI measures relative importance** beyond a specified conditioning set
5. **Method choice depends on your question**: 
   - Use PFI for simple, interpretable marginal importance
   - Use CFI when feature dependencies matter
   - Use RFI to understand incremental value beyond known important features
