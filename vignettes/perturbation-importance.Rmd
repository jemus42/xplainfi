---
title: "Perturbation-based Feature Importance Methods"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Perturbation-based Feature Importance Methods}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
set.seed(123)
# Quiet down
lgr::get_logger("mlr3")$set_threshold("warn")
```

```{r setup}
library(xplainfi)
library(mlr3)
library(mlr3learners)
library(data.table)
library(ggplot2)
```

This vignette demonstrates the three perturbation-based feature importance methods implemented in xplainfi:

- **PFI (Permutation Feature Importance)**: Uses marginal sampling (simple permutation)
- **CFI (Conditional Feature Importance)**: Uses conditional sampling via Adversarial Random Forests
- **RFI (Relative Feature Importance)**: Uses conditional sampling on a user-specified subset of features

## Problem Setup: Illustrative Scenarios

We'll demonstrate the methods using two carefully designed scenarios that highlight key differences between PFI, CFI, and RFI:

### Scenario 1: Interaction Effects

This scenario demonstrates how marginal methods (PFI) can miss important interaction effects that conditional methods (CFI) capture:

```{r setup-interactions}
# Generate interaction scenario
task_int <- sim_dgp_interactions(n = 500)
data_int <- task_int$data()

# Quick visualization of the interaction effect
library(ggplot2)
ggplot(data_int, aes(x = x1, y = y, color = cut(x2, 3))) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Interaction Effect: y depends on x1 × x2",
    subtitle = "Different slopes for different x2 levels show interaction",
    color = "x2 level"
  ) +
  theme_minimal()
```

**Key insight**: The relationship between x1 and y changes depending on x2's value. PFI, which permutes features independently, may underestimate the importance of x1 and x2.

### Scenario 2: Confounding

This scenario shows how hidden confounders affect importance estimates and how conditioning can help:

```{r setup-confounding}
# Generate confounding scenario  
task_conf <- sim_dgp_confounded(n = 500)
data_conf <- task_conf$data()

# Visualize confounding structure
cor_data <- cor(data_conf[, .(x1, x2, proxy, independent, y)])
# Convert correlation matrix to long format for ggplot
cor_df <- data.table(
  var1 = rep(rownames(cor_data), each = ncol(cor_data)),
  var2 = rep(colnames(cor_data), nrow(cor_data)),
  correlation = as.vector(cor_data)
)

ggplot(cor_df, aes(x = var1, y = var2, fill = correlation)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(correlation, 2)), size = 3) +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  labs(
    title = "Correlation Structure with Hidden Confounder",
    subtitle = "Note high correlations between x1, x2, and proxy due to confounder"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Key insight**: x1 and x2 are correlated through a hidden confounder that also affects y. The proxy provides a noisy measurement of this confounder. RFI conditioning on the proxy should help isolate direct effects.

```{r setup-common}
# Common setup for both scenarios
learner <- lrn("regr.ranger", num.trees = 100)
resampling <- rsmp("cv", folds = 3)
measure <- msr("regr.mse")
```

## Analysis 1: Interaction Effects

Let's first analyze the interaction scenario where $y = 0.3 \cdot x_1 + 0.2 \cdot x_2 + 1.5 \cdot x_1 \cdot x_2 + x_3 + \epsilon$.

### PFI on Interactions

```{r pfi-interactions}
pfi_int <- PFI$new(
  task = task_int,
  learner = learner,
  measure = measure,
  resampling = resampling,
  iters_perm = 5
)

# Compute importance scores
pfi_int_results <- pfi_int$compute(relation = "difference")
pfi_int_results

# Expected: x1 and x2 may show lower importance than their true contribution
# because PFI breaks the interaction by permuting independently
```

### CFI on Interactions

CFI preserves the joint distribution, which should better capture the interaction effect:

```{r cfi-interactions}
# Create ARF sampler for the interaction task
sampler_int = ARFSampler$new(
  task = task_int, 
  verbose = FALSE
)

cfi_int <- CFI$new(
  task = task_int,
  learner = learner,
  measure = measure,
  resampling = resampling,
  iters_perm = 5,
  sampler = sampler_int
)

# Compute importance scores
cfi_int_results <- cfi_int$compute(relation = "difference")
cfi_int_results

# Expected: x1 and x2 should show higher importance than in PFI
# because CFI preserves their interaction when sampling
```

### Comparing Methods on Interactions

Let's compare how the methods handle the interaction:

```{r compare-interactions}
# Combine results  
comp_int <- merge(
  pfi_int_results[, .(feature, pfi = importance)],
  cfi_int_results[, .(feature, cfi = importance)],
  by = "feature"
)

# Visualize
library(ggplot2)
ggplot(comp_int, aes(x = feature)) +
  geom_col(aes(y = pfi, fill = "PFI"), position = position_nudge(x = -0.2), width = 0.35, alpha = 0.8) +
  geom_col(aes(y = cfi, fill = "CFI"), position = position_nudge(x = 0.2), width = 0.35, alpha = 0.8) +
  scale_fill_manual(values = c("PFI" = "steelblue", "CFI" = "darkgreen")) +
  labs(
    title = "PFI vs CFI on Interaction Effects",
    subtitle = "True model: y = 0.3·x1 + 0.2·x2 + 1.5·x1·x2 + x3 + noise",
    y = "Importance Score",
    x = "Feature"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

## Analysis 2: Confounding

Now let's analyze the confounding scenario where a hidden confounder affects both features and the outcome.

### PFI on Confounded Data

```{r pfi-confounding}
pfi_conf <- PFI$new(
  task = task_conf,
  learner = learner,
  measure = measure,
  resampling = resampling,
  iters_perm = 5
)

pfi_conf_results <- pfi_conf$compute(relation = "difference")
pfi_conf_results
```

### RFI Conditioning on Proxy

RFI can condition on the proxy to help isolate direct effects:

```{r rfi-confounding}
# Create sampler for confounding task
sampler_conf = ARFSampler$new(
  task = task_conf,
  verbose = FALSE
)

# RFI conditioning on the proxy
rfi_conf <- RFI$new(
  task = task_conf,
  learner = learner,
  measure = measure,
  resampling = resampling,
  conditioning_set = "proxy",  # Condition on proxy to reduce confounding
  iters_perm = 5,
  sampler = sampler_conf
)

rfi_conf_results <- rfi_conf$compute(relation = "difference")
rfi_conf_results

# Also try CFI for comparison
cfi_conf <- CFI$new(
  task = task_conf,
  learner = learner,
  measure = measure,
  resampling = resampling,
  iters_perm = 5,
  sampler = sampler_conf
)

cfi_conf_results <- cfi_conf$compute(relation = "difference")
```

### Comparing Methods on Confounding

```{r compare-confounding}
# Combine all results
comp_conf <- merge(
  pfi_conf_results[, .(feature, pfi = importance)],
  cfi_conf_results[, .(feature, cfi = importance)],
  by = "feature"
) |>
  merge(
    rfi_conf_results[, .(feature, rfi = importance)],
    by = "feature"
  )

# Visualize the comparison
comp_conf_long <- melt(
  comp_conf, 
  id.vars = "feature",
  measure.vars = c("pfi", "cfi", "rfi"),
  variable.name = "method",
  value.name = "importance"
)
comp_conf_long[, method := toupper(method)]

ggplot(comp_conf_long, aes(x = feature, y = importance, fill = method)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c("PFI" = "steelblue", "CFI" = "darkgreen", "RFI" = "orange")) +
  labs(
    title = "Feature Importance Under Confounding",
    subtitle = "RFI conditions on proxy to reduce confounding bias",
    y = "Importance Score",
    x = "Feature",
    fill = "Method"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

## Key Insights from Both Scenarios

### Interaction Effects (Scenario 1)

In the interaction scenario, we observed:

1. **PFI underestimates interacting features**: When x1 and x2 interact strongly, permuting them independently breaks their relationship, leading to lower importance scores.

2. **CFI captures interaction effects**: By preserving the conditional distribution, CFI maintains the interaction structure during sampling, resulting in more accurate importance estimates.

```{r summary-interactions}
# Calculate the ratio of CFI to PFI importance for interacting features
int_ratio <- comp_int[feature %in% c("x1", "x2"), .(
  feature,
  pfi_importance = pfi,
  cfi_importance = cfi,
  cfi_pfi_ratio = cfi / pfi
)]

int_ratio |> 
  knitr::kable(
    digits = 3,
    caption = "CFI vs PFI for Interacting Features"
  )
```

### Confounding Effects (Scenario 2)

In the confounding scenario, we observed:

1. **PFI shows confounded effects**: Without accounting for the hidden confounder, PFI may overestimate the importance of x1 and x2.

2. **RFI conditioning on proxy reduces bias**: By conditioning on the proxy (noisy measurement of the confounder), RFI can better isolate direct effects.

3. **CFI partially accounts for confounding**: Through its conditional sampling, CFI captures some of the confounding structure.

```{r summary-confounding}
# Show how conditioning affects importance estimates
conf_summary <- comp_conf[, .(
  feature,
  pfi_importance = round(pfi, 3),
  cfi_importance = round(cfi, 3),
  rfi_proxy_importance = round(rfi, 3),
  pfi_rfi_diff = round(pfi - rfi, 3)
)]

conf_summary |> 
  knitr::kable(
    caption = "Effect of Conditioning on Proxy in Confounded Scenario"
  )
```

## Key Takeaways

Through these two scenarios, we've demonstrated:

1. **Method choice matters**: 
   - **PFI** is simple and fast but can miss interaction effects and be affected by confounding
   - **CFI** captures feature dependencies and interactions through conditional sampling
   - **RFI** allows targeted conditioning to isolate specific relationships

2. **When to use each method**:
   - Use **PFI** when features are believed to be independent and you want a quick baseline
   - Use **CFI** when you suspect feature interactions or dependencies
   - Use **RFI** when you want to understand importance relative to specific features or to control for confounders

3. **Practical considerations**:
   - All methods benefit from cross-validation and multiple permutation iterations for stability
   - ARF-based conditional sampling (used in CFI/RFI) is more computationally intensive than marginal sampling
   - The choice of conditioning set in RFI requires domain knowledge

## Further Reading

For more details on these methods and their theoretical foundations, see:

- Breiman (2001) for the original PFI formulation
- Strobl et al. (2008) for limitations of PFI with correlated features  
- Watson & Wright (2021) for conditional sampling with ARF
- König et al. (2021) for relative feature importance
- Ewald et al. (2024) for a comprehensive review of feature importance methods
