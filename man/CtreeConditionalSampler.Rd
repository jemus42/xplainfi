% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/FeatureSampler-CtreeConditionalSampler.R
\name{CtreeConditionalSampler}
\alias{CtreeConditionalSampler}
\title{Conditional Inference Tree Conditional Sampler}
\description{
Implements conditional sampling using conditional inference trees (ctree).
Builds a tree predicting target features from conditioning features, then samples
from the terminal node corresponding to each test observation.
}
\details{
This sampler approximates the conditional distribution \eqn{P(X_B | X_A = x_A)} by:
\enumerate{
\item Building a conditional inference tree with \eqn{X_B} as response and \eqn{X_A} as predictors
\item For each test observation, finding its terminal (leaf) node in the tree
\item Sampling uniformly from training observations in that same terminal node
}

Conditional inference trees (ctree) use permutation tests to determine splits,
which helps avoid overfitting and handles mixed feature types naturally. The tree
partitions the feature space based on the conditioning variables, creating local
neighborhoods that respect the conditional distribution structure.

\strong{Key advantages over other samplers:}
\itemize{
\item Handles mixed feature types (continuous and categorical)
\item Non-parametric (no distributional assumptions)
\item Automatic feature selection (splits only on informative features)
\item Can capture non-linear conditional relationships
\item Statistically principled splitting criteria
}

\strong{Hyperparameters} control tree complexity:
\itemize{
\item \code{mincriterion}: Significance level for splits (higher = fewer splits)
\item \code{minsplit}: Minimum observations required for a split
\item \code{minbucket}: Minimum observations in terminal nodes
}

This implementation is inspired by shapr's ctree approach but simplified for our
use case (we build trees on-demand rather than pre-computing all subsets).

\strong{Advantages:}
\itemize{
\item Works with any feature types
\item Robust to outliers
\item Interpretable tree structure
\item Handles high-dimensional conditioning
}

\strong{Limitations:}
\itemize{
\item Requires model fitting (slower than kNN)
\item Can produce duplicates if terminal nodes are small
\item Tree building time increases with data size
}
}
\examples{
\dontrun{
library(mlr3)
task = tgen("friedman1")$generate(n = 100)

# Create sampler with default parameters
sampler = CtreeConditionalSampler$new(task)

# Sample features conditioned on others
test_data = task$data(rows = 1:5)
sampled = sampler$sample_newdata(
  feature = c("important2", "important3"),
  newdata = test_data,
  conditioning_set = "important1"
)
}

}
\references{
\itemize{
\item Hothorn, T., Hornik, K., & Zeileis, A. (2006). Unbiased recursive partitioning: A conditional inference framework. Journal of Computational and Graphical statistics, 15(3), 651-674.
\item Aas, K., Jullum, M., & LÃ¸land, A. (2021). Explaining individual predictions when features are dependent: More accurate approximations to Shapley values. Artificial Intelligence, 298, 103502.
}
}
\section{Super classes}{
\code{\link[xplainfi:FeatureSampler]{xplainfi::FeatureSampler}} -> \code{\link[xplainfi:ConditionalSampler]{xplainfi::ConditionalSampler}} -> \code{CtreeConditionalSampler}
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{training_data}}{(\code{data.table}) Training data used for building trees.}

\item{\code{tree_cache}}{(\code{environment}) Cache for fitted ctree models.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-CtreeConditionalSampler-new}{\code{CtreeConditionalSampler$new()}}
\item \href{#method-CtreeConditionalSampler-sample}{\code{CtreeConditionalSampler$sample()}}
\item \href{#method-CtreeConditionalSampler-sample_newdata}{\code{CtreeConditionalSampler$sample_newdata()}}
\item \href{#method-CtreeConditionalSampler-clone}{\code{CtreeConditionalSampler$clone()}}
}
}
\if{html}{\out{
<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="xplainfi" data-topic="FeatureSampler" data-id="print"><a href='../../xplainfi/html/FeatureSampler.html#method-FeatureSampler-print'><code>xplainfi::FeatureSampler$print()</code></a></span></li>
</ul>
</details>
}}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CtreeConditionalSampler-new"></a>}}
\if{latex}{\out{\hypertarget{method-CtreeConditionalSampler-new}{}}}
\subsection{Method \code{new()}}{
Creates a new CtreeConditionalSampler.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CtreeConditionalSampler$new(
  task,
  mincriterion = 0.95,
  minsplit = 20L,
  minbucket = 7L,
  use_cache = TRUE
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{task}}{(\link[mlr3:Task]{mlr3::Task}) Task to sample from.}

\item{\code{mincriterion}}{(\code{numeric(1)}: \code{0.95}) Significance level threshold for splitting (1 - p-value).
Higher values result in fewer splits (simpler trees).}

\item{\code{minsplit}}{(\code{integer(1)}: \code{20L}) Minimum number of observations required for a split.}

\item{\code{minbucket}}{(\code{integer(1)}: \code{7L}) Minimum number of observations in terminal nodes.}

\item{\code{use_cache}}{(\code{logical(1)}: \code{TRUE}) Whether to cache fitted trees.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CtreeConditionalSampler-sample"></a>}}
\if{latex}{\out{\hypertarget{method-CtreeConditionalSampler-sample}{}}}
\subsection{Method \code{sample()}}{
Sample features using conditional inference trees.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CtreeConditionalSampler$sample(
  feature,
  row_ids = NULL,
  conditioning_set = NULL
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{feature}}{(\code{character()}) Feature name(s) to sample.}

\item{\code{row_ids}}{(\code{integer()} | \code{NULL}) Row IDs from task to use.}

\item{\code{conditioning_set}}{(\code{character()} | \code{NULL}) Features to condition on.
If \code{NULL}, samples from marginal distribution.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Modified copy with sampled feature(s).
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CtreeConditionalSampler-sample_newdata"></a>}}
\if{latex}{\out{\hypertarget{method-CtreeConditionalSampler-sample_newdata}{}}}
\subsection{Method \code{sample_newdata()}}{
Sample from external data conditionally.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CtreeConditionalSampler$sample_newdata(
  feature,
  newdata,
  conditioning_set = NULL
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{feature}}{(\code{character()}) Feature(s) to sample.}

\item{\code{newdata}}{(\code{\link[data.table:data.table]{data.table}}) External data to use.}

\item{\code{conditioning_set}}{(\code{character()} | \code{NULL}) Features to condition on.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Modified copy with sampled feature(s).
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CtreeConditionalSampler-clone"></a>}}
\if{latex}{\out{\hypertarget{method-CtreeConditionalSampler-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CtreeConditionalSampler$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
