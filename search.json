[{"path":"https://jemus42.github.io/xplainfi/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 xplainfi authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Comparison with fippy (Python Implementation)","text":"article compares xplainfi’s feature importance implementations fippy, Python package implementing similar methods. comparison serves regression test ensure methodological consistency across language implementations. comparison includes: PFI (Permutation Feature Importance) CFI (Conditional Feature Importance) RFI (Relative Feature Importance) SAGE (Shapley Additive Global Importance) - marginal conditional variants","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"methodology","dir":"Articles","previous_headings":"","what":"Methodology","title":"Comparison with fippy (Python Implementation)","text":"implementations use: Dataset: Ewald et al. simulation 5000 observations (sim_dgp_ewald()) Evaluation: train/test data (70% train) Metrics: Mean Squared Error importance calculations Ewald simulation provides interpretable test case can better understand expected feature importance patterns, particularly conditional methods account feature dependencies. Due difference underlying conditional samplers (ARF xplainfi vs Gaussian samplers fippy) expect conditional methods show variation marginal ones.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"setup-and-execution","dir":"Articles","previous_headings":"","what":"Setup and Execution","title":"Comparison with fippy (Python Implementation)","text":"comparison uses separate calculation scripts: scripts generate JSON files results loaded comparison.","code":"# 1. Calculate xplainfi results cd vignettes/articles/fippy-comparison Rscript calculate_xplainfi.R  # 2. Calculate fippy results (portable with uv - automatically installs dependencies) ./calculate_fippy.py"},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"expected-feature-importance-patterns","dir":"Articles","previous_headings":"Setup and Execution","what":"Expected Feature Importance Patterns","title":"Comparison with fippy (Python Implementation)","text":"Ewald simulation (sim_dgp_ewald) generates regression task 5 features (x1-x5) : features contribute target variable, different weights features correlated, making conditional methods particularly interesting known data generating process allows us validate whether methods identify sensible patterns See article details DGP","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"load-results","dir":"Articles","previous_headings":"","what":"Load Results","title":"Comparison with fippy (Python Implementation)","text":"","code":"# Check that both result files exist # Look in the fippy-comparison subdirectory base_dir <- here::here(\"vignettes\", \"articles\", \"fippy-comparison\") xplainfi_results_path <- file.path(base_dir, \"xplainfi_results.json\") fippy_results_path <- file.path(base_dir, \"fippy_results.json\")  if (!file.exists(xplainfi_results_path)) {   stop(\"xplainfi_results.json not found. Please run calculate_xplainfi.R first.\") }  if (!file.exists(fippy_results_path)) {   stop(\"fippy_results.json not found. Please run calculate_fippy.py first.\") }  # Load results from both implementations xplainfi_results <- fromJSON(xplainfi_results_path) fippy_results <- fromJSON(fippy_results_path)"},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"model-performance-comparison","dir":"Articles","previous_headings":"","what":"Model Performance Comparison","title":"Comparison with fippy (Python Implementation)","text":"Model Performance Comparison","code":"performance_comparison <- data.table(   Implementation = c(\"xplainfi (R)\", \"fippy (Python)\"),   R_squared = c(     round(xplainfi_results$model_performance$r_squared, 4),     round(fippy_results$model_performance$r_squared, 4)   ) )  kable(performance_comparison, caption = \"Model Performance Comparison\")"},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"method-comparisons","dir":"Articles","previous_headings":"","what":"Method Comparisons","title":"Comparison with fippy (Python Implementation)","text":"","code":"compare_method <- function(method_name, xplainfi_result, fippy_result) {   # Both implementations available   method_dt <- data.table(     feature = xplainfi_result$feature,     xplainfi = xplainfi_result$importance,     fippy = fippy_result$importance   )      # Return table and correlation for display   correlation <- cor(method_dt$xplainfi, method_dt$fippy)   correlation_spearman = cor(method_dt$xplainfi, method_dt$fippy, method = \"spearman\")      list(     method = method_name,      table = kable(method_dt[order(-xplainfi)],                    caption = glue(\"{method_name} Results Comparison\"),                    digits = 4),     correlation = correlation,     correlation_spearman = correlation_spearman   ) }"},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"pfi-permutation-feature-importance","dir":"Articles","previous_headings":"Method Comparisons","what":"PFI (Permutation Feature Importance)","title":"Comparison with fippy (Python Implementation)","text":"PFI Results Comparison","code":"pfi_result <- compare_method(\"PFI\", xplainfi_results$PFI, fippy_results$PFI) pfi_result$table"},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"cfi-conditional-feature-importance","dir":"Articles","previous_headings":"Method Comparisons","what":"CFI (Conditional Feature Importance)","title":"Comparison with fippy (Python Implementation)","text":"CFI Results Comparison","code":"cfi_result <- compare_method(\"CFI\", xplainfi_results$CFI, fippy_results$CFI) cfi_result$table"},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"rfi-relative-feature-importance","dir":"Articles","previous_headings":"Method Comparisons","what":"RFI (Relative Feature Importance)","title":"Comparison with fippy (Python Implementation)","text":"RFI Results Comparison","code":"rfi_result <- compare_method(\"RFI\", xplainfi_results$RFI, fippy_results$RFI) rfi_result$table glue(\"xplainfi conditioning set: {paste(xplainfi_results$RFI$conditioning_set, collapse = ', ')}\") ## xplainfi conditioning set: x1, x2 glue(\"fippy conditioning set: {paste(fippy_results$RFI$conditioning_set, collapse = ', ')}\") ## fippy conditioning set: x1, x2"},{"path":[]},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"marginal-sage","dir":"Articles","previous_headings":"Method Comparisons > SAGE Methods","what":"Marginal SAGE","title":"Comparison with fippy (Python Implementation)","text":"Marginal SAGE Results Comparison","code":"sage_marginal_result <- compare_method(\"Marginal SAGE\", xplainfi_results$SAGE_Marginal, fippy_results$SAGE_Marginal) sage_marginal_result$table"},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"conditional-sage","dir":"Articles","previous_headings":"Method Comparisons > SAGE Methods","what":"Conditional SAGE","title":"Comparison with fippy (Python Implementation)","text":"Conditional SAGE Results Comparison","code":"sage_conditional_result <- compare_method(\"Conditional SAGE\", xplainfi_results$SAGE_Conditional, fippy_results$SAGE_Conditional) sage_conditional_result$table"},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"correlation-summary","dir":"Articles","previous_headings":"","what":"Correlation Summary","title":"Comparison with fippy (Python Implementation)","text":"Pearson Spearman Correlations xplainfi fippy","code":"correlations <- rbindlist(list(   pfi_result[c(\"method\", \"correlation\", \"correlation_spearman\")],   cfi_result[c(\"method\", \"correlation\", \"correlation_spearman\")],   rfi_result[c(\"method\", \"correlation\", \"correlation_spearman\")],   sage_marginal_result[c(\"method\", \"correlation\", \"correlation_spearman\")],   sage_conditional_result[c(\"method\", \"correlation\", \"correlation_spearman\")] ))  kable(   correlations,    digits = 4,   caption = \"Pearson and Spearman Correlations between xplainfi and fippy\",    col.names = c(\"Method\", \"Pearson Corr.\", \"Spearman Corr.\") ) melt(correlations, id.vars = \"method\") |>   ggplot(aes(x = reorder(method, value), y = value)) +     facet_wrap(vars(variable), ncol = 1, labeller = as_labeller(c(correlation = \"Pearson's\", correlation_spearman = \"Spearman's\"))) +     geom_col(fill = \"steelblue\", alpha = 0.7) +     geom_hline(yintercept = c(0.5, 1), linetype = \"dashed\", color = \"red\", alpha = 0.7) +     coord_flip() +     labs(       title = \"Implementation Correlations\",       subtitle = \"xplainfi (R) vs fippy (Python)\",       x = \"Method\",       y = \"Correlation\"     ) +     theme_minimal(base_size = 14) +     theme(       plot.title.position = \"plot\"     )"},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"interpretation-in-context-of-ewald-simulation","dir":"Articles","previous_headings":"","what":"Interpretation in Context of Ewald Simulation","title":"Comparison with fippy (Python Implementation)","text":"Ewald simulation provides interpretable feature importance patterns help validate implementations:","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/fippy-comparison.html","id":"method-specific-insights","dir":"Articles","previous_headings":"Interpretation in Context of Ewald Simulation","what":"Method-Specific Insights","title":"Comparison with fippy (Python Implementation)","text":"Marginal vs Conditional Methods: PFI Marginal SAGE ignore feature correlations CFI, RFI, Conditional SAGE account feature dependencies Differences marginal conditional variants reveal impact feature correlations RFI Conditioning: implementations use {x3} conditioning set. Cross-Implementation Consistency: High correlations indicate xplainfi fippy identify similar underlying feature importance patterns despite using different: Programming languages (R vs Python) Conditional sampling approaches (ARF vs Gaussian) Implementation details","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/loco-loci.html","id":"example-data-interaction-effects","dir":"Articles","previous_headings":"","what":"Example Data: Interaction Effects","title":"LOCO and LOCI","text":"illustrate key differences LOCO LOCI, ’ll use data generating process interaction effects: \\[y = 2 \\cdot x_1 \\cdot x_2 + x_3 + \\epsilon\\] \\(\\epsilon \\sim N(0, 0.5^2)\\) features \\(x_1, x_2, x_3, noise_1, noise_2 \\sim N(0,1)\\) independent. Key characteristics: \\(x_1, x_2\\): individual effects, interact \\(x_3\\): direct main effect \\(y\\) \\(noise_1, noise_2\\): Pure noise variables effect \\(y\\) setup highlights LOCO LOCI handle interaction effects differently.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/loco-loci.html","id":"leave-one-covariate-out-loco","dir":"Articles","previous_headings":"","what":"Leave-One-Covariate-Out (LOCO)","title":"LOCO and LOCI","text":"LOCO measures feature importance comparing model performance without feature. feature, learner retrained without feature performance difference indicates feature’s importance. feature \\(j\\), LOCO calculated difference expected loss model fit without feature full model: \\[\\text{LOCO}_j = \\mathbb{E}(L(Y, f_{-j}(X_{-j}))) - \\mathbb{E}(L(Y, f(X)))\\] Higher values indicate important features (larger performance drop removed).","code":"task <- sim_dgp_interactions(n = 500) learner <- lrn(\"regr.lm\") measure <- msr(\"regr.mse\")  loco <- LOCO$new(   task = task,   learner = learner,   measure = measure ) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`.  loco$compute() #> Key: <feature> #>    feature  importance #>     <char>       <num> #> 1:  noise1  0.02343712 #> 2:  noise2  0.01808013 #> 3:      x1 -0.14135998 #> 4:      x2  0.11463314 #> 5:      x3  0.65082799"},{"path":"https://jemus42.github.io/xplainfi/articles/loco-loci.html","id":"leave-one-covariate-in-loci","dir":"Articles","previous_headings":"","what":"Leave-One-Covariate-In (LOCI)","title":"LOCO and LOCI","text":"LOCI measures feature importance training models individual feature comparing performance featureless (baseline) model. shows much predictive power feature provides , beyond optimal constant prediction. Since method measures univariate associations, recommend use “real” feature importance method, rather exists special case broader class refitting-based importances methods. illustrate completeness highlight limitation. feature \\(j\\), LOCI calculated difference expected loss featureless learner constant model model including feature: \\[\\text{LOCI}_j = \\mathbb{E}(L(Y, f_{\\emptyset})) - \\mathbb{E}(L(Y, f_j(X_{j})))\\] Higher values indicate important features (better individual performance compared baseline).","code":"loci <- LOCI$new(   task = task,   learner = learner,   measure = measure ) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`.  loci$compute() #> Key: <feature> #>    feature  importance #>     <char>       <num> #> 1:  noise1  0.09848467 #> 2:  noise2 -0.05579254 #> 3:      x1 -0.44010340 #> 4:      x2  0.11155710 #> 5:      x3  0.58359019"},{"path":"https://jemus42.github.io/xplainfi/articles/loco-loci.html","id":"understanding-the-results","dir":"Articles","previous_headings":"","what":"Understanding the Results","title":"LOCO and LOCI","text":"LOCO results interpretation: \\(x_1\\) \\(x_2\\) show low importance removing either one eliminates interaction (\\(x_1 \\cdot x_2\\) becomes zero) \\(x_3\\) shows higher importance due direct main effect demonstrates LOCO’s limitation interaction effects LOCI results interpretation: \\(x_1\\) \\(x_2\\) show low importance individually predictive power \\(x_3\\) shows high importance alone can predict \\(y\\) reasonably well shows LOCI focuses individual feature contributions","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/loco-loci.html","id":"multiple-refits","dir":"Articles","previous_headings":"","what":"Multiple Refits","title":"LOCO and LOCI","text":"Like PFI iters_perm multiple permutation iterations, LOCO LOCI support iters_refit multiple refit iterations per resampling iteration, also provide standard deviations: LOCO scores per refit resampling fold","code":"loco_multi = LOCO$new(   task = task,   learner = learner,   measure = measure,   resampling = rsmp(\"cv\", folds = 3),   iters_refit = 3L )  loco_multi$compute() #> Key: <feature> #>    feature  importance         sd #>     <char>       <num>      <num> #> 1:  noise1 -0.01212711 0.05104637 #> 2:  noise2 -0.02467126 0.05152184 #> 3:      x1 -0.08943524 0.08786365 #> 4:      x2 -0.07556117 0.29284316 #> 5:      x3  0.82401461 0.34619308  # Check individual scores with multiple refits loco_multi$scores[1:10, ] |>   knitr::kable(digits = 4, caption = \"LOCO scores per refit and resampling fold\")"},{"path":"https://jemus42.github.io/xplainfi/articles/loco-loci.html","id":"comparing-loco-and-loci","dir":"Articles","previous_headings":"","what":"Comparing LOCO and LOCI","title":"LOCO and LOCI","text":"LOCO vs LOCI importance scores Interpreting results: LOCO: Higher values indicate important features (larger performance drop removed) Positive values: feature performs better featureless baseline Negative values: feature performs worse featureless baseline","code":"# Combine results for comparison importance_combined <- rbind(   loco$importance[, method := \"LOCO\"],   loci$importance[, method := \"LOCI\"] )  importance_combined <- importance_combined |>   dcast(feature ~ method, value.var = \"importance\")  importance_combined |>   knitr::kable(digits = 4, caption = \"LOCO vs LOCI importance scores\")"},{"path":"https://jemus42.github.io/xplainfi/articles/loco-loci.html","id":"different-aggregation-methods-in-loco","dir":"Articles","previous_headings":"","what":"Different aggregation methods in LOCO","title":"LOCO and LOCI","text":"examples aggregates predictions test set (obs_loss = FALSE), computing importance difference average losses using measure’s default aggregation: \\(\\mathrm{mean}(L(Y, f_{-j}(X_{-j}))) - \\mathrm{mean}(L(Y, f(X)))\\). LOCO can also computed using alternative approach (obs_loss = TRUE), follows original paper formulation aggregating observation-wise differences median absolute differences: \\(\\mathrm{median}(\\{|L(y_i, f_{-j}(x_{,-j}))| - |L(y_i, f(x_i))|\\}_{=1}^n)\\), need select mlr3 measure \\(L_1\\) loss used observation level, e.g regr.mae. Setting obs_loss = TRUE provides access individual observation-level differences: Comparison LOCO aggregation approaches $obs_losses field provides detailed observation-level information: Observation-wise losses x3 row contains: row_ids: Test set observation identifiers truth: Original target values response_ref: Full model predictions response_feature: Reduced model predictions (without feature) loss_ref / loss_feature: Individual losses model obs_diff: Observation-wise importance differences Note case regr.mse aggregates mean using aggregation_fun = mean aggregate observation-wise losses, approaches yield result, slightly different ways:","code":"# Use macro-averaged approach with median aggregation loco_obs <- LOCO$new(   task = task,   learner = learner,   measure = msr(\"regr.mae\"),  # MAE supports obs_loss   obs_loss = TRUE,   aggregation_fun = median # for the \"outer\" aggregation ) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`.  loco_obs$compute() #> Key: <feature> #>    feature   importance #>     <char>        <num> #> 1:  noise1  0.001274411 #> 2:  noise2  0.003229655 #> 3:      x1  0.001995886 #> 4:      x2 -0.077958164 #> 5:      x3  0.315709247  # Compare aggregated results rbind(   loco$importance[, method := \"Set-wise MSE (measure default)\"],   loco_obs$importance[, method := \"Observation-wise (median of diffs)\"] ) |>   dcast(feature ~ method, value.var = \"importance\") |>   knitr::kable(digits = 4, caption = \"Comparison of LOCO aggregation approaches\") # Examine observation-wise data for first feature feature_data <- loco_obs$obs_losses[feature == \"x3\"]  # Show structure str(feature_data) #> Classes 'data.table' and 'data.frame':   167 obs. of  10 variables: #>  $ row_ids         : int  3 4 5 6 8 9 11 12 13 15 ... #>  $ feature         : chr  \"x3\" \"x3\" \"x3\" \"x3\" ... #>  $ iteration       : int  1 1 1 1 1 1 1 1 1 1 ... #>  $ iter_refit      : int  1 1 1 1 1 1 1 1 1 1 ... #>  $ truth           : num  2.831 -0.293 -2.552 0.476 8.165 ... #>  $ response_ref    : num  0.468 0.0826 -2.7257 0.7273 1.0292 ... #>  $ response_feature: num  0.828 0.247 -0.983 -0.334 -0.975 ... #>  $ loss_ref        : num  2.363 0.376 0.173 0.251 7.136 ... #>  $ loss_feature    : num  2.003 0.541 1.569 0.81 9.14 ... #>  $ obs_diff        : num  -0.36 0.165 1.396 0.559 2.004 ... #>  - attr(*, \".internal.selfref\")=<externalptr>  # Display first few observations head(feature_data) |>   knitr::kable(digits = 3, caption = \"Observation-wise losses for x3\") set.seed(1) resampling = rsmp(\"holdout\") resampling$instantiate(task)  loco_orig <- LOCO$new(   task = task,   learner = learner,   resampling = resampling,   measure = measure )  loco_obsloss <- LOCO$new(   task = task,   learner = learner,   resampling = resampling,   measure = measure,   obs_loss = TRUE,   aggregation_fun = mean )  rbind(   loco_orig$compute()[, method := \"Set-wise\"],   loco_obsloss$compute()[, method := \"Observation-wise\"] ) |>   dcast(feature ~ method, value.var = \"importance\") #> Key: <feature> #>    feature Observation-wise    Set-wise #>     <char>            <num>       <num> #> 1:  noise1      -0.06116845 -0.06116845 #> 2:  noise2      -0.03037297 -0.03037297 #> 3:      x1      -0.04521521 -0.04521521 #> 4:      x2       0.02801885  0.02801885 #> 5:      x3       1.10230314  1.10230314"},{"path":"https://jemus42.github.io/xplainfi/articles/loco-loci.html","id":"classification-example-with-probability-access","dir":"Articles","previous_headings":"","what":"Classification Example with Probability Access","title":"LOCO and LOCI","text":"classification tasks, obs_losses field stores predicted classes preserving access probabilities original mlr3 prediction objects: LOCO importance classification Examine observation-wise classification results: Observation-wise classification losses classification, response columns contain predicted classes (factors). Note $resample_result contains reference model predictions (full model LOCO, featureless LOCI), feature-specific models: $predictions field provides complete access feature-specific prediction objects: feature: feature left (LOCO) left (LOCI) iteration: Resampling iteration iter_refit: Refit iteration within resampling prediction: Complete mlr3 prediction object responses, probabilities, etc. gives full access : Feature-specific probabilities: prediction$prob Feature-specific predicted classes: prediction$response Additional prediction data: fields learner provides observation-wise approach particularly useful : Robust aggregation: Using median instead mean reduce outlier influence Individual analysis: Examining specific observations drive importance scores Distribution analysis: Understanding variability feature effects across observations Model debugging: Identifying problematic observations prediction patterns","code":"# Binary classification task task_classif <- tgen(\"circle\", d = 6)$generate(n = 200)  # Learner with probability predictions (required for accessing probabilities later) learner_classif <- lrn(\"classif.rpart\", predict_type = \"prob\")  # LOCO with observation-wise losses loco_classif <- LOCO$new(   task = task_classif,   learner = learner_classif,   measure = msr(\"classif.ce\"),  # Classification error supports obs_loss (zero-one in this case)   obs_loss = TRUE,   features = c(\"x1\", \"x2\"),  # Focus on just two features for clarity   aggregation_fun = mean ) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`.  loco_classif$compute() #> Key: <feature> #>    feature importance #>     <char>      <num> #> 1:      x1 0.04477612 #> 2:      x2 0.00000000  # Show classification importance loco_classif$importance |>   knitr::kable(digits = 4, caption = \"LOCO importance for classification\") # Look at observation-wise data classif_data <- head(loco_classif$obs_losses, 8)  classif_data |>   knitr::kable(caption = \"Observation-wise classification losses\") # Access feature-specific predictions (available with obs_loss = TRUE) head(loco_classif$predictions) #>    feature iteration iter_refit          prediction #>     <char>     <int>      <int>              <list> #> 1:      x1         1          1 <PredictionClassif> #> 2:      x2         1          1 <PredictionClassif>  # Get probabilities for a specific feature and iteration   if (nrow(loco_classif$predictions) > 0) {   feature_pred <- loco_classif$predictions[1, ]$prediction[[1]]      # Access both predicted classes and probabilities   # Note: probabilities are only available because we used predict_type = \"prob\"   head(feature_pred$response)  # Predicted classes   head(feature_pred$prob)      # Probability matrix } #>               A         B #> [1,] 0.06060606 0.9393939 #> [2,] 0.06060606 0.9393939 #> [3,] 0.11111111 0.8888889 #> [4,] 0.09523810 0.9047619 #> [5,] 0.80851064 0.1914894 #> [6,] 0.06060606 0.9393939  # Compare with reference model (full model for LOCO) ref_pred <- loco_classif$resample_result$prediction(1) head(ref_pred$prob)  # Reference model probabilities #>               A          B #> [1,] 0.06060606 0.93939394 #> [2,] 0.06060606 0.93939394 #> [3,] 0.11111111 0.88888889 #> [4,] 0.09523810 0.90476190 #> [5,] 0.91891892 0.08108108 #> [6,] 0.06060606 0.93939394"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"scenario-1-interaction-effects","dir":"Articles","previous_headings":"","what":"Scenario 1: Interaction Effects","title":"Perturbation-based Feature Importance Methods","text":"scenario demonstrates marginal methods (PFI) can miss important interaction effects conditional methods (CFI) capture: Causal Structure: key insight: x1 x2 direct effects - affect y interaction (thick red arrow). However, PFI still show important permuting either feature destroys crucial interaction term.","code":"# Generate interaction scenario task_int <- sim_dgp_interactions(n = 1000) data_int <- task_int$data()"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"analysis","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects","what":"Analysis","title":"Perturbation-based Feature Importance Methods","text":"Let’s analyze interaction scenario \\(y = 2 \\cdot x_1 \\cdot x_2 + x_3 + \\epsilon\\). Note x1 x2 main effects.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"pfi-on-interactions","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects > Analysis","what":"PFI on Interactions","title":"Perturbation-based Feature Importance Methods","text":"Expected: x1 x2 show high importance PFI permuting either feature destroys interaction term x1×x2, crucial prediction. demonstrates key limitation PFI interactions.","code":"pfi_int <- PFI$new(   task = task_int,   learner = learner,   measure = measure,   resampling = resampling,   iters_perm = 5 )  # Compute importance scores pfi_int_results <- pfi_int$compute(relation = \"difference\") pfi_int_results #> Key: <feature> #>    feature  importance         sd #>     <char>       <num>      <num> #> 1:  noise1 0.011151109 0.06592808 #> 2:  noise2 0.007140595 0.04380357 #> 3:      x1 2.396084412 0.49513544 #> 4:      x2 2.081716910 0.37950171 #> 5:      x3 2.022106644 0.21062276"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"cfi-on-interactions","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects > Analysis","what":"CFI on Interactions","title":"Perturbation-based Feature Importance Methods","text":"CFI preserves joint distribution, better capture interaction effect: Expected: CFI show somewhat lower importance x1 x2 compared PFI better preserves interaction structure conditional sampling, providing nuanced importance estimate.","code":"# Create ARF sampler for the interaction task sampler_int = ARFSampler$new(task = task_int, finite_bounds = \"local\")  cfi_int <- CFI$new(   task = task_int,   learner = learner,   measure = measure,   resampling = resampling,   iters_perm = 5,   sampler = sampler_int )  # Compute importance scores cfi_int_results <- cfi_int$compute(relation = \"difference\") cfi_int_results #> Key: <feature> #>    feature  importance         sd #>     <char>       <num>      <num> #> 1:  noise1 -0.02125244 0.02681519 #> 2:  noise2 -0.02958837 0.03942009 #> 3:      x1  1.00940147 0.22695657 #> 4:      x2  1.03622368 0.18141070 #> 5:      x3  0.76545513 0.16371265"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"rfi-on-interactions-targeted-conditional-questions","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects > Analysis","what":"RFI on Interactions: Targeted Conditional Questions","title":"Perturbation-based Feature Importance Methods","text":"RFI’s unique strength answering specific conditional questions. Let’s explore happens condition different features: RFI Results: x1 given x2: 2.095 (important x1 condition x2) x2 given x1: 1.912 (important x2 condition x1) x3 given x2: 1.964 (important x3 condition x2) Key insight: pure interaction case (y = 2·x1·x2 + x3), condition one interacting feature, becomes extremely important matter together. demonstrates RFI’s power answer targeted questions like “Given already know x2, much x1 add?”","code":"# RFI conditioning on x2: \"How important is x1 given we know x2?\" rfi_int_x2 <- RFI$new(   task = task_int,   learner = learner,   measure = measure,   resampling = resampling,   conditioning_set = \"x2\",  # Condition on x2   iters_perm = 5,   sampler = sampler_int ) rfi_int_x2_results <- rfi_int_x2$compute(relation = \"difference\")  # RFI conditioning on x1: \"How important is x2 given we know x1?\"   rfi_int_x1 <- RFI$new(   task = task_int,   learner = learner,   measure = measure,   resampling = resampling,   conditioning_set = \"x1\",  # Condition on x1   iters_perm = 5,   sampler = sampler_int ) rfi_int_x1_results <- rfi_int_x1$compute(relation = \"difference\")"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"comparing-methods-on-interactions","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects > Analysis","what":"Comparing Methods on Interactions","title":"Perturbation-based Feature Importance Methods","text":"Let’s compare methods handle interaction:  RFI Conditional Summary: x1 given x2 importance 2.095, x2 given x1 importance 1.912, x3 given x2 importance 1.964. shows RFI reveals conditional dependencies pure marginal methods miss.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"key-insights-interaction-effects","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects","what":"Key Insights: Interaction Effects","title":"Perturbation-based Feature Importance Methods","text":"CFI vs PFI Interacting Features Important insight interaction effects: example illustrates crucial subtlety PFI interactions. x1 x2 main effects, PFI still correctly identifies important permuting either feature destroys interaction term x1×x2, crucial prediction. key limitation PFI distinguish main effects interaction effects - measures total contribution including interactions.","code":"# Combine results and calculate ratios comp_int <- rbindlist(list(   pfi_int_results[, .(feature, importance, method = \"PFI\")],   cfi_int_results[, .(feature, importance, method = \"CFI\")] ))  # Calculate the ratio of CFI to PFI importance for interacting features int_ratio <- dcast(comp_int[feature %in% c(\"x1\", \"x2\")],                     feature ~ method, value.var = \"importance\") int_ratio[, cfi_pfi_ratio := CFI / PFI] setnames(int_ratio, c(\"PFI\", \"CFI\"), c(\"pfi_importance\", \"cfi_importance\"))  int_ratio |>    knitr::kable(     digits = 3,     caption = \"CFI vs PFI for Interacting Features\"   )"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"scenario-2-confounding","dir":"Articles","previous_headings":"","what":"Scenario 2: Confounding","title":"Perturbation-based Feature Importance Methods","text":"scenario shows hidden confounders affect importance estimates conditioning can help: Causal Structure: red arrows show confounding paths: hidden confounder creates spurious correlations x1, x2, proxy, y. blue arrows show true direct causal effects. Note independent truly independent (confounding) proxy provides noisy measurement confounder. observable confounder scenario (used later), confounder H included feature dataset, allowing direct conditioning rather relying noisy proxy.  Key insight: hidden confounder creates spurious correlations x1, x2, y (red paths), making appear important truly . RFI conditioning proxy (measures confounder) help isolate true direct effects (blue paths).","code":"# Generate confounding scenario   task_conf <- sim_dgp_confounded(n = 1000) data_conf <- task_conf$data()"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"analysis-1","dir":"Articles","previous_headings":"Scenario 2: Confounding","what":"Analysis","title":"Perturbation-based Feature Importance Methods","text":"Now let’s analyze confounding scenario hidden confounder affects features outcome.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"pfi-on-confounded-data","dir":"Articles","previous_headings":"Scenario 2: Confounding > Analysis","what":"PFI on Confounded Data","title":"Perturbation-based Feature Importance Methods","text":"","code":"pfi_conf <- PFI$new(   task = task_conf,   learner = learner,   measure = measure,   resampling = resampling,   iters_perm = 5 )  pfi_conf_results <- pfi_conf$compute(relation = \"difference\") pfi_conf_results #> Key: <feature> #>        feature importance         sd #>         <char>      <num>      <num> #> 1: independent  1.5558886 0.14955983 #> 2:       proxy  0.2164108 0.06458989 #> 3:          x1  1.6184945 0.17254140 #> 4:          x2  2.0832008 0.43771483"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"rfi-conditioning-on-proxy","dir":"Articles","previous_headings":"Scenario 2: Confounding > Analysis","what":"RFI Conditioning on Proxy","title":"Perturbation-based Feature Importance Methods","text":"RFI can condition proxy help isolate direct effects:","code":"# Create sampler for confounding task sampler_conf = ARFSampler$new(   task = task_conf,   verbose = FALSE,   finite_bounds = \"local\" )  # RFI conditioning on the proxy rfi_conf <- RFI$new(   task = task_conf,   learner = learner,   measure = measure,   resampling = resampling,   conditioning_set = \"proxy\",  # Condition on proxy to reduce confounding   iters_perm = 5,   sampler = sampler_conf )  rfi_conf_results <- rfi_conf$compute(relation = \"difference\") rfi_conf_results #> Key: <feature> #>        feature importance         sd #>         <char>      <num>      <num> #> 1: independent  1.5370390 0.12262544 #> 2:       proxy  0.0000000 0.00000000 #> 3:          x1  0.5846276 0.08205033 #> 4:          x2  0.7221616 0.09826116"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"also-trying-cfi-for-comparison","dir":"Articles","previous_headings":"Scenario 2: Confounding > Analysis","what":"Also trying CFI for comparison","title":"Perturbation-based Feature Importance Methods","text":"","code":"cfi_conf <- CFI$new(   task = task_conf,   learner = learner,   measure = measure,   resampling = resampling,   iters_perm = 5,   sampler = sampler_conf )  cfi_conf_results <- cfi_conf$compute(relation = \"difference\") cfi_conf_results #> Key: <feature> #>        feature importance         sd #>         <char>      <num>      <num> #> 1: independent  1.5247804 0.15991909 #> 2:       proxy  0.0000000 0.00000000 #> 3:          x1  0.6137199 0.07982261 #> 4:          x2  0.6813112 0.08152289"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"observable-confounder-scenario","dir":"Articles","previous_headings":"Scenario 2: Confounding > Analysis","what":"Observable Confounder Scenario","title":"Perturbation-based Feature Importance Methods","text":"many real-world situations, confounders actually observable (e.g., demographics, baseline characteristics). Let’s explore RFI performs can condition directly true confounder: Key Results: x1 importance: PFI = 0.594, RFI|confounder = 0.098 x2 importance: PFI = 0.612, RFI|confounder = 0.142 independent importance: PFI = 1.444, RFI|confounder = 1.407 Insight: conditioning true confounder, RFI show reduced importance x1 x2 (since much apparent importance due confounding) independent maintains importance (since ’s truly causally related y).","code":"# Generate scenario where confounder is observable task_conf_obs <- sim_dgp_confounded(n = 1000, hidden = FALSE)  # Now we can condition directly on the true confounder sampler_conf_obs = ARFSampler$new(   task = task_conf_obs,   verbose = FALSE,   finite_bounds = \"local\" )  # RFI conditioning on the true confounder (not just proxy) rfi_conf_obs <- RFI$new(   task = task_conf_obs,   learner = learner,   measure = measure,   resampling = resampling,   conditioning_set = \"confounder\",  # Condition on true confounder   iters_perm = 5,   sampler = sampler_conf_obs )  rfi_conf_obs_results <- rfi_conf_obs$compute(relation = \"difference\")  # Compare with PFI on the same data pfi_conf_obs <- PFI$new(   task = task_conf_obs,   learner = learner,   measure = measure,   resampling = resampling,   iters_perm = 5 ) pfi_conf_obs_results <- pfi_conf_obs$compute(relation = \"difference\")"},{"path":[]},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"key-insights-confounding-effects","dir":"Articles","previous_headings":"Scenario 2: Confounding","what":"Key Insights: Confounding Effects","title":"Perturbation-based Feature Importance Methods","text":"Effect Conditioning Proxy Confounded Scenario confounding scenario, observed: PFI shows confounded effects: Without accounting confounders, PFI overestimates importance x1 x2 due spurious correlation y hidden confounder. RFI conditioning proxy reduces bias: conditioning proxy (noisy measurement confounder), RFI can partially isolate direct effects, though confounding remains due measurement error. RFI conditioning true confounder removes bias: confounder observable can condition directly , RFI dramatically reduces apparent importance x1 x2, revealing true direct effects. CFI partially accounts confounding: conditional sampling, CFI captures confounding structure target specific confounders like RFI can.","code":"# Show how conditioning affects importance estimates conf_wide <- dcast(comp_conf_long, feature ~ method, value.var = \"importance\") conf_summary <- conf_wide[, .(   feature,   pfi_importance = round(PFI, 3),   cfi_importance = round(CFI, 3),   rfi_proxy_importance = round(RFI, 3),   pfi_rfi_diff = round(PFI - RFI, 3) )]  conf_summary |>    knitr::kable(     caption = \"Effect of Conditioning on Proxy in Confounded Scenario\"   )"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"scenario-3-correlated-features","dir":"Articles","previous_headings":"","what":"Scenario 3: Correlated Features","title":"Perturbation-based Feature Importance Methods","text":"scenario demonstrates fundamental difference marginal conditional methods features highly correlated: Causal Structure: Key feature: x1 x2 nearly identical (correlation ≈ 0.999) x1 causal effect y. x2 spurious predictor - highly correlated causal feature causal .","code":"# Generate correlated features scenario task_cor <- sim_dgp_correlated(n = 1000) data_cor <- task_cor$data()"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"analysis-2","dir":"Articles","previous_headings":"Scenario 3: Correlated Features","what":"Analysis","title":"Perturbation-based Feature Importance Methods","text":"Let’s analyze different methods handle highly correlated features:","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"pfi-on-correlated-features","dir":"Articles","previous_headings":"Scenario 3: Correlated Features > Analysis","what":"PFI on Correlated Features","title":"Perturbation-based Feature Importance Methods","text":"Expected: PFI show high importance x1 x2, even though x1 true causal effect. happens x2 highly correlated x1, permuting x2 destroys predictive information x1.","code":"pfi_cor <- PFI$new(   task = task_cor,   learner = learner,   measure = measure,   resampling = resampling,   iters_perm = 5 )  pfi_cor_results <- pfi_cor$compute(relation = \"difference\") pfi_cor_results #> Key: <feature> #>    feature    importance          sd #>     <char>         <num>       <num> #> 1:      x1  2.6153147412 0.314095423 #> 2:      x2  1.7406419236 0.186699236 #> 3:      x3  1.6022695961 0.148962767 #> 4:      x4 -0.0002146506 0.001726942"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"cfi-on-correlated-features","dir":"Articles","previous_headings":"Scenario 3: Correlated Features > Analysis","what":"CFI on Correlated Features","title":"Perturbation-based Feature Importance Methods","text":"Expected: CFI show high importance x1 (true causal feature) near-zero importance x2 (spurious correlated feature) conditional sampling preserves correlation structure can distinguish causal spurious predictors.","code":"# Create ARF sampler for correlated task sampler_cor = ARFSampler$new(task = task_cor, finite_bounds = \"local\")  cfi_cor <- CFI$new(   task = task_cor,   learner = learner,   measure = measure,   resampling = resampling,   iters_perm = 5,   sampler = sampler_cor )  cfi_cor_results <- cfi_cor$compute(relation = \"difference\") cfi_cor_results #> Key: <feature> #>    feature   importance          sd #>     <char>        <num>       <num> #> 1:      x1  0.138252296 0.039126754 #> 2:      x2  0.100258615 0.022697687 #> 3:      x3  1.138774915 0.105238356 #> 4:      x4 -0.001959176 0.001837566"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"rfi-to-answer-conditional-questions","dir":"Articles","previous_headings":"Scenario 3: Correlated Features > Analysis","what":"RFI to Answer Conditional Questions","title":"Perturbation-based Feature Importance Methods","text":"RFI Results: - x2 given x1: 0.078 (much x2 add already know x1?) - x1 given x2: 0.134 (much x1 add already know x2?) Expected: conditioning x1, importance x2 near zero (vice versa) ’re almost identical - knowing one tells almost everything .","code":"# RFI conditioning on x1: \"How important is x2 given we know x1?\" rfi_cor_x1 <- RFI$new(   task = task_cor,   learner = learner,   measure = measure,   resampling = resampling,   conditioning_set = \"x1\",   iters_perm = 5,   sampler = sampler_cor ) rfi_cor_x1_results <- rfi_cor_x1$compute(relation = \"difference\")  # RFI conditioning on x2: \"How important is x1 given we know x2?\" rfi_cor_x2 <- RFI$new(   task = task_cor,   learner = learner,   measure = measure,   resampling = resampling,   conditioning_set = \"x2\",   iters_perm = 5,   sampler = sampler_cor ) rfi_cor_x2_results <- rfi_cor_x2$compute(relation = \"difference\")"},{"path":[]},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"key-insights-correlated-features","dir":"Articles","previous_headings":"Scenario 3: Correlated Features","what":"Key Insights: Correlated Features","title":"Perturbation-based Feature Importance Methods","text":"CFI vs PFI Highly Correlated Features correlated features scenario: PFI overestimates importance spurious features: PFI assigns high importance x1 (causal) x2 (spurious) ’re highly correlated. Permuting x2 destroys information x1, making x2 appear important even though causal effect. CFI correctly identifies causal features: preserving correlation structure conditional sampling, CFI can distinguish x1 (truly causal) x2 (merely correlated), assigning high importance x1. RFI reveals redundancy: conditioning x1, additional importance x2 near zero (vice versa), correctly identifying redundancy prediction. Practical implication: PFI mislead think features important. CFI correctly shows x1 truly important, x2 just along ride due correlation.","code":"cor_ratio |>    knitr::kable(     digits = 3,     caption = \"CFI vs PFI for Highly Correlated Features\"   )"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"scenario-4-independent-features-baseline","dir":"Articles","previous_headings":"","what":"Scenario 4: Independent Features (Baseline)","title":"Perturbation-based Feature Importance Methods","text":"provide baseline comparison, let’s examine scenario feature importance methods produce similar results: Causal Structure: simplest scenario: features independent, interactions, confounding. feature direct effect y (effect case noise).","code":"# Generate independent features scenario task_ind <- sim_dgp_independent(n = 1000) data_ind <- task_ind$data()"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"running-all-methods-on-independent-features","dir":"Articles","previous_headings":"Scenario 4: Independent Features (Baseline)","what":"Running All Methods on Independent Features","title":"Perturbation-based Feature Importance Methods","text":"First PFI: Now CFI ARF sampler: RFI empty conditioning set, basically equivalent PFI different sampler: now visualize:","code":"# PFI pfi_ind <- PFI$new(   task = task_ind,   learner = learner,   measure = measure,   resampling = resampling,   iters_perm = 5 ) pfi_ind_results <- pfi_ind$compute(relation = \"difference\") sampler_ind = ARFSampler$new(task = task_ind, finite_bounds = \"local\") cfi_ind <- CFI$new(   task = task_ind,   learner = learner,   measure = measure,   resampling = resampling,   iters_perm = 5,   sampler = sampler_ind ) cfi_ind_results <- cfi_ind$compute(relation = \"difference\") rfi_ind <- RFI$new(   task = task_ind,   learner = learner,   measure = measure,   resampling = resampling,   conditioning_set = character(0),  # Empty set   iters_perm = 5,   sampler = sampler_ind ) rfi_ind_results <- rfi_ind$compute(relation = \"difference\")"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"agreement-between-methods","dir":"Articles","previous_headings":"Scenario 4: Independent Features (Baseline)","what":"Agreement Between Methods","title":"Perturbation-based Feature Importance Methods","text":"Method Agreement Independent Features Key insight: independent features complex relationships, three methods (PFI, CFI, RFI) produce similar importance estimates. confirms differences observe Scenarios 1 2 truly due interactions confounding, artifacts methods .","code":"# Calculate coefficient of variation for each feature across methods comp_ind_wide <- dcast(comp_ind_long, feature ~ method, value.var = \"importance\") comp_ind_wide[, `:=`(   mean_importance = rowMeans(.SD),   sd_importance = apply(.SD, 1, sd),   cv = apply(.SD, 1, sd) / rowMeans(.SD) ), .SDcols = c(\"PFI\", \"CFI\", \"RFI\")]  comp_ind_wide[, .(   feature,   mean_importance = round(mean_importance, 3),   cv = round(cv, 3),   agreement = ifelse(cv < 0.1, \"High\", ifelse(cv < 0.2, \"Moderate\", \"Low\")) )] |>   knitr::kable(     caption = \"Method Agreement on Independent Features\",     col.names = c(\"Feature\", \"Mean Importance\", \"Coef. of Variation\", \"Agreement Level\")   )"},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"key-insights-independent-features","dir":"Articles","previous_headings":"Scenario 4: Independent Features (Baseline)","what":"Key Insights: Independent Features","title":"Perturbation-based Feature Importance Methods","text":"baseline scenario independent features: methods agree: PFI, CFI, RFI produce nearly identical importance estimates features truly independent. Validates methodology: agreement methods confirms differences scenarios due data structure, method artifacts. Noise correctly identified: methods correctly assign near-zero importance noise features.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"key-takeaways","dir":"Articles","previous_headings":"","what":"Key Takeaways","title":"Perturbation-based Feature Importance Methods","text":"four scenarios, ’ve demonstrated: PFI simple fast can miss interaction effects, underestimate importance correlated features, affected confounding CFI captures feature dependencies interactions conditional sampling, correctly handling correlated features RFI allows targeted conditioning isolate specific relationships reveal redundancy Use PFI features believed independent (Scenario 4) want quick baseline importance ranking Use CFI suspect feature interactions, correlations, dependencies (Scenarios 1 & 3) want sophisticated analysis respects feature relationships Use RFI specific conditional questions: “important feature X given already know feature Y?” (Scenarios 1, 2 & 3). Essential feature selection understanding incremental value. methods benefit cross-validation multiple permutation iterations stability ARF-based conditional sampling (used CFI/RFI) computationally intensive marginal sampling choice conditioning set RFI requires domain knowledge","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/perturbation-importance.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further Reading","title":"Perturbation-based Feature Importance Methods","text":"details methods theoretical foundations, see: Breiman (2001) original PFI formulation Strobl et al. (2008) limitations PFI correlated features Watson & Wright (2021) conditional sampling ARF König et al. (2021) relative feature importance Ewald et al. (2024) comprehensive review feature importance methods","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/sage-methods.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Shapley Additive Global Importance (SAGE)","text":"Shapley Additive Global Importance (SAGE) feature importance method based cooperative game theory uses Shapley values fairly distribute total prediction performance among features. Unlike permutation-based methods measure drop performance features perturbed, SAGE measures much feature contributes model’s overall performance marginalizing (removing) features. key insight SAGE provides complete decomposition model’s performance: sum SAGE values equals difference model’s performance performance features marginalized. xplainfi provides two implementations SAGE: MarginalSAGE: Marginalizes features independently (standard SAGE) ConditionalSAGE: Marginalizes features conditionally using ARF sampling","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/sage-methods.html","id":"demonstration-with-correlated-features","dir":"Articles","previous_headings":"","what":"Demonstration with Correlated Features","title":"Shapley Additive Global Importance (SAGE)","text":"showcase difference Marginal Conditional SAGE, ’ll use sim_dgp_correlated() function creates highly correlated features. similar PFI CFI behave differently correlated features. Model: \\[X_1 \\sim N(0,1)\\] \\[X_2 = X_1 + \\varepsilon_2, \\quad \\varepsilon_2 \\sim N(0, 0.05^2)\\] \\[X_3 \\sim N(0,1), \\quad X_4 \\sim N(0,1)\\] \\[Y = 2 \\cdot X_1 + X_3 + \\varepsilon\\] \\(\\varepsilon \\sim N(0, 0.2^2)\\). Key properties: x1 direct causal effect y (β=2.0) x2 highly correlated x1 (r ≈ 0.999) causal effect y x3 independent causal effect (β=1.0) x4 independent noise (β=0) Expected behavior: Marginal SAGE: show high importance x1 x2 due correlation, even though x2 causal effect Conditional SAGE: show high importance x1 near-zero importance x2 (correctly identifying spurious predictor) Let’s set learner measure. ’ll use random forest instantiate resampling ensure methods see data:","code":"set.seed(123) task = sim_dgp_correlated(n = 1000)  # Check the correlation structure task_data = task$data() correlation_matrix = cor(task_data[, c(\"x1\", \"x2\", \"x3\", \"x4\")]) round(correlation_matrix, 3) #>        x1     x2     x3     x4 #> x1  1.000  0.999 -0.019 -0.003 #> x2  0.999  1.000 -0.018 -0.003 #> x3 -0.019 -0.018  1.000  0.051 #> x4 -0.003 -0.003  0.051  1.000 learner = lrn(\"regr.ranger\") measure = msr(\"regr.mse\") resampling = rsmp(\"holdout\") resampling$instantiate(task)"},{"path":"https://jemus42.github.io/xplainfi/articles/sage-methods.html","id":"marginal-sage","dir":"Articles","previous_headings":"","what":"Marginal SAGE","title":"Shapley Additive Global Importance (SAGE)","text":"Marginal SAGE marginalizes features independently averaging predictions reference dataset. standard SAGE implementation described original paper. Let’s visualize results:  can also keep track SAGE value approximation across permutations:","code":"# Create Marginal SAGE instance marginal_sage = MarginalSAGE$new(   task = task,   learner = learner,   measure = measure,   resampling = resampling,   n_permutations = 30L,  # More permutations for stable results   max_reference_size = 100L,   batch_size = 5000L )  # Compute SAGE values marginal_sage$compute() #> Key: <feature> #>    feature   importance #>     <char>        <num> #> 1:      x1  2.337115391 #> 2:      x2  1.681811032 #> 3:      x3  0.912696736 #> 4:      x4 -0.001287703 marginal_sage$plot_convergence()"},{"path":"https://jemus42.github.io/xplainfi/articles/sage-methods.html","id":"conditional-sage","dir":"Articles","previous_headings":"","what":"Conditional SAGE","title":"Shapley Additive Global Importance (SAGE)","text":"Conditional SAGE uses conditional sampling (via ARF default) marginalize features preserving dependencies remaining features. can provide different insights, especially features correlated. Let’s visualize conditional SAGE results:","code":"# Create Conditional SAGE instance conditional_sage = ConditionalSAGE$new(   task = task,   learner = learner,   measure = measure,   resampling = resampling,   n_permutations = 30L,   max_reference_size = 100L ) #> ℹ No <ConditionalSampler> provided, using <ARFSampler> with default settings.  # Compute SAGE values conditional_sage$compute(batch_size = 5000L) #> Key: <feature> #>    feature importance #>     <char>      <num> #> 1:      x1  2.6082408 #> 2:      x2  3.2397583 #> 3:      x3  0.3593231 #> 4:      x4 -0.4862185 conditional_sage$plot_convergence()"},{"path":"https://jemus42.github.io/xplainfi/articles/sage-methods.html","id":"comparison-of-methods","dir":"Articles","previous_headings":"","what":"Comparison of Methods","title":"Shapley Additive Global Importance (SAGE)","text":"Let’s compare two SAGE methods side side:  Let’s also create correlation plot see similar rankings :","code":"#> `geom_smooth()` using formula = 'y ~ x'"},{"path":"https://jemus42.github.io/xplainfi/articles/sage-methods.html","id":"interpretation","dir":"Articles","previous_headings":"Comparison of Methods","what":"Interpretation","title":"Shapley Additive Global Importance (SAGE)","text":"results demonstrate key difference marginal conditional SAGE: Marginal SAGE treats feature independently, highly correlated features x1 x2 receive substantial importance scores reflecting individual marginal contributions. Conditional SAGE accounts feature dependencies conditional sampling. marginalizing x1, properly conditions x2 (vice versa), leading lower importance scores correlated features since provide redundant information. Independent feature x3 shows similar importance methods since doesn’t depend features. Noise feature x4 correctly receives near-zero importance methods. pattern mirrors difference PFI CFI: marginal methods show inflated importance correlated features, conditional methods provide accurate assessment feature’s unique contribution.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/sage-methods.html","id":"comparison-with-pfi-and-cfi","dir":"Articles","previous_headings":"","what":"Comparison with PFI and CFI","title":"Shapley Additive Global Importance (SAGE)","text":"reference, let’s compare SAGE methods analogous PFI CFI methods data:  Key Observations: Marginal methods (PFI, Marginal SAGE) assign high importance correlated features x1 x2 Conditional methods (CFI, Conditional SAGE) reduce importance correlated features, accounting redundancy Independent feature x3 receives consistent importance across methods Noise feature x4 correctly identified unimportant methods demonstrates marginal vs conditional distinction fundamental concept applies across different importance method families.","code":"# Quick PFI and CFI comparison for context pfi = PFI$new(task, learner, measure) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. cfi = CFI$new(task, learner, measure)  #> ℹ No <ConditionalSampler> provided, using <ARFSampler> with default settings. #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`.  pfi_results = pfi$compute() cfi_results = cfi$compute()  # Create comparison data frame method_comparison = data.frame(   feature = rep(c(\"x1\", \"x2\", \"x3\", \"x4\"), 4),   importance = c(     pfi_results$importance,     cfi_results$importance,     marginal_results$importance,     conditional_results$importance   ),   method = rep(c(\"PFI\", \"CFI\", \"Marginal SAGE\", \"Conditional SAGE\"), each = 4),   approach = rep(c(\"Marginal\", \"Conditional\", \"Marginal\", \"Conditional\"), each = 4) )  # Create comparison plot ggplot(method_comparison, aes(x = feature, y = importance, fill = method)) +   geom_col(position = \"dodge\", alpha = 0.8) +   scale_fill_manual(values = c(     \"PFI\" = \"lightblue\",      \"CFI\" = \"blue\",      \"Marginal SAGE\" = \"lightcoral\",      \"Conditional SAGE\" = \"darkred\"   )) +   labs(     title = \"Comparison: PFI/CFI vs Marginal/Conditional SAGE\",     subtitle = \"Both pairs show similar patterns: marginal methods inflate correlated feature importance\",     x = \"Features\",      y = \"Importance Value\",     fill = \"Method\"   ) +   theme_minimal(base_size = 14) +   theme(axis.text.x = element_text(angle = 45, hjust = 1))"},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Simulation Settings for Feature Importance Methods","text":"xplainfi package provides several data generating processes (DGPs) designed illustrate specific strengths weaknesses different feature importance methods. DGP focuses one primary challenge make differences methods clear. article provides comprehensive overview simulation settings, including mathematical formulations causal structures visualized directed acyclic graphs (DAGs).","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"overview-of-simulation-settings","dir":"Articles","previous_headings":"","what":"Overview of Simulation Settings","title":"Simulation Settings for Feature Importance Methods","text":"Overview simulation settings expected method behavior","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"correlated-features-dgp","dir":"Articles","previous_headings":"","what":"1. Correlated Features DGP","title":"Simulation Settings for Feature Importance Methods","text":"DGP creates highly correlated spurious predictor illustrate fundamental difference marginal conditional importance methods.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model","dir":"Articles","previous_headings":"1. Correlated Features DGP","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[X_1 \\sim N(0,1)\\] \\[X_2 = X_1 + \\varepsilon_2, \\quad \\varepsilon_2 \\sim N(0, 0.05^2)\\] \\[X_3 \\sim N(0,1), \\quad X_4 \\sim N(0,1)\\] \\[Y = 2 \\cdot X_1 + X_3 + \\varepsilon\\] \\(\\varepsilon \\sim N(0, 0.2^2)\\).","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure","dir":"Articles","previous_headings":"1. Correlated Features DGP","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG correlated features DGP","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example","dir":"Articles","previous_headings":"1. Correlated Features DGP","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"set.seed(123) task <- sim_dgp_correlated(n = 500)  # Check correlation between X1 and X2 cor(task$data()[, c(\"x1\", \"x2\")]) #>           x1        x2 #> x1 1.0000000 0.9986492 #> x2 0.9986492 1.0000000  # True coefficients: x1=2.0, x2=0, x3=1.0, x4=0 # Note: x2 is highly correlated with x1 but has NO causal effect!"},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"expected-behavior","dir":"Articles","previous_headings":"1. Correlated Features DGP","what":"Expected Behavior","title":"Simulation Settings for Feature Importance Methods","text":"Marginal methods (PFI, Marginal SAGE): falsely assign high importance x2 permuting breaks correlation x1, creating unrealistic data confuses model Conditional methods (CFI, Conditional SAGE): correctly assign near-zero importance x2 conditional sampling preserves correlation, revealing x2 adds information beyond x1 provides Key insight: x2 spurious predictor - appears predictive due correlation x1 causal effect y","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"mediated-effects-dgp","dir":"Articles","previous_headings":"","what":"2. Mediated Effects DGP","title":"Simulation Settings for Feature Importance Methods","text":"DGP demonstrates difference total direct causal effects. features affect outcome mediators.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model-1","dir":"Articles","previous_headings":"2. Mediated Effects DGP","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[\\text{exposure} \\sim N(0,1), \\quad \\text{direct} \\sim N(0,1)\\] \\[\\text{mediator} = 0.8 \\cdot \\text{exposure} + 0.6 \\cdot \\text{direct} + \\varepsilon_m\\] \\[Y = 1.5 \\cdot \\text{mediator} + 0.5 \\cdot \\text{direct} + \\varepsilon\\] \\(\\varepsilon_m \\sim N(0, 0.3^2)\\) \\(\\varepsilon \\sim N(0, 0.2^2)\\).","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure-1","dir":"Articles","previous_headings":"2. Mediated Effects DGP","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG mediated effects DGP","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example-1","dir":"Articles","previous_headings":"2. Mediated Effects DGP","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"set.seed(123) task <- sim_dgp_mediated(n = 500)  # Calculate total effect of exposure # Total effect = 0.8 * 1.5 = 1.2 (through mediator) # Direct effect = 0 (no direct path to Y)"},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"expected-behavior-1","dir":"Articles","previous_headings":"2. Mediated Effects DGP","what":"Expected Behavior","title":"Simulation Settings for Feature Importance Methods","text":"PFI: Shows total effects (exposure appears important effect ≈ 1.2) CFI: Shows direct effects (exposure appears unimportant conditioning mediator) RFI mediator: show direct effects similar CFI","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"confounding-dgp","dir":"Articles","previous_headings":"","what":"3. Confounding DGP","title":"Simulation Settings for Feature Importance Methods","text":"DGP includes confounder affects features outcome.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model-2","dir":"Articles","previous_headings":"3. Confounding DGP","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[H \\sim N(0,1) \\quad \\text{(confounder)}\\] \\[X_1 = H + \\varepsilon_1, \\quad X_2 = H + \\varepsilon_2\\] \\[\\text{proxy} = H + \\varepsilon_p, \\quad \\text{independent} \\sim N(0,1)\\] \\[Y = H + 0.5 \\cdot X_1 + 0.5 \\cdot X_2 + \\text{independent} + \\varepsilon\\] \\(\\varepsilon \\sim N(0, 0.5^2)\\) independently.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure-2","dir":"Articles","previous_headings":"3. Confounding DGP","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG confounding DGP","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example-2","dir":"Articles","previous_headings":"3. Confounding DGP","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"set.seed(123) # Hidden confounder scenario (default) task_hidden <- sim_dgp_confounded(n = 500, hidden = TRUE) task_hidden$feature_names  # proxy available but not confounder #> [1] \"independent\" \"proxy\"       \"x1\"          \"x2\"  # Observable confounder scenario task_observed <- sim_dgp_confounded(n = 500, hidden = FALSE) task_observed$feature_names  # both confounder and proxy available #> [1] \"confounder\"  \"independent\" \"proxy\"       \"x1\"          \"x2\""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"expected-behavior-2","dir":"Articles","previous_headings":"3. Confounding DGP","what":"Expected Behavior","title":"Simulation Settings for Feature Importance Methods","text":"PFI: show inflated importance x1 x2 due confounding CFI: partially account confounding conditional sampling RFI conditioning proxy: reduce confounding bias","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"interaction-effects-dgp","dir":"Articles","previous_headings":"","what":"4. Interaction Effects DGP","title":"Simulation Settings for Feature Importance Methods","text":"DGP demonstrates pure interaction effect features main effects.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model-3","dir":"Articles","previous_headings":"4. Interaction Effects DGP","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[Y = 2 \\cdot X_1 \\cdot X_2 + X_3 + \\varepsilon\\] \\(X_j \\sim N(0,1)\\) independently \\(\\varepsilon \\sim N(0, 0.5^2)\\).","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure-3","dir":"Articles","previous_headings":"4. Interaction Effects DGP","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG interaction effects DGP","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example-3","dir":"Articles","previous_headings":"4. Interaction Effects DGP","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"set.seed(123) task <- sim_dgp_interactions(n = 500)  # Note: X1 and X2 have NO main effects # Their importance comes ONLY through their interaction"},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"expected-behavior-3","dir":"Articles","previous_headings":"4. Interaction Effects DGP","what":"Expected Behavior","title":"Simulation Settings for Feature Importance Methods","text":"PFI: assign near-zero importance x1 x2 (marginal effect) CFI: capture interaction assign high importance x1 x2 LOCO: May show high importance x1 x2 (removing either breaks interaction) LOCI: show near-zero importance x1 x2 (individually useless)","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"independent-features-dgp-baseline","dir":"Articles","previous_headings":"","what":"5. Independent Features DGP (Baseline)","title":"Simulation Settings for Feature Importance Methods","text":"baseline scenario features independent effects additive. importance methods give similar results.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model-4","dir":"Articles","previous_headings":"5. Independent Features DGP (Baseline)","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[Y = 2.0 \\cdot X_1 + 1.0 \\cdot X_2 + 0.5 \\cdot X_3 + \\varepsilon\\] \\(X_j \\sim N(0,1)\\) independently \\(\\varepsilon \\sim N(0, 0.2^2)\\).","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure-4","dir":"Articles","previous_headings":"5. Independent Features DGP (Baseline)","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG independent features DGP","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example-4","dir":"Articles","previous_headings":"5. Independent Features DGP (Baseline)","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"set.seed(123) task <- sim_dgp_independent(n = 500)  # All methods should rank features consistently: # important1 > important2 > important3 > unimportant1,2 ≈ 0"},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"expected-behavior-4","dir":"Articles","previous_headings":"5. Independent Features DGP (Baseline)","what":"Expected Behavior","title":"Simulation Settings for Feature Importance Methods","text":"methods: rank features consistently true effect sizes Ground truth: important1 (2.0) > important2 (1.0) > important3 (0.5) > unimportant1,2 (0)","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"ewald-et-al--2024-dgp","dir":"Articles","previous_headings":"","what":"6. Ewald et al. (2024) DGP","title":"Simulation Settings for Feature Importance Methods","text":"Reproduces data generating process Ewald et al. (2024) benchmarking feature importance methods. Includes correlated features interaction effects.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model-5","dir":"Articles","previous_headings":"6. Ewald et al. (2024) DGP","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[X_1, X_3, X_5 \\sim \\text{Uniform}(0,1)\\] \\[X_2 = X_1 + \\varepsilon_2, \\quad \\varepsilon_2 \\sim N(0, 0.001)\\] \\[X_4 = X_3 + \\varepsilon_4, \\quad \\varepsilon_4 \\sim N(0, 0.1)\\] \\[Y = X_4 + X_5 + X_4 \\cdot X_5 + \\varepsilon, \\quad \\varepsilon \\sim N(0, 0.1)\\]","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure-5","dir":"Articles","previous_headings":"6. Ewald et al. (2024) DGP","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG Ewald et al. (2024) DGP","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example-5","dir":"Articles","previous_headings":"6. Ewald et al. (2024) DGP","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"task <- sim_dgp_ewald(n = 500) task #>  #> ── <TaskRegr> (500x6) ────────────────────────────────────────────────────────── #> • Target: y #> • Properties: - #> • Features (5): #>   • dbl (5): x1, x2, x3, x4, x5"},{"path":"https://jemus42.github.io/xplainfi/articles/simulation-settings.html","id":"comparing-methods-on-different-dgps","dir":"Articles","previous_headings":"","what":"Comparing Methods on Different DGPs","title":"Simulation Settings for Feature Importance Methods","text":"’s practical example comparing PFI CFI correlated features DGP: Expected pattern: PFI falsely shows high importance spurious x2 CFI correctly shows near-zero importance x2","code":"task <- sim_dgp_correlated(n = 1000) learner <- lrn(\"regr.ranger\") measure <- msr(\"regr.mse\")  # Compute PFI pfi <- PFI$new(task, learner, measure) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. pfi$compute() #> Key: <feature> #>    feature  importance #>     <char>       <num> #> 1:      x1 2.121359858 #> 2:      x2 2.225174152 #> 3:      x3 1.482607702 #> 4:      x4 0.001266445  # Compute CFI cfi <- CFI$new(task, learner, measure) #> ℹ No <ConditionalSampler> provided, using <ARFSampler> with default settings. #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. cfi$compute() #> Key: <feature> #>    feature   importance #>     <char>        <num> #> 1:      x1 0.1640989568 #> 2:      x2 0.0801231825 #> 3:      x3 1.3948312601 #> 4:      x4 0.0001695743"},{"path":"https://jemus42.github.io/xplainfi/articles/xplainfi.html","id":"core-concepts","dir":"Articles","previous_headings":"","what":"Core Concepts","title":"Getting Started with xplainfi","text":"Feature importance methods xplainfi answer different related questions: much feature contribute model performance? (Permutation Feature Importance) happens remove features retrain? (Leave-One-Covariate-) much feature contribute individually? (Leave-One-Covariate-) features depend ? (Conditional Relative methods) methods share common interface built mlr3, making easy use task, learner, measure, resampling strategy.","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/xplainfi.html","id":"basic-example","dir":"Articles","previous_headings":"","what":"Basic Example","title":"Getting Started with xplainfi","text":"Let’s use Friedman1 task, provides ideal setup demonstrating feature importance methods known ground truth: task 300 observations 10 features. Features important1 important5 truly affect target, unimportant1 unimportant5 pure noise. ’ll use random forest learner cross-validation stable estimates. target function : \\(y = 10 * \\operatorname{sin}(\\pi * x_1 * x_2) + 20 * (x_3 - 0.5)^2 + 10 * x_4 + 5 * x_5 + \\epsilon\\)","code":"task <- tgen(\"friedman1\")$generate(n = 300) learner <- lrn(\"regr.ranger\", num.trees = 100) measure <- msr(\"regr.mse\") resampling <- rsmp(\"cv\", folds = 3)"},{"path":"https://jemus42.github.io/xplainfi/articles/xplainfi.html","id":"permutation-feature-importance-pfi","dir":"Articles","previous_headings":"","what":"Permutation Feature Importance (PFI)","title":"Getting Started with xplainfi","text":"PFI straightforward method: feature, permute (shuffle) values measure much model performance deteriorates. important features cause larger performance drops shuffled. importance column shows performance difference feature permuted. Higher values indicate important features. stable estimates, can use multiple permutation iterations per resampling fold: can also use ratio instead difference importance calculation:","code":"pfi <- PFI$new(   task = task,   learner = learner,   measure = measure,   resampling = resampling )  pfi_results <- pfi$compute() pfi_results #> Key: <feature> #>          feature   importance         sd #>           <char>        <num>      <num> #>  1:   important1  4.858724892 0.68442453 #>  2:   important2  8.155693005 2.26484810 #>  3:   important3  1.109254345 0.69151561 #>  4:   important4 10.784727349 1.29361802 #>  5:   important5  2.395793708 0.87273890 #>  6: unimportant1  0.009618005 0.11138825 #>  7: unimportant2  0.080903445 0.08050202 #>  8: unimportant3  0.044057887 0.04528352 #>  9: unimportant4 -0.082032243 0.10855146 #> 10: unimportant5 -0.137666350 0.08268950 pfi_stable <- PFI$new(   task = task,   learner = learner,   measure = measure,   resampling = resampling,   iters_perm = 5 )  pfi_stable$compute() #> Key: <feature> #>          feature   importance         sd #>           <char>        <num>      <num> #>  1:   important1  5.625322621 0.84130375 #>  2:   important2  9.609986341 1.77518863 #>  3:   important3  1.196388744 0.44992082 #>  4:   important4 12.648328883 2.92740759 #>  5:   important5  1.705056896 0.54745713 #>  6: unimportant1 -0.002597636 0.09029340 #>  7: unimportant2  0.108962283 0.17123736 #>  8: unimportant3  0.039131183 0.08291645 #>  9: unimportant4 -0.058408934 0.08647166 #> 10: unimportant5 -0.041202334 0.10787124 pfi_stable$compute(relation = \"ratio\") #> Key: <feature> #>          feature importance         sd #>           <char>      <num>      <num> #>  1:   important1  1.9484469 0.29293110 #>  2:   important2  2.4425122 0.27894990 #>  3:   important3  1.2212920 0.07502941 #>  4:   important4  2.9619275 0.45741386 #>  5:   important5  1.3790643 0.13580065 #>  6: unimportant1  0.9862282 0.01836363 #>  7: unimportant2  1.0098376 0.01946505 #>  8: unimportant3  1.0227458 0.01668362 #>  9: unimportant4  1.0090616 0.01819052 #> 10: unimportant5  0.9909342 0.01275179"},{"path":"https://jemus42.github.io/xplainfi/articles/xplainfi.html","id":"leave-one-covariate-out-loco","dir":"Articles","previous_headings":"","what":"Leave-One-Covariate-Out (LOCO)","title":"Getting Started with xplainfi","text":"LOCO measures importance retraining model without feature comparing performance full model. shows contribution feature features present. LOCO computationally expensive (requires retraining feature) provides clear interpretation: higher values mean larger performance drop feature removed. Important limitation: LOCO distinguish direct effects indirect effects correlated features.","code":"loco <- LOCO$new(   task = task,   learner = learner,   measure = measure,   resampling = resampling )  loco_results <- loco$compute() loco_results #> Key: <feature> #>          feature importance        sd #>           <char>      <num>     <num> #>  1:   important1  3.5341950 0.4799813 #>  2:   important2  5.5076635 0.8946863 #>  3:   important3  0.8231575 0.4514476 #>  4:   important4  7.5628028 1.7412825 #>  5:   important5  0.7647955 0.7375444 #>  6: unimportant1 -0.3884817 0.4774518 #>  7: unimportant2 -0.3159022 0.1183964 #>  8: unimportant3 -0.1991742 0.4288578 #>  9: unimportant4 -0.3039987 0.3220437 #> 10: unimportant5 -0.3435275 0.5206174"},{"path":"https://jemus42.github.io/xplainfi/articles/xplainfi.html","id":"feature-samplers","dir":"Articles","previous_headings":"","what":"Feature Samplers","title":"Getting Started with xplainfi","text":"advanced methods account feature dependencies, xplainfi provides different sampling strategies. PFI uses simple permutation (marginal sampling), conditional samplers can preserve feature relationships. Let’s demonstrate conditional sampling using Adversarial Random Forests, preserves relationships features sampling: Now ’ll conditionally sample important1 feature given values important2 important3: conditional sampling essential methods like CFI RFI need preserve feature dependencies. See vignette(\"perturbation-importance\") detailed comparisons.","code":"arf_sampler <- ARFSampler$new(task)  sample_data <- task$data(rows = 1:5) sample_data[, .(y, important1, important2)] #>           y important1  important2 #>       <num>      <num>       <num> #> 1: 20.59935  0.2875775 0.784575267 #> 2: 10.48474  0.7883051 0.009429905 #> 3: 19.99049  0.4089769 0.779065883 #> 4: 19.70521  0.8830174 0.729390652 #> 5: 21.94251  0.9404673 0.630131853 sampled_conditional <- arf_sampler$sample(   feature = \"important1\",    data = sample_data,   conditioning_set = c(\"important2\", \"important3\") )  sample_data[, .(y, important1, important2, important3)] #>           y important1  important2 important3 #>       <num>      <num>       <num>      <num> #> 1: 20.59935  0.2875775 0.784575267  0.2372297 #> 2: 10.48474  0.7883051 0.009429905  0.6864904 #> 3: 19.99049  0.4089769 0.779065883  0.2258184 #> 4: 19.70521  0.8830174 0.729390652  0.3184946 #> 5: 21.94251  0.9404673 0.630131853  0.1739838 sampled_conditional[, .(y, important1, important2, important3)] #>           y important1  important2 important3 #>       <num>      <num>       <num>      <num> #> 1: 20.59935  0.2666614 0.784575267  0.2372297 #> 2: 10.48474  0.2551529 0.009429905  0.6864904 #> 3: 19.99049  0.4023150 0.779065883  0.2258184 #> 4: 19.70521  1.0645308 0.729390652  0.3184946 #> 5: 21.94251  0.6319506 0.630131853  0.1739838"},{"path":"https://jemus42.github.io/xplainfi/articles/xplainfi.html","id":"advanced-features","dir":"Articles","previous_headings":"","what":"Advanced Features","title":"Getting Started with xplainfi","text":"xplainfi supports many advanced features robust importance estimation: Multiple resampling strategies: Cross-validation, bootstrap, custom splits Multiple permutation/refit iterations: stable estimates Feature grouping: Compute importance groups related features Different relation types: Difference vs. ratio scoring Conditional sampling: Account feature dependencies (see vignette(\"perturbation-importance\")) SAGE methods: Shapley-based approaches (see vignette(\"sage-methods\"))","code":""},{"path":"https://jemus42.github.io/xplainfi/articles/xplainfi.html","id":"detailed-scoring-information","dir":"Articles","previous_headings":"","what":"Detailed Scoring Information","title":"Getting Started with xplainfi","text":"methods store detailed scoring information analysis. Let’s examine structure PFI’s detailed scores: Detailed PFI scores (first 10 rows) can also summarize scoring structure:","code":"head(pfi$scores, 10) |>   knitr::kable(digits = 4, caption = \"Detailed PFI scores (first 10 rows)\") pfi$scores[, .(   features = uniqueN(feature),   resampling_folds = uniqueN(iter_rsmp),    permutation_iters = uniqueN(iter_perm),   total_scores = .N )] #>    features resampling_folds permutation_iters total_scores #>       <int>            <int>             <int>        <int> #> 1:       10                3                 1           30"},{"path":"https://jemus42.github.io/xplainfi/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lukas Burk. Author, maintainer.","code":""},{"path":"https://jemus42.github.io/xplainfi/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Burk L (2025). xplainfi: Feature Importance Methods Model Interpretability. R package version 0.1.0.9000, https://jemus42.github.io/xplainfi/.","code":"@Manual{,   title = {xplainfi: Feature Importance Methods for Model Interpretability},   author = {Lukas Burk},   year = {2025},   note = {R package version 0.1.0.9000},   url = {https://jemus42.github.io/xplainfi/}, }"},{"path":"https://jemus42.github.io/xplainfi/index.html","id":"xplainfi","dir":"","previous_headings":"","what":"xplainfi: Feature importance methods","title":"xplainfi: Feature importance methods","text":"goal xplainfi collect common feature importance methods unified extensible interface. built around mlr3 available abstractions learners, tasks, measures, etc. greatly simplify implementation importance measures.","code":""},{"path":"https://jemus42.github.io/xplainfi/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"xplainfi: Feature importance methods","text":"can install development version xplainfi like :","code":"# install.packages(pak) pak::pak(\"jemus42/xplainfi\")"},{"path":"https://jemus42.github.io/xplainfi/index.html","id":"example-pfi","dir":"","previous_headings":"","what":"Example: PFI","title":"xplainfi: Feature importance methods","text":"basic example calculate PFI given learner task, using repeated cross-validation resampling strategy computing PFI within resampling 5 times friedman1 task (see ?mlbench::mlbench.friedman1). friedman1 task following structure: \\[y = 10 \\sin(\\pi x_1 x_2) + 20(x_3 - 0.5)^2 + 10x_4 + 5x_5 + \\varepsilon\\] \\(x_{1,2,3,4,5}\\) named important1 important5 Task, additional numbered unimportant features without effect \\(y\\). Compute print PFI scores: Retrieve scores later pfi$importance. PFI computed based resampling multiple iterations, / multiple permutation iterations, individual scores can retrieved data.table: iter_rsmp corresponds resampling iteration, .e., 3 3-fold cross-validation, iter_perm corresponds permutation iteration within resampling iteration, 5 case. pfi$importance contains means standard deviations across iterations, pfi$scores allows manually aggregate way see fit. course also enables visualization across iterations:  measure question needs maximized ratehr minimized (like \\(R^2\\)), internal importance calculation takes account via $minimize property measure object calculates importances “performance improvement” -> higher importance score holds:","code":"library(xplainfi) library(mlr3learners) #> Loading required package: mlr3  task = tgen(\"friedman1\")$generate(1000) learner = lrn(\"regr.ranger\", num.trees = 100) measure = msr(\"regr.mse\")  pfi = PFI$new(   task = task,   learner = learner,   measure = measure,   resampling = rsmp(\"cv\", folds = 3),   iters_perm = 5 ) pfi$compute() #> Key: <feature> #>          feature    importance         sd #>           <char>         <num>      <num> #>  1:   important1  7.9320281224 0.77703482 #>  2:   important2  8.1368297554 0.65578878 #>  3:   important3  1.9003588453 0.35593120 #>  4:   important4 13.5083721272 0.73670025 #>  5:   important5  2.2748507393 0.24176933 #>  6: unimportant1 -0.0004293556 0.03863355 #>  7: unimportant2  0.0219242406 0.04763718 #>  8: unimportant3  0.0314298729 0.04041201 #>  9: unimportant4  0.0091510237 0.06797763 #> 10: unimportant5  0.0108304225 0.05918420 str(pfi$scores) #> Classes 'data.table' and 'data.frame':   150 obs. of  6 variables: #>  $ feature      : chr  \"important1\" \"important1\" \"important1\" \"important1\" ... #>  $ iter_rsmp    : int  1 1 1 1 1 2 2 2 2 2 ... #>  $ iter_perm    : int  1 2 3 4 5 1 2 3 4 5 ... #>  $ regr.mse_orig: num  5.3 5.3 5.3 5.3 5.3 ... #>  $ regr.mse_perm: num  13.9 14.1 12.9 12.5 12.8 ... #>  $ importance   : num  8.65 8.84 7.55 7.21 7.46 ... #>  - attr(*, \".internal.selfref\")=<externalptr>  #>  - attr(*, \"sorted\")= chr [1:2] \"feature\" \"iter_rsmp\" library(ggplot2)  ggplot(pfi$scores, aes(x = importance, y = reorder(feature, importance))) +   geom_boxplot() +   labs(     title = \"PFI Scores on Friedman1\",     subtitle = \"Aggregated over 3-fold CV with 5 permutations per iteration\",     x = \"Importance\",     y = \"Feature\"   ) +   theme_minimal(base_size = 16) +   theme(     plot.title.position = \"plot\",     panel.grid.major.y = element_blank()   ) pfi = PFI$new(   task = task,   learner = learner,   measure = msr(\"regr.rsq\"),   iters_perm = 1 ) #> ℹ No <Resampling> provided, using holdout resampling with default ratio.  pfi$compute() #> Key: <feature> #>          feature    importance #>           <char>         <num> #>  1:   important1  0.2461477003 #>  2:   important2  0.3354065147 #>  3:   important3  0.0534960678 #>  4:   important4  0.5744520955 #>  5:   important5  0.1116336289 #>  6: unimportant1 -0.0016660726 #>  7: unimportant2 -0.0031085541 #>  8: unimportant3 -0.0002086101 #>  9: unimportant4  0.0037567083 #> 10: unimportant5 -0.0022176601"},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"ARF-based Conditional Sampler — ARFSampler","title":"ARF-based Conditional Sampler — ARFSampler","text":"Implements conditional sampling using Adversarial Random Forests (ARF). ARF can handle mixed data types (continuous categorical) provides flexible conditional sampling modeling joint distribution.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ARF-based Conditional Sampler — ARFSampler","text":"ARFSampler fits Adversarial Random Forest model task data, uses generate samples \\(P(X_j | X_{-j})\\) \\(X_j\\) feature interest \\(X_{-j}\\) conditioning features.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"ARF-based Conditional Sampler — ARFSampler","text":"Watson, S. D, Blesch, Kristin, Kapar, Jan, Wright, N. M (2023). “Adversarial Random Forests Density Estimation Generative Modeling.” Proceedings 26th International Conference Artificial Intelligence Statistics, 5357–5375. https://proceedings.mlr.press/v206/watson23a.html. Blesch, Kristin, Koenen, Niklas, Kapar, Jan, Golchian, Pegah, Burk, Lukas, Loecher, Markus, Wright, N. M (2025). “Conditional Feature Importance Generative Modeling Using Adversarial Random Forests.” Proceedings AAAI Conference Artificial Intelligence, 39(15), 15596–15604. doi:10.1609/aaai.v39i15.33712 .","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"ARF-based Conditional Sampler — ARFSampler","text":"xplainfi::FeatureSampler -> xplainfi::ConditionalSampler -> ARFSampler","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"ARF-based Conditional Sampler — ARFSampler","text":"arf_model Adversarial Random Forest model psi Distribution parameters estimated ARF","code":""},{"path":[]},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"ARF-based Conditional Sampler — ARFSampler","text":"ARFSampler$new() ARFSampler$sample() ARFSampler$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"ARF-based Conditional Sampler — ARFSampler","text":"Creates new instance ARFSampler class. fit ARF parallel, set arf_args = list(parallel = TRUE) register parallel backend (see arf::arf).","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ARF-based Conditional Sampler — ARFSampler","text":"","code":"ARFSampler$new(   task,   conditioning_set = NULL,   finite_bounds = \"no\",   round = TRUE,   stepsize = 0,   verbose = FALSE,   parallel = FALSE,   arf_args = NULL )"},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ARF-based Conditional Sampler — ARFSampler","text":"task (mlr3::Task) Task sample conditioning_set (character | NULL) Default conditioning set use $sample(). parameter affects sampling behavior, ARF model fitting. finite_bounds (character(1): \"\") Passed arf::forde(). Default \"\" compatibility. \"local\" may improve extrapolation can cause issues data. round (logical(1): TRUE) Whether round continuous variables back original precision. stepsize (numeric(1): 0) Number rows evidence process time wehn parallel TRUE. Default (0) spreads evidence evenly registered workers. verbose (logical(1): FALSE) Whether print progress messages. Default FALSE default arf TRUE. parallel (logical(1): FALSE) Whether use parallel processing via foreach. See examples arf::forge(). arf_args (list) Additional passed arf::adversarial_rf.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"method-sample-","dir":"Reference","previous_headings":"","what":"Method sample()","title":"ARF-based Conditional Sampler — ARFSampler","text":"Sample values feature(s) conditionally features using ARF","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"ARF-based Conditional Sampler — ARFSampler","text":"","code":"ARFSampler$sample(   feature,   data = self$task$data(),   conditioning_set = NULL,   round = NULL,   stepsize = NULL,   verbose = NULL,   parallel = NULL,   ... )"},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"ARF-based Conditional Sampler — ARFSampler","text":"feature (character) Feature(s) interest sample (can single multiple) data (data.table) Data containing conditioning features. Defaults $task$data(), typically dedicated test set provided. conditioning_set (character(n) | NULL) Features condition . NULL, uses stored parameter available, otherwise defaults features. round (logical(1) | NULL) Whether round continuous variables. NULL, uses stored parameter value. stepsize (numeric(1) | NULL) Step size variance adjustment. NULL, uses stored parameter value. verbose (logical(1) | NULL) Whether print progress messages. NULL, uses stored parameter value. parallel (logical(1) | NULL) Whether use parallel processing. NULL, uses stored parameter value. ... arguments passed arf::forge().","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"ARF-based Conditional Sampler — ARFSampler","text":"Modified copy input data feature(s) sampled conditionally","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"ARF-based Conditional Sampler — ARFSampler","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"ARF-based Conditional Sampler — ARFSampler","text":"","code":"ARFSampler$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"ARF-based Conditional Sampler — ARFSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ARFSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ARF-based Conditional Sampler — ARFSampler","text":"","code":"library(mlr3) task = tgen(\"2dnormals\")$generate(n = 100) # Create sampler with default parameters sampler = ARFSampler$new(task, conditioning_set = \"x2\", verbose = FALSE) data = task$data() # Will use the stored parameters sampled_data = sampler$sample(\"x1\", data)  # Example with custom parameters sampler_custom = ARFSampler$new(task, round = FALSE) sampled_custom = sampler_custom$sample(\"x1\", data)"},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Feature Importance — CFI","title":"Conditional Feature Importance — CFI","text":"Implementation CFI using modular sampling approach","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Conditional Feature Importance — CFI","text":"Blesch, Kristin, Koenen, Niklas, Kapar, Jan, Golchian, Pegah, Burk, Lukas, Loecher, Markus, Wright, N. M (2025). “Conditional Feature Importance Generative Modeling Using Adversarial Random Forests.” Proceedings AAAI Conference Artificial Intelligence, 39(15), 15596–15604. doi:10.1609/aaai.v39i15.33712 .","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Conditional Feature Importance — CFI","text":"xplainfi::FeatureImportanceMethod -> xplainfi::PerturbationImportance -> CFI","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Conditional Feature Importance — CFI","text":"xplainfi::FeatureImportanceMethod$combine() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Conditional Feature Importance — CFI","text":"CFI$new() CFI$compute() CFI$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Conditional Feature Importance — CFI","text":"Creates new instance CFI class","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Importance — CFI","text":"","code":"CFI$new(   task,   learner,   measure,   resampling = NULL,   features = NULL,   relation = \"difference\",   iters_perm = 1L,   sampler = NULL )"},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Importance — CFI","text":"task, learner, measure, resampling, features Passed PerturbationImportance. relation (character(1)) relate perturbed scores originals. Can overridden $compute(). iters_perm (integer(1)) Number sampling iterations. Can overridden $compute(). sampler (ConditionalSampler) Optional custom sampler. Defaults instantiationg ARFSampler internally default parameters.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Conditional Feature Importance — CFI","text":"Compute CFI scores","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Importance — CFI","text":"","code":"CFI$compute(relation = NULL, iters_perm = NULL, store_backends = TRUE)"},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Importance — CFI","text":"relation (character(1)) relate perturbed scores originals. NULL, uses stored value. iters_perm (integer(1)) Number permutation iterations. NULL, uses stored value. store_backends (logical(1)) Whether store backends","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Conditional Feature Importance — CFI","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Importance — CFI","text":"","code":"CFI$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Importance — CFI","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/CFI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional Feature Importance — CFI","text":"","code":"library(mlr3) task = tgen(\"2dnormals\")$generate(n = 100) cfi = CFI$new(   task = task,   learner = lrn(\"classif.ranger\", num.trees = 50, predict_type = \"prob\"),   measure = msr(\"classif.ce\") ) #> ℹ No <ConditionalSampler> provided, using <ARFSampler> with default settings. #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. cfi$compute() #> Key: <feature> #>    feature importance #>     <char>      <num> #> 1:      x1 0.12121212 #> 2:      x2 0.09090909"},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional SAGE — ConditionalSAGE","title":"Conditional SAGE — ConditionalSAGE","text":"SAGE conditional sampling (features marginalized conditionally). Uses ARF default conditional marginalization.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Conditional SAGE — ConditionalSAGE","text":"xplainfi::FeatureImportanceMethod -> xplainfi::SAGE -> ConditionalSAGE","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Conditional SAGE — ConditionalSAGE","text":"xplainfi::FeatureImportanceMethod$combine() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::SAGE$compute() xplainfi::SAGE$plot_convergence()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Conditional SAGE — ConditionalSAGE","text":"ConditionalSAGE$new() ConditionalSAGE$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Conditional SAGE — ConditionalSAGE","text":"Creates new instance ConditionalSAGE class.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional SAGE — ConditionalSAGE","text":"","code":"ConditionalSAGE$new(   task,   learner,   measure,   resampling = NULL,   features = NULL,   n_permutations = 10L,   reference_data = NULL,   sampler = NULL,   batch_size = 5000L,   max_reference_size = 100L )"},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional SAGE — ConditionalSAGE","text":"task, learner, measure, resampling, features Passed SAGE. n_permutations (integer(1)) Number permutations sample. reference_data (data.table) Optional reference dataset. sampler (ConditionalSampler) Optional custom sampler. Defaults ARFSampler. batch_size (integer(1): 5000L) Maximum number observations process single prediction call. max_reference_size (integer(1)) Maximum size reference dataset.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Conditional SAGE — ConditionalSAGE","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional SAGE — ConditionalSAGE","text":"","code":"ConditionalSAGE$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional SAGE — ConditionalSAGE","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional SAGE — ConditionalSAGE","text":"","code":"library(mlr3) task = tgen(\"friedman1\")$generate(n = 100) sage = ConditionalSAGE$new(   task = task,   learner = lrn(\"regr.ranger\", num.trees = 50),   measure = msr(\"regr.mse\"),   n_permutations = 3L ) #> ℹ No <ConditionalSampler> provided, using <ARFSampler> with default settings. #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. sage$compute() #> Key: <feature> #>          feature  importance #>           <char>       <num> #>  1:   important1  4.58861248 #>  2:   important2 -0.10848016 #>  3:   important3  0.08622897 #>  4:   important4  4.78590789 #>  5:   important5  2.26613012 #>  6: unimportant1 -0.51814805 #>  7: unimportant2  0.28900725 #>  8: unimportant3 -0.69336275 #>  9: unimportant4 -0.05010833 #> 10: unimportant5 -0.14724626  # Use batching for memory efficiency with large datasets sage$compute(batch_size = 1000) #> Key: <feature> #>          feature  importance #>           <char>       <num> #>  1:   important1  4.58861248 #>  2:   important2 -0.10848016 #>  3:   important3  0.08622897 #>  4:   important4  4.78590789 #>  5:   important5  2.26613012 #>  6: unimportant1 -0.51814805 #>  7: unimportant2  0.28900725 #>  8: unimportant3 -0.69336275 #>  9: unimportant4 -0.05010833 #> 10: unimportant5 -0.14724626"},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Feature Sampler — ConditionalSampler","title":"Conditional Feature Sampler — ConditionalSampler","text":"Base class conditional sampling methods features sampled conditionally features. abstract class extended concrete implementations.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Conditional Feature Sampler — ConditionalSampler","text":"xplainfi::FeatureSampler -> ConditionalSampler","code":""},{"path":[]},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Conditional Feature Sampler — ConditionalSampler","text":"ConditionalSampler$new() ConditionalSampler$sample() ConditionalSampler$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Conditional Feature Sampler — ConditionalSampler","text":"Creates new instance ConditionalSampler class","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Sampler — ConditionalSampler","text":"","code":"ConditionalSampler$new(task)"},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Sampler — ConditionalSampler","text":"task (mlr3::Task) Task sample ","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"method-sample-","dir":"Reference","previous_headings":"","what":"Method sample()","title":"Conditional Feature Sampler — ConditionalSampler","text":"Sample values feature(s) conditionally features","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Sampler — ConditionalSampler","text":"","code":"ConditionalSampler$sample(feature, data, conditioning_set = NULL)"},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Sampler — ConditionalSampler","text":"feature (character) Feature name(s) sample (can single multiple) data (data.table ) Data containing conditioning features conditioning_set (character) Features condition (default: features)","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Conditional Feature Sampler — ConditionalSampler","text":"Modified copy input data feature(s) sampled conditionally","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Conditional Feature Sampler — ConditionalSampler","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Sampler — ConditionalSampler","text":"","code":"ConditionalSampler$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/ConditionalSampler.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Sampler — ConditionalSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":null,"dir":"Reference","previous_headings":"","what":"Feature Importance Method Class — FeatureImportanceMethod","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Feature Importance Method Class Feature Importance Method Class","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"label (character(1)) Method label. task (mlr3::Task) learner (mlr3::Learner) measure (mlr3::Measure) resampling (mlr3::Resampling) resample_result (mlr3::ResampleResult) features (character) param_set (paradox::ps()) importance (data.table) Aggregated importance scores scores (data.table) Individual performance scores used compute $importance per resampling iteration permutation iteration. obs_losses (data.table) Observation-wise losses available (e.g., using obs_loss = TRUE). Contains columns row_ids, feature, iteration indices, individual loss values, reference feature-specific predictions. predictions (data.table) Feature-specific prediction objects using obs_loss = TRUE. Contains columns feature, iteration, iter_refit, prediction objects. Similar ResampleResult$predictions() extended feature-specific models.","code":""},{"path":[]},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"FeatureImportanceMethod$new() FeatureImportanceMethod$compute() FeatureImportanceMethod$combine() FeatureImportanceMethod$reset() FeatureImportanceMethod$print() FeatureImportanceMethod$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Creates new instance R6 class. typically intended use derived classes.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$new(   task,   learner,   measure,   resampling = NULL,   features = NULL,   param_set = paradox::ps(),   label )"},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"task, learner, measure, resampling, features, param_set, label Used set fields","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Compute feature importance scores","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$compute(   relation = c(\"difference\", \"ratio\"),   store_backends = TRUE )"},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"relation (character(1): \"difference\") relate perturbed scores originals (\"difference\" \"ratio\") store_backends (logical(1): TRUE) Whether store backends.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-combine-","dir":"Reference","previous_headings":"","what":"Method combine()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Combine two FeatureImportanceMethod objects computed scores.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$combine(y, ...)"},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"y (FeatureImportanceMethod) Object combine. Must computed scores. ... () Unused.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"new FeatureImportanceMethod subclass x y. Currently method merges following: $scores combined, iter_rsmp increased y. $importance re-computed combined $scores. $resample_result combined mlr3::BenchmarkResult $resampling combined mlr3::ResamplingCustom, continuing te iteration count x y.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-reset-","dir":"Reference","previous_headings":"","what":"Method reset()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Resets stored fields populated $compute: $resample_result, $importance, $scores, $obs_losses, $predictions.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$reset()"},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Print importance scores","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$print(...)"},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"... Passed print()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Feature Sampler Class — FeatureSampler","title":"Feature Sampler Class — FeatureSampler","text":"Base class implementing different sampling strategies feature importance methods like PFI CFI","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Feature Sampler Class — FeatureSampler","text":"task (mlr3::Task) Original task. label (character(1)) Name sampler. param_set (paradox::ParamSet) Parameter set sampler.","code":""},{"path":[]},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Feature Sampler Class — FeatureSampler","text":"FeatureSampler$new() FeatureSampler$sample() FeatureSampler$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Feature Sampler Class — FeatureSampler","text":"Creates new instance FeatureSampler class","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Sampler Class — FeatureSampler","text":"","code":"FeatureSampler$new(task)"},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Sampler Class — FeatureSampler","text":"task (mlr3::Task) Task sample ","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"method-sample-","dir":"Reference","previous_headings":"","what":"Method sample()","title":"Feature Sampler Class — FeatureSampler","text":"Sample values feature(s)","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Sampler Class — FeatureSampler","text":"","code":"FeatureSampler$sample(feature, data)"},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Sampler Class — FeatureSampler","text":"feature (character) Feature name(s) sample (can single multiple) data (data.table ) Data use sampling context","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature Sampler Class — FeatureSampler","text":"Modified copy input data feature(s) sampled","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Feature Sampler Class — FeatureSampler","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Sampler Class — FeatureSampler","text":"","code":"FeatureSampler$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/FeatureSampler.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Sampler Class — FeatureSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Knockoff-based Conditional Sampler — KnockoffSampler","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"Implements conditional sampling using Knockoffs.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"KnockoffSampler samples Knockoffs based task data.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"Watson D, Wright M (2021). “Testing conditional independence supervised learning algorithms.” Machine Learning, 110(8), 2107-2129. doi:10.1007/s10994-021-06030-6 . Blesch K, Watson D, Wright M (2023). “Conditional feature importance mixed data.” AStA Advances Statistical Analysis, 108(2), 259-278. doi:10.1007/s10182-023-00477-9 .","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"xplainfi::FeatureSampler -> xplainfi::ConditionalSampler -> KnockoffSampler","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"x_tilde Knockoff matrix","code":""},{"path":[]},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"KnockoffSampler$new() KnockoffSampler$sample() KnockoffSampler$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"Creates new instance KnockoffSampler class.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"","code":"KnockoffSampler$new(   task,   knockoff_fun = function(x) knockoff::create.second_order(as.matrix(x)) )"},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"task (mlr3::Task) Task sample knockoff_fun (function) Step size variance adjustment. Default second-order Gaussian knockoffs.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"method-sample-","dir":"Reference","previous_headings":"","what":"Method sample()","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"Sample values feature(s) conditionally features using Knockoffs","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"","code":"KnockoffSampler$sample(feature, data = self$task$data())"},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"feature (character) Feature(s) interest sample (can single multiple) data (data.table) Data containing conditioning features. Defaults $task$data(), typically dedicated test set provided.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"Modified copy input data feature(s) sampled conditionally","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"","code":"KnockoffSampler$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/KnockoffSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Knockoff-based Conditional Sampler — KnockoffSampler","text":"","code":"library(mlr3) task = tgen(\"2dnormals\")$generate(n = 100) # Create sampler with default parameters sampler = KnockoffSampler$new(task) # Will use the stored parameters sampled_data = sampler$sample(\"x1\") if (FALSE) { # \\dontrun{ # Example with sequential knockoffs (https://github.com/kormama1/seqknockoff) task = tgen(\"simplex\")$generate(n = 100) sampler_seq = KnockoffSampler$new(task, knockoff_fun = seqknockoff::knockoffs_seq) sampled_seq = sampler_seq$sample(\"x1\") } # }"},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":null,"dir":"Reference","previous_headings":"","what":"Leave-One-Covariate-In (LOCI) — LOCI","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"Calculates Leave-One-Covariate-(LOCI) scores. Despite name, implementation can leave one features time.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"LOCI measures feature importance training models individual feature (feature subset) comparing performance featureless baseline model (optimal constant prediction). importance calculated (featureless_model_loss - single_feature_loss). Positive values indicate feature performs better baseline, negative values indicate worse performance.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"xplainfi::FeatureImportanceMethod -> xplainfi::LeaveOutIn -> LOCI","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"xplainfi::FeatureImportanceMethod$combine() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::LeaveOutIn$compute()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"LOCI$new() LOCI$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"Creates new instance R6 class.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"","code":"LOCI$new(   task,   learner,   measure,   resampling = NULL,   features = NULL,   iters_refit = 1L,   obs_loss = FALSE,   aggregation_fun = median )"},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"task (mlr3::Task) Task compute importance . learner (mlr3::Learner) Learner use prediction. measure (mlr3::Measure) Measure use scoring. resampling (mlr3::Resampling) Resampling strategy. Defaults holdout. features (character()) Features compute importance . Defaults features. iters_refit (integer(1)) Number refit iterations per resampling iteration. obs_loss (logical(1)) Whether use observation-wise loss calculation (analogous LOCO) supported measure. FALSE (default), uses aggregated scores. aggregation_fun (function) Function aggregate observation-wise losses obs_loss = TRUE. Defaults median, analogous LOCO.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"","code":"LOCI$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Leave-One-Covariate-In (LOCI) — LOCI","text":"","code":"library(mlr3) task = tgen(\"friedman1\")$generate(n = 200) loci = LOCI$new(   task = task,   learner = lrn(\"regr.ranger\", num.trees = 50),   measure = msr(\"regr.mse\") ) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. loci$compute() #> Key: <feature> #>          feature importance #>           <char>      <num> #>  1:   important1  -1.615471 #>  2:   important2   1.159116 #>  3:   important3  -7.230327 #>  4:   important4   5.860099 #>  5:   important5  -3.065838 #>  6: unimportant1  -4.728536 #>  7: unimportant2  -5.626825 #>  8: unimportant3  -6.667612 #>  9: unimportant4  -5.255167 #> 10: unimportant5  -3.365825"},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":null,"dir":"Reference","previous_headings":"","what":"Leave-One-Covariate-Out (LOCO) — LOCO","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"Calculates Leave-One-Covariate-(LOCO) scores. Despite name, implementation can leave one features time.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"LOCO measures feature importance comparing model performance without feature. feature, model retrained without feature performance difference (reduced_model_loss - full_model_loss) indicates feature's importance. Higher values indicate important features.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"Lei, Jing, Max, G'Sell, Alessandro, Rinaldo, J. R, Tibshirani, Wasserman, Larry (2018). “Distribution-Free Predictive Inference Regression.” Journal American Statistical Association, 113(523), 1094–1111. ISSN 0162-1459, doi:10.1080/01621459.2017.1307116 .","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"xplainfi::FeatureImportanceMethod -> xplainfi::LeaveOutIn -> LOCO","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"xplainfi::FeatureImportanceMethod$combine() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::LeaveOutIn$compute()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"LOCO$new() LOCO$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"Creates new instance R6 class.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"","code":"LOCO$new(   task,   learner,   measure,   resampling = NULL,   features = NULL,   iters_refit = 1L,   obs_loss = FALSE,   aggregation_fun = median )"},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"task (mlr3::Task) Task compute importance . learner (mlr3::Learner) Learner use prediction. measure (mlr3::Measure) Measure use scoring. resampling (mlr3::Resampling) Resampling strategy. Defaults holdout. features (character()) Features compute importance . Defaults features. iters_refit (integer(1): 1L) Number refit iterations per resampling iteration. obs_loss (logical(1): FALSE) Whether use observation-wise loss calculation (original LOCO formulation). FALSE, uses aggregated scores. aggregation_fun (function) Function aggregate observation-wise losses obs_loss = TRUE. Defaults median original LOCO formulation.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"","code":"LOCO$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LOCO.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"","code":"library(mlr3learners) task = tgen(\"friedman1\")$generate(n = 200) loco = LOCO$new(   task = task,   learner = lrn(\"regr.ranger\", num.trees = 50),   measure = msr(\"regr.mse\"), obs_loss = TRUE ) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. loco$compute() #> Key: <feature> #>          feature  importance #>           <char>       <num> #>  1:   important1  1.77642119 #>  2:   important2  0.78483613 #>  3:   important3  0.07920417 #>  4:   important4  3.62372184 #>  5:   important5  0.61506361 #>  6: unimportant1  0.12979459 #>  7: unimportant2 -0.01680615 #>  8: unimportant3  0.15576714 #>  9: unimportant4 -0.12916100 #> 10: unimportant5  0.36595416  # Using observation-wise losses to compute the median instead loco_obsloss = LOCO$new(   task = task,   learner = lrn(\"regr.ranger\", num.trees = 50),   measure = msr(\"regr.mae\"), # to use absolute differences observation-wise   obs_loss = TRUE,   aggregation_fun = median ) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. loco_obsloss$compute() #> Key: <feature> #>          feature   importance #>           <char>        <num> #>  1:   important1  0.443246431 #>  2:   important2  0.304168859 #>  3:   important3  0.111510773 #>  4:   important4  0.805161323 #>  5:   important5  0.097861187 #>  6: unimportant1 -0.020882300 #>  7: unimportant2  0.145543352 #>  8: unimportant3  0.003488681 #>  9: unimportant4  0.031361178 #> 10: unimportant5  0.045435103 loco_obsloss$obs_losses #>      row_ids      feature iteration iter_refit     truth response_ref #>        <int>       <char>     <int>      <int>     <num>        <num> #>   1:       1   important1         1          1 13.690317    13.820347 #>   2:       1   important2         1          1 13.690317    13.820347 #>   3:       1   important3         1          1 13.690317    13.820347 #>   4:       1   important4         1          1 13.690317    13.820347 #>   5:       1   important5         1          1 13.690317    13.820347 #>  ---                                                                  #> 666:     195 unimportant1         1          1  6.371902     9.950791 #> 667:     195 unimportant2         1          1  6.371902     9.950791 #> 668:     195 unimportant3         1          1  6.371902     9.950791 #> 669:     195 unimportant4         1          1  6.371902     9.950791 #> 670:     195 unimportant5         1          1  6.371902     9.950791 #>      response_feature  loss_ref loss_feature    obs_diff #>                 <num>     <num>        <num>       <num> #>   1:        13.306309 0.1300299    0.3840078  0.25397785 #>   2:        13.529533 0.1300299    0.1607837  0.03075379 #>   3:        14.141133 0.1300299    0.4508165  0.32078658 #>   4:        15.158180 0.1300299    1.4678634  1.33783347 #>   5:        12.552168 0.1300299    1.1381483  1.00811837 #>  ---                                                     #> 666:         9.193108 3.5788897    2.8212058 -0.75768388 #> 667:         9.066730 3.5788897    2.6948286 -0.88406104 #> 668:         9.204020 3.5788897    2.8321183 -0.74677132 #> 669:         9.335935 3.5788897    2.9640331 -0.61485659 #> 670:         8.895666 3.5788897    2.5237638 -1.05512585"},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":null,"dir":"Reference","previous_headings":"","what":"Leave-Out/In Base Class — LeaveOutIn","title":"Leave-Out/In Base Class — LeaveOutIn","text":"Base class Leave-Leave-feature importance methods. abstract class - use LOCO LOCI.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Leave-Out/In Base Class — LeaveOutIn","text":"xplainfi::FeatureImportanceMethod -> LeaveOutIn","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Leave-Out/In Base Class — LeaveOutIn","text":"direction (character(1)) Either \"leave-\" \"leave-\".","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Leave-Out/In Base Class — LeaveOutIn","text":"xplainfi::FeatureImportanceMethod$combine() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Leave-Out/In Base Class — LeaveOutIn","text":"LeaveOutIn$new() LeaveOutIn$compute() LeaveOutIn$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Leave-Out/In Base Class — LeaveOutIn","text":"Creates new instance R6 class.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Leave-Out/In Base Class — LeaveOutIn","text":"","code":"LeaveOutIn$new(   task,   learner,   measure,   resampling = NULL,   features = NULL,   direction,   label,   iters_refit = 1L,   obs_loss = FALSE,   aggregation_fun = median )"},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leave-Out/In Base Class — LeaveOutIn","text":"task, learner, measure, resampling, features Passed FeatureImportanceMethod construction. direction (character(1)) Either \"leave-\" \"leave-\". label (character(1)) Method label. iters_refit (integer(1)) Number refit iterations per resampling iteration. obs_loss (logical(1)) Whether use observation-wise loss calculation (original LOCO formulation) supported measure. FALSE (default), uses aggregated scores. aggregation_fun (function) Function aggregate observation-wise losses obs_loss = TRUE. Defaults median original LOCO formulation.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Leave-Out/In Base Class — LeaveOutIn","text":"Computes leave-leave-feature importance.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Leave-Out/In Base Class — LeaveOutIn","text":"","code":"LeaveOutIn$compute(relation = c(\"difference\", \"ratio\"), store_backends = TRUE)"},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leave-Out/In Base Class — LeaveOutIn","text":"relation (character(1)) Calculate \"difference\" (default) \"ratio\" original scores scores leaving /features. store_backends (logical(1)) Passed mlr3::resample store backends resample result. Required measures, may increase memory footprint.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Leave-Out/In Base Class — LeaveOutIn","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Leave-Out/In Base Class — LeaveOutIn","text":"","code":"LeaveOutIn$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/LeaveOutIn.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leave-Out/In Base Class — LeaveOutIn","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSAGE.html","id":null,"dir":"Reference","previous_headings":"","what":"Marginal SAGE — MarginalSAGE","title":"Marginal SAGE — MarginalSAGE","text":"SAGE marginal sampling (features marginalized independently). standard SAGE implementation.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSAGE.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Marginal SAGE — MarginalSAGE","text":"xplainfi::FeatureImportanceMethod -> xplainfi::SAGE -> MarginalSAGE","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSAGE.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Marginal SAGE — MarginalSAGE","text":"xplainfi::FeatureImportanceMethod$combine() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::SAGE$compute() xplainfi::SAGE$plot_convergence()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSAGE.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Marginal SAGE — MarginalSAGE","text":"MarginalSAGE$new() MarginalSAGE$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSAGE.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Marginal SAGE — MarginalSAGE","text":"Creates new instance MarginalSAGE class.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSAGE.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal SAGE — MarginalSAGE","text":"","code":"MarginalSAGE$new(   task,   learner,   measure,   resampling = NULL,   features = NULL,   n_permutations = 10L,   reference_data = NULL,   batch_size = 5000L,   max_reference_size = 100L )"},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSAGE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal SAGE — MarginalSAGE","text":"task, learner, measure, resampling, features Passed SAGE. n_permutations (integer(1)) Number permutations sample. reference_data (data.table) Optional reference dataset. batch_size (integer(1): 5000L) Maximum number observations process single prediction call. max_reference_size (integer(1)) Maximum size reference dataset.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSAGE.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Marginal SAGE — MarginalSAGE","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSAGE.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal SAGE — MarginalSAGE","text":"","code":"MarginalSAGE$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSAGE.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal SAGE — MarginalSAGE","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSAGE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Marginal SAGE — MarginalSAGE","text":"","code":"library(mlr3) task = tgen(\"friedman1\")$generate(n = 100) sage = MarginalSAGE$new(   task = task,   learner = lrn(\"regr.ranger\", num.trees = 50),   measure = msr(\"regr.mse\"),   n_permutations = 3L ) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. sage$compute() #> Key: <feature> #>          feature  importance #>           <char>       <num> #>  1:   important1  3.07176661 #>  2:   important2  3.20725455 #>  3:   important3  0.36612343 #>  4:   important4  6.47751789 #>  5:   important5 -0.70632859 #>  6: unimportant1 -0.05034109 #>  7: unimportant2  0.02116871 #>  8: unimportant3 -0.04220989 #>  9: unimportant4  0.17476385 #> 10: unimportant5 -0.13025925  # Use batching for memory efficiency with large datasets sage$compute(batch_size = 1000) #> Key: <feature> #>          feature  importance #>           <char>       <num> #>  1:   important1  3.07176661 #>  2:   important2  3.20725455 #>  3:   important3  0.36612343 #>  4:   important4  6.47751789 #>  5:   important5 -0.70632859 #>  6: unimportant1 -0.05034109 #>  7: unimportant2  0.02116871 #>  8: unimportant3 -0.04220989 #>  9: unimportant4  0.17476385 #> 10: unimportant5 -0.13025925"},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Marginal Feature Sampler — MarginalSampler","title":"Marginal Feature Sampler — MarginalSampler","text":"Implements marginal sampling PFI, feature interest sampled independently features","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Marginal Feature Sampler — MarginalSampler","text":"xplainfi::FeatureSampler -> MarginalSampler","code":""},{"path":[]},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Marginal Feature Sampler — MarginalSampler","text":"MarginalSampler$new() MarginalSampler$sample() MarginalSampler$print() MarginalSampler$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Marginal Feature Sampler — MarginalSampler","text":"Creates new instance MarginalSampler class","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal Feature Sampler — MarginalSampler","text":"","code":"MarginalSampler$new(task)"},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal Feature Sampler — MarginalSampler","text":"task (mlr3::Task) Task sample ","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"method-sample-","dir":"Reference","previous_headings":"","what":"Method sample()","title":"Marginal Feature Sampler — MarginalSampler","text":"Sample values feature(s) permutation (marginal distribution)","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal Feature Sampler — MarginalSampler","text":"","code":"MarginalSampler$sample(feature, data)"},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal Feature Sampler — MarginalSampler","text":"feature (character) Feature name(s) sample (can single multiple) data (data.table ) Data permute feature(s) ","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Marginal Feature Sampler — MarginalSampler","text":"Modified copy input data feature(s) permuted","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Marginal Feature Sampler — MarginalSampler","text":"Print sampler","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal Feature Sampler — MarginalSampler","text":"","code":"MarginalSampler$print(...)"},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal Feature Sampler — MarginalSampler","text":"... Passed print()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Marginal Feature Sampler — MarginalSampler","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal Feature Sampler — MarginalSampler","text":"","code":"MarginalSampler$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal Feature Sampler — MarginalSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/MarginalSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Marginal Feature Sampler — MarginalSampler","text":"","code":"library(mlr3) task = tgen(\"2dnormals\")$generate(n = 100) sampler = MarginalSampler$new(task) data = task$data() sampled_data = sampler$sample(\"x1\", data)"},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":null,"dir":"Reference","previous_headings":"","what":"Permutation Feature Importance — PFI","title":"Permutation Feature Importance — PFI","text":"Implementation Permutation Feature Importance (PFI) using modular sampling approach. PFI measures importance feature calculating increase model error feature's values randomly permuted, breaking relationship feature target variable.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Permutation Feature Importance — PFI","text":"Permutation Feature Importance originally introduced Breiman (2001) part Random Forest algorithm. method works : Computing baseline model performance original dataset feature, randomly permuting values keeping features unchanged Computing model performance permuted dataset Calculating importance difference (ratio) permuted original performance","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Permutation Feature Importance — PFI","text":"Breiman, Leo (2001). “Random Forests.” Machine Learning, 45(1), 5–32. doi:10.1023/:1010933404324 . Fisher, Aaron, Rudin, Cynthia, Dominici, Francesca (2019). “Models Wrong, Many Useful: Learning Variable's Importance Studying Entire Class Prediction Models Simultaneously.” Journal Machine Learning Research, 20, 177. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323609/. Strobl, Carolin, Boulesteix, Anne-Laure, Kneib, Thomas, Augustin, Thomas, Zeileis, Achim (2008). “Conditional Variable Importance Random Forests.” BMC Bioinformatics, 9(1), 307. ISSN 1471-2105, doi:10.1186/1471-2105-9-307 .","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Permutation Feature Importance — PFI","text":"xplainfi::FeatureImportanceMethod -> xplainfi::PerturbationImportance -> PFI","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Permutation Feature Importance — PFI","text":"xplainfi::FeatureImportanceMethod$combine() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Permutation Feature Importance — PFI","text":"PFI$new() PFI$compute() PFI$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Permutation Feature Importance — PFI","text":"Creates new instance PFI class","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutation Feature Importance — PFI","text":"","code":"PFI$new(   task,   learner,   measure,   resampling = NULL,   features = NULL,   relation = \"difference\",   iters_perm = 1L )"},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutation Feature Importance — PFI","text":"task, learner, measure, resampling, features Passed PerturbationImportance relation (character(1)) relate perturbed scores originals. Can overridden $compute(). iters_perm (integer(1)) Number permutation iterations. Can overridden $compute().","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Permutation Feature Importance — PFI","text":"Compute PFI scores","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutation Feature Importance — PFI","text":"","code":"PFI$compute(relation = NULL, iters_perm = NULL, store_backends = TRUE)"},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutation Feature Importance — PFI","text":"relation (character(1)) relate perturbed scores originals. NULL, uses stored value. iters_perm (integer(1)) Number permutation iterations. NULL, uses stored value. store_backends (logical(1)) Whether store backends","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Permutation Feature Importance — PFI","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutation Feature Importance — PFI","text":"","code":"PFI$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutation Feature Importance — PFI","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PFI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Permutation Feature Importance — PFI","text":"","code":"library(mlr3learners) task = tgen(\"xor\", d = 5)$generate(n = 100) pfi = PFI$new(   task = task,   learner = lrn(\"classif.ranger\", num.trees = 50, predict_type = \"prob\"),   measure = msr(\"classif.ce\"),   resampling = rsmp(\"cv\", folds = 3),   iters_perm = 3 ) pfi$compute() #> Key: <feature> #>    feature importance         sd #>     <char>      <num>      <num> #> 1:      x1 0.07367796 0.04072362 #> 2:      x2 0.09011685 0.03054924 #> 3:      x3 0.07377699 0.04076694 #> 4:      x4 0.11031888 0.03761779 #> 5:      x5 0.06001188 0.03369467"},{"path":"https://jemus42.github.io/xplainfi/reference/PerturbationImportance.html","id":null,"dir":"Reference","previous_headings":"","what":"Perturbation Feature Importance Base Class — PerturbationImportance","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"Abstract base class perturbation-based importance methods PFI, CFI, RFI","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PerturbationImportance.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"xplainfi::FeatureImportanceMethod -> PerturbationImportance","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PerturbationImportance.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"sampler (FeatureSampler) Sampler object feature perturbation","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PerturbationImportance.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"xplainfi::FeatureImportanceMethod$combine() xplainfi::FeatureImportanceMethod$compute() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PerturbationImportance.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"PerturbationImportance$new() PerturbationImportance$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PerturbationImportance.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"Creates new instance PerturbationImportance class","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PerturbationImportance.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"","code":"PerturbationImportance$new(   task,   learner,   measure,   resampling = NULL,   features = NULL,   sampler = NULL,   relation = \"difference\",   iters_perm = 1L )"},{"path":"https://jemus42.github.io/xplainfi/reference/PerturbationImportance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"task, learner, measure, resampling, features Passed FeatureImportanceMethod sampler (FeatureSampler) Sampler use feature perturbation relation (character(1)) relate perturbed scores originals. Can overridden $compute(). iters_perm (integer(1)) Number permutation iterations. Can overridden $compute().","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PerturbationImportance.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/PerturbationImportance.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"","code":"PerturbationImportance$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/PerturbationImportance.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":null,"dir":"Reference","previous_headings":"","what":"Relative Feature Importance — RFI","title":"Relative Feature Importance — RFI","text":"Implementation RFI using modular sampling approach","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Relative Feature Importance — RFI","text":"König, Gunnar, Molnar, Christoph, Bischl, Bernd, Grosse-Wentrup, Moritz (2021). “Relative Feature Importance.” 2020 25th International Conference Pattern Recognition (ICPR), 9318–9325. doi:10.1109/ICPR48806.2021.9413090 .","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Relative Feature Importance — RFI","text":"xplainfi::FeatureImportanceMethod -> xplainfi::PerturbationImportance -> RFI","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Relative Feature Importance — RFI","text":"xplainfi::FeatureImportanceMethod$combine() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Relative Feature Importance — RFI","text":"RFI$new() RFI$compute() RFI$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Relative Feature Importance — RFI","text":"Creates new instance RFI class","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Relative Feature Importance — RFI","text":"","code":"RFI$new(   task,   learner,   measure,   resampling = NULL,   features = NULL,   conditioning_set = NULL,   relation = \"difference\",   iters_perm = 1L,   sampler = NULL )"},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relative Feature Importance — RFI","text":"task, learner, measure, resampling, features Passed PerturbationImportance conditioning_set (character()) Set features condition . Can overridden $compute(). Default (character(0)) equivalent PFI. CFI, set features except tat interest. relation (character(1)) relate perturbed scores originals. Can overridden $compute(). iters_perm (integer(1)) Number permutation iterations. Can overridden $compute(). sampler (ConditionalSampler) Optional custom sampler. Defaults ARFSampler","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Relative Feature Importance — RFI","text":"Compute RFI scores","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Relative Feature Importance — RFI","text":"","code":"RFI$compute(   relation = NULL,   conditioning_set = NULL,   iters_perm = NULL,   store_backends = TRUE )"},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relative Feature Importance — RFI","text":"relation (character(1)) relate perturbed scores originals. NULL, uses stored value. conditioning_set (character()) Set features condition . NULL, uses stored parameter value. iters_perm (integer(1)) Number permutation iterations. NULL, uses stored value. store_backends (logical(1)) Whether store backends","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Relative Feature Importance — RFI","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Relative Feature Importance — RFI","text":"","code":"RFI$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relative Feature Importance — RFI","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/RFI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Relative Feature Importance — RFI","text":"","code":"library(mlr3) task = tgen(\"friedman1\")$generate(n = 200) rfi = RFI$new(   task = task,   learner = lrn(\"regr.ranger\", num.trees = 50),   measure = msr(\"regr.mse\"),   conditioning_set = c(\"important1\") ) #> ℹ No <ConditionalSampler> provided, using <ARFSampler> with default settings. #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. rfi$compute() #> Key: <feature> #>          feature importance #>           <char>      <num> #>  1:   important1  0.0000000 #>  2:   important2  4.7041325 #>  3:   important3  2.6131956 #>  4:   important4 11.3741061 #>  5:   important5  3.1800034 #>  6: unimportant1  0.1037830 #>  7: unimportant2 -0.2147035 #>  8: unimportant3  0.2413733 #>  9: unimportant4 -0.4715925 #> 10: unimportant5  0.2696917"},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":null,"dir":"Reference","previous_headings":"","what":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"Base class SAGE (Shapley Additive Global Importance) feature importance based Shapley values marginalization. abstract class - use MarginalSAGE ConditionalSAGE.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"SAGE uses Shapley values fairly distribute total prediction performance among features. Unlike perturbation-based methods, SAGE marginalizes features integrating distribution. approximated averaging predictions reference dataset.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"Covert, Ian, Lundberg, M S, Lee, Su-(2020). “Understanding Global Feature Contributions Additive Importance Measures.” Advances Neural Information Processing Systems, volume 33, 17212–17223. https://proceedings.neurips.cc/paper/2020/hash/c7bf0b7c1a86d5eb3be2c722cf2cf746-Abstract.html.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"xplainfi::FeatureImportanceMethod -> SAGE","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"n_permutations (integer(1)) Number permutations sample. reference_data (data.table) Reference dataset marginalization. sampler (FeatureSampler) Sampler object marginalization. convergence_history (data.table) History SAGE values computation. converged (logical(1)) Whether convergence detected. n_permutations_used (integer(1)) Actual number permutations used.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"xplainfi::FeatureImportanceMethod$combine() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"SAGE$new() SAGE$compute() SAGE$plot_convergence() SAGE$clone()","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"Creates new instance SAGE class.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"","code":"SAGE$new(   task,   learner,   measure,   resampling = NULL,   features = NULL,   n_permutations = 10L,   reference_data = NULL,   batch_size = 5000L,   sampler = NULL,   max_reference_size = 100L )"},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"task, learner, measure, resampling, features Passed FeatureImportanceMethod. n_permutations (integer(1): 10L) Number permutations per coalition sample Shapley value estimation. total number evaluated coalitions 1 (empty) + n_permutations * n_features. reference_data (data.table | NULL) Optional reference dataset. NULL, uses training data. coalition evaluate, expanded datasets size n_test * n_reference created evaluted batches batch_size. batch_size (integer(1): 5000L) Maximum number observations process single prediction call. sampler (FeatureSampler) Sampler marginalization. relevant ConditionalSAGE. max_reference_size (integer(1): 100L) Maximum size reference dataset. reference larger, subsampled.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"Compute SAGE values.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"","code":"SAGE$compute(   store_backends = TRUE,   batch_size = NULL,   early_stopping = NULL,   convergence_threshold = NULL,   min_permutations = NULL,   check_interval = NULL )"},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"store_backends (logical(1)) Whether store backends. batch_size (integer(1): 5000L) Maximum number observations process single prediction call. early_stopping (logical(1)) Whether check convergence stop early. convergence_threshold (numeric(1)) Relative change threshold convergence detection. min_permutations (integer(1)) Minimum permutations checking convergence. check_interval (integer(1)) Check convergence every N permutations.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"method-plot-convergence-","dir":"Reference","previous_headings":"","what":"Method plot_convergence()","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"Plot convergence history SAGE values.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"","code":"SAGE$plot_convergence(features = NULL)"},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"features (character | NULL) Features plot. NULL, plots features.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"ggplot2 object","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"objects class cloneable method.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"","code":"SAGE$clone(deep = FALSE)"},{"path":"https://jemus42.github.io/xplainfi/reference/SAGE.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"deep Whether make deep clone.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/c.FeatureImportanceMethod.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine two FeatureImportanceMethod objects — c.FeatureImportanceMethod","title":"Combine two FeatureImportanceMethod objects — c.FeatureImportanceMethod","text":"Combine two FeatureImportanceMethod objects","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/c.FeatureImportanceMethod.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine two FeatureImportanceMethod objects — c.FeatureImportanceMethod","text":"","code":"# S3 method for class 'FeatureImportanceMethod' c(x, y, ...)"},{"path":"https://jemus42.github.io/xplainfi/reference/c.FeatureImportanceMethod.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine two FeatureImportanceMethod objects — c.FeatureImportanceMethod","text":"x, y ([FeatureImportanceMethod]) Objects combine. Must computed scores. ... () Ignored.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/c.FeatureImportanceMethod.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine two FeatureImportanceMethod objects — c.FeatureImportanceMethod","text":"New object subclass x y.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/c.FeatureImportanceMethod.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Combine two FeatureImportanceMethod objects — c.FeatureImportanceMethod","text":"","code":"library(mlr3) task = tgen(\"2dnormals\")$generate(n = 100)  pfi1 = PFI$new(   task = task,   learner = lrn(\"classif.ranger\", num.trees = 50, predict_type = \"prob\"),   measure = msr(\"classif.ce\"),   features = \"x1\" ) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. pfi1$compute() #> Key: <feature> #>    feature importance #>     <char>      <num> #> 1:      x1  0.1818182  pfi2 = PFI$new(   task = task,   learner = lrn(\"classif.ranger\", num.trees = 50, predict_type = \"prob\"),   measure = msr(\"classif.ce\"),   features = \"x2\" ) #> ℹ No <Resampling> provided #> Using `resampling = rsmp(\"holdout\")` with default `ratio = 0.67`. pfi2$compute() #> Key: <feature> #>    feature importance #>     <char>      <num> #> 1:      x2  0.3030303  combined = c(pfi1, pfi2)"},{"path":"https://jemus42.github.io/xplainfi/reference/op-null-default.html","id":null,"dir":"Reference","previous_headings":"","what":"Default value for NULL — op-null-default","title":"Default value for NULL — op-null-default","text":"backport %||% available R versions 4.4.0.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/op-null-default.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default value for NULL — op-null-default","text":"","code":"x %||% y"},{"path":"https://jemus42.github.io/xplainfi/reference/op-null-default.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Default value for NULL — op-null-default","text":"x, y x NULL length 0, return y; otherwise returns x.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/op-null-default.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Default value for NULL — op-null-default","text":"","code":"1 %||% 2 #> [1] 1 NULL %||% 2 #> [1] 2"},{"path":"https://jemus42.github.io/xplainfi/reference/print_bib.html","id":null,"dir":"Reference","previous_headings":"","what":"Print an Rd-formatted bib entry — print_bib","title":"Print an Rd-formatted bib entry — print_bib","text":"Print Rd-formatted bib entry","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/print_bib.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print an Rd-formatted bib entry — print_bib","text":"","code":"print_bib(...)"},{"path":"https://jemus42.github.io/xplainfi/reference/print_bib.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print an Rd-formatted bib entry — print_bib","text":"... (character) One quoted names bibentries print.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_ewald.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"Reproduces data generating process Ewald et al. (2024) benchmarking feature importance methods. Includes correlated features interaction effects.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"","code":"sim_dgp_ewald(n = 500)"},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"n (integer(1)) Number samples create.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"regression task (mlr3::TaskRegr) data.table backend.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"Mathematical Model: $$X_1, X_3, X_5 \\sim \\text{Uniform}(0,1)$$ $$X_2 = X_1 + \\varepsilon_2, \\quad \\varepsilon_2 \\sim N(0, 0.001)$$ $$X_4 = X_3 + \\varepsilon_4, \\quad \\varepsilon_4 \\sim N(0, 0.1)$$ $$Y = X_4 + X_5 + X_4 \\cdot X_5 + \\varepsilon, \\quad \\varepsilon \\sim N(0, 0.1)$$ Feature Properties: X1, X3, X5: Independent uniform(0,1) distributions X2: Nearly perfect copy X1 (correlation ≈ 0.99) X4: Noisy copy X3 (correlation ≈ 0.67) Y depends X4, X5, interaction","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"Ewald, Katharina F, Bothmann, Ludwig, Wright, N. M, Bischl, Bernd, Casalicchio, Giuseppe, König, Gunnar (2024). “Guide Feature Importance Methods Scientific Inference.” Longo, Luca, Lapuschkin, Sebastian, Seifert, Christin (eds.), Explainable Artificial Intelligence, 440–464. ISBN 978-3-031-63797-1, doi:10.1007/978-3-031-63797-1_22 .","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"","code":"sim_dgp_ewald(100) #>  #> ── <TaskRegr> (100x6) ────────────────────────────────────────────────────────── #> • Target: y #> • Properties: - #> • Features (5): #>   • dbl (5): x1, x2, x3, x4, x5"},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"data generating processes (DGPs) designed illustrate specific strengths weaknesses different feature importance methods like PFI, CFI, RFI. DGP focuses one primary challenge make differences methods clear.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"","code":"sim_dgp_correlated(n = 500L)  sim_dgp_mediated(n = 500L)  sim_dgp_confounded(n = 500L, hidden = TRUE)  sim_dgp_interactions(n = 500L)  sim_dgp_independent(n = 500L)"},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"n (integer(1): 500L) Number observations generate. hidden (logical(1): TRUE) Whether hide confounder returned task. FALSE, confounder included feature, allowing direct adjustment. TRUE (default), proxy available, simulating unmeasured confounding.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"regression task (mlr3::TaskRegr) data.table backend.","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"Correlated Features DGP: DGP creates highly correlated predictors PFI show artificially low importance due redundancy, CFI correctly identify feature's conditional contribution. Mathematical Model: $$X_1 \\sim N(0,1)$$ $$X_2 = X_1 + \\varepsilon_2, \\quad \\varepsilon_2 \\sim N(0, 0.05^2)$$ $$X_3 \\sim N(0,1), \\quad X_4 \\sim N(0,1)$$ $$Y = 2 \\cdot X_1 + X_3 + \\varepsilon$$ \\(\\varepsilon \\sim N(0, 0.2^2)\\). Feature Properties: x1: Standard normal, direct causal effect y (β=2.0) x2: Nearly perfect copy x1 (x1 + small noise), causal effect y (β=0) x3: Independent standard normal, direct causal effect y (β=1.0) x4: Independent standard normal, effect y (β=0) Expected Behavior: Marginal methods (PFI, Marginal SAGE): falsely assign importance x2 due correlation x1 Conditional methods (CFI, Conditional SAGE): correctly assign near-zero importance x2 Key insight: x2 \"spurious predictor\" - correlated causal feature causal Mediated Effects DGP: DGP demonstrates difference total direct causal effects. features affect outcome mediators. Mathematical Model: $$\\text{exposure} \\sim N(0,1), \\quad \\text{direct} \\sim N(0,1)$$ $$\\text{mediator} = 0.8 \\cdot \\text{exposure} + 0.6 \\cdot \\text{direct} + \\varepsilon_m$$ $$Y = 1.5 \\cdot \\text{mediator} + 0.5 \\cdot \\text{direct} + \\varepsilon$$ \\(\\varepsilon_m \\sim N(0, 0.3^2)\\) \\(\\varepsilon \\sim N(0, 0.2^2)\\). Feature Properties: exposure: direct effect y, mediator (total effect = 1.2) mediator: Mediates effect exposure y direct: direct effect y effect mediator noise: causal relationship y Causal Structure: exposure → mediator → y ← direct → mediator Expected Behavior: PFI: Shows total effects (exposure appears important) CFI: Shows direct effects (exposure appears less important conditioning mediator) RFI mediator: show direct effects similar CFI Confounding DGP: DGP includes confounder affects features outcome. Uses simple coefficients easy interpretation. Mathematical Model: $$H \\sim N(0,1)$$ $$X_1 = H + \\varepsilon_1, \\quad X_2 = H + \\varepsilon_2$$ $$\\text{proxy} = H + \\varepsilon_p, \\quad \\text{independent} \\sim N(0,1)$$ $$Y = H + 0.5 \\cdot X_1 + 0.5 \\cdot X_2 + \\text{independent} + \\varepsilon$$ \\(\\varepsilon \\sim N(0, 0.5^2)\\) independently. Model Structure: Confounder H ~ N(0,1) (dashed red node = potentially unobserved) x1 = H + noise, x2 = H + noise (affected confounder) proxy = H + noise (noisy measurement confounder) independent ~ N(0,1) (truly independent) y = H + 0.5x1 + 0.5x2 + independent + noise Expected Behavior: PFI: show inflated importance x1 x2 due confounding CFI: partially account confounding conditional sampling RFI conditioning confounder/proxy: reduce confounding bias Interaction Effects DGP: DGP demonstrates pure interaction effect features main effects. Mathematical Model: $$Y = 2 \\cdot X_1 \\cdot X_2 + X_3 + \\varepsilon$$ \\(X_j \\sim N(0,1)\\) independently \\(\\varepsilon \\sim N(0, 0.5^2)\\). Feature Properties: x1, x2: Independent features interaction effect (main effects) x3: Independent feature main effect noise1, noise2: causal effects Expected Behavior: PFI: assign near-zero importance x1 x2 (marginal effect) CFI: capture interaction assign high importance x1 x2 Ground truth: x1 x2 important interaction Independent Features DGP: baseline scenario features independent effects additive. importance methods give similar results. Mathematical Model: $$Y = 2.0 \\cdot X_1 + 1.0 \\cdot X_2 + 0.5 \\cdot X_3 + \\varepsilon$$ \\(X_j \\sim N(0,1)\\) independently \\(\\varepsilon \\sim N(0, 0.2^2)\\). Feature Properties: important1-3: Independent features different effect sizes unimportant1-2: Independent noise features effect Expected Behavior: methods: rank features consistently true effect sizes Ground truth: important1 > important2 > important3 > unimportant1,2 ≈ 0","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"sim_dgp_correlated(): Correlated features demonstrating PFI's limitations sim_dgp_mediated(): Mediated effects showing direct vs total importance sim_dgp_confounded(): Confounding scenario conditional sampling sim_dgp_interactions(): Interaction effects features sim_dgp_independent(): Independent features baseline scenario","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"Ewald, Katharina F, Bothmann, Ludwig, Wright, N. M, Bischl, Bernd, Casalicchio, Giuseppe, König, Gunnar (2024). “Guide Feature Importance Methods Scientific Inference.” Longo, Luca, Lapuschkin, Sebastian, Seifert, Christin (eds.), Explainable Artificial Intelligence, 440–464. ISBN 978-3-031-63797-1, doi:10.1007/978-3-031-63797-1_22 .","code":""},{"path":"https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"","code":"task = sim_dgp_correlated(200) task$data() #>                 y         x1         x2          x3          x4 #>             <num>      <num>      <num>       <num>       <num> #>   1:  0.541720651  0.1055768  0.1241149  0.15107710 -1.07048046 #>   2:  3.434848698  1.1542704  1.1429292  0.81639270  0.07823951 #>   3: -0.003763878  0.4398628  0.4163472 -0.89330718  0.08869865 #>   4:  0.661379593  0.7680182  0.7257787 -0.85287554 -0.44789096 #>   5:  3.004805830  1.8755039  1.8810255 -0.64117105 -0.45062279 #>  ---                                                            #> 196: -1.685294577 -1.0331424 -1.0040098  0.24093188 -0.30966184 #> 197: -1.362309252 -0.4802121 -0.4372220 -0.46732499  0.47758248 #> 198: -1.865575355 -1.0670027 -1.1202889  0.09559857  0.67402610 #> 199: -1.380668063 -0.5852271 -0.5354215 -0.48768747 -2.62684422 #> 200:  0.370175643  0.4714608  0.4199878 -0.54723728 -0.84441133 task = sim_dgp_mediated(200) task$data() #>               y      direct     exposure    mediator      noise #>           <num>       <num>        <num>       <num>      <num> #>   1: -2.4507620 -2.49033750  0.855039009 -0.91849623  2.0193068 #>   2: -0.4856845 -1.10159444  0.316437054 -0.08388302 -0.4899665 #>   3:  2.6265686 -0.74597827  3.001975867  2.07588119  1.7836495 #>   4: -3.8320292 -2.76114026  0.007748219 -1.57275053 -1.4220784 #>   5:  1.1031660  0.12343807  0.704587807  0.58472061  1.2859328 #>  ---                                                            #> 196: -0.6422871 -1.39379597  1.801892795 -0.10375105 -1.2034256 #> 197:  1.6419980 -0.36869554  1.323232204  1.18918174  0.1302005 #> 198:  2.8214264  1.15289554  0.722126393  1.44443821  1.2508633 #> 199:  0.7754254 -0.52385655  1.130385702  0.51736246  0.3963370 #> 200: -0.5367418 -0.09452166 -0.017235406 -0.14436170 -0.5612657 # Hidden confounder scenario (traditional) task_hidden = sim_dgp_confounded(200, hidden = TRUE) task_hidden$feature_names  # proxy available but not confounder #> [1] \"independent\" \"proxy\"       \"x1\"          \"x2\"           # Observable confounder scenario task_observed = sim_dgp_confounded(200, hidden = FALSE) task_observed$feature_names  # both confounder and proxy available #> [1] \"confounder\"  \"independent\" \"proxy\"       \"x1\"          \"x2\"          task = sim_dgp_interactions(200) task$data() #>               y     noise1     noise2         x1         x2         x3 #>           <num>      <num>      <num>      <num>      <num>      <num> #>   1:  1.9203856  0.3292959  1.0452286  0.6109849  2.5141493 -0.9545340 #>   2:  2.7785905  0.7228571  1.3840878  1.0076382  0.9498442  0.7523446 #>   3: -0.8162857  1.1655195 -2.3383271 -0.4425555 -1.3743170 -1.7070809 #>   4:  3.0811893 -0.2844699  0.5882943  0.8555139  1.5253820 -0.0119134 #>   5: -1.0680510  1.3407997 -0.5332860  0.8247902 -1.4444450  1.0448261 #>  ---                                                                   #> 196: -1.6346280 -0.8556112  0.3384712 -0.3866522  0.2095041 -1.1562896 #> 197:  1.9338887 -0.8774731 -0.3369532  0.5841519  0.8965394  0.6261360 #> 198:  1.2200553  1.3560324  1.8875147  0.4470443  0.1009925  1.3702767 #> 199: -1.8091672  0.4951773  0.3385215 -1.2370485  0.0757572 -1.0561417 #> 200:  4.2870066 -1.0673890  0.6591085 -1.2262141 -1.5657245 -0.2107789 task = sim_dgp_independent(200) task$data() #>                y  important1  important2  important3 unimportant1 unimportant2 #>            <num>       <num>       <num>       <num>        <num>        <num> #>   1: -3.06605137 -1.81160252  0.05484335  0.70671245   -0.5245799   1.80144884 #>   2: -3.56293039 -0.04450919 -2.29478284 -1.76792367   -0.1982537   0.28720781 #>   3: -0.05117797  0.07052612 -0.11359546 -0.20201293   -0.6362117   0.41579580 #>   4: -2.93674258 -1.48581868 -0.76440968  2.11917659   -0.2619648  -0.28566872 #>   5: -1.65710544 -0.61662909 -0.80095171  0.50585971   -1.7902694   0.07506766 #>  ---                                                                           #> 196:  3.23708566  0.22488684  2.94754036 -0.08430800   -0.5906424  -0.29109668 #> 197:  0.21377350  0.09675109 -0.85792056  1.41169361   -0.5413057   1.51304649 #> 198:  2.45032676  1.17755583 -0.07006885 -0.05831616   -0.7487085  -1.88254414 #> 199: -0.20609617 -0.33222060  0.20053489  0.56315509   -0.4753334   1.11598344 #> 200: -2.72682759 -0.89138084 -0.60225482 -0.61977748    2.4784545  -1.25447596"},{"path":"https://jemus42.github.io/xplainfi/reference/xplainfi-package.html","id":null,"dir":"Reference","previous_headings":"","what":"xplainfi: Feature Importance Methods for Model Interpretability — xplainfi-package","title":"xplainfi: Feature Importance Methods for Model Interpretability — xplainfi-package","text":"Provides consistent interface common feature importance methods, permutation feature importance, 'LOCO', 'SAGE'.","code":""},{"path":[]},{"path":"https://jemus42.github.io/xplainfi/reference/xplainfi-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"xplainfi: Feature Importance Methods for Model Interpretability — xplainfi-package","text":"Maintainer: Lukas Burk github@quantenbrot.de (ORCID)","code":""},{"path":"https://jemus42.github.io/xplainfi/news/index.html","id":"xplainfi-0109000","dir":"Changelog","previous_headings":"","what":"xplainfi 0.1.0.9000","title":"xplainfi 0.1.0.9000","text":"Extend ARFSampler store arguments construction, making easier “preconfigure” sampler via arguments used $sample(). Standardize conditioning_set name character vector defining features condition ConditionalSampler RFI. Streamline PerturbationImportance implementation. Add batch_size argument SAGE methods control number observations used per learner$predict_newdata() call (lead excessive RAM usage). Add sim_dgp_ewald() simulate data simple DGP used illustration Ewald et al. (2024), make easier interpret results various importance methods. Currently support conditioning_set get arugments obs_loss = FALSE aggregation_fun, defaulting median case obs_loss = TRUE, allow macro-averaged median absolute differences calculcation original LOCO formulation, rather micro-averaged approach calculated default. Fix accidentally marginal ConditionalSAGE. Permutations evaluated steps check_interval time, convergence checked values change less convergence_threshold, convergence assumed $converged field set TRUE least min_permutations perfomed case, $n_permutations_used shows number performed permutations $convergence_history tracks convergence history can analyzed see per-feature values checkpoint $plot_convergence_history() plots convergence history per feature Convergence tracked first resampling iteration","code":""},{"path":"https://jemus42.github.io/xplainfi/news/index.html","id":"xplainfi-010","dir":"Changelog","previous_headings":"","what":"xplainfi 0.1.0","title":"xplainfi 0.1.0","text":"PFI CFI RFI (via arf-powered conditional sampling) SAGE (marginal conditional, latter via arf) LOCO LOCI Includes comparison reference implementation Python via fippy","code":""}]
